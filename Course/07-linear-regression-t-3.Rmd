# Linear Regression Theory III: Diagnostics {#log-t-3}

``` {r init, include = FALSE}
library(tidyverse)

load("../datasets/grades.RData")
```


## Model fit

Over the last two sessions we learned the theory and mathematics behind linear
regression, how to interpret coefficients for different types of explanatory
variables and how we can use p-values to assess our hypothesis. But if you
looked closely, there are still parts of the output we have not talked about
yet.

Let us again inspect the output from the simplest model we computed, regressing
the grade solely on the invested hours:

``` {r reg_grade_hours, echo = FALSE}
m1 <- lm(grade ~ hours_centered, grades)
summary(m1)
```

Up to now, we exclusively talked about the coefficient block. We will return to
the "Call" next session and to the "Residuals" later in this session. For now
let us focus on the bottom block in the output.

$R^2$ or *R-squared* is a measure for the amount of variance in the data that is
"explained" by the model. Real world data will always have variance. Not every
value will neatly fall onto the mean value of a variable. Rather the data is
dispersed around it. The same is true for our dependent variable:

``` {r plot_grade_distribution, echo = FALSE}
grade_vector_small_lbl <- c("1.0", "2.0", "3.0", "4.0", "5.0")

grades %>% 
  ggplot(aes(x = grade)) +
  geom_density() +
  geom_vline(aes(xintercept = mean(grade)), colour = "red") +
  scale_x_continuous(breaks = 1:5, labels = grade_vector_small_lbl) +
  labs(title = "Distribution of grades",
       subtitle = "Mean of grades displayed in red",
       x = "Grade achieved")
```

This is a density plot and shows the distribution of a metric variable as a
smoothed line. We do not see every individual actual value but the general shape
of the data. The red line represent the mean of `grade`, which is about $2.97$.
Most actual values are not exactly at the mean but are rather dispersed around
it, ranging between $1.0$ and $5.0$. This is the variance in our outcome 
variable.

Let us now plot `grade` against `hours_centered` and add the regression line
from our model above:

``` {r plot_lm_grade_hours_centered, echo = FALSE}
grades %>% 
  ggplot(aes(x = hours_centered, y = grade)) +
  geom_point() +
  geom_abline(slope = -0.05236, intercept = 2.9675, colour = "blue") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  scale_y_continuous(breaks = 1:6, labels = c("1.0", "2.0", "3.0", "4.0", "5.0", "6.0"), limits = c(0, 6)) +
  scale_x_continuous(breaks = seq(-20, 20, 10), limits = c(-20, 20)) +
  labs(title = "Regression of seminar paper grade on hours invested in writing",
       x = "Hours invested (centered)",
       y = "Grade achieved")
```

Without our regression line, all we would have is a cloud of points without much
order to it. What linear regression does, is trying to bring order into this by
fitting a line that best explains the variance of the dependent variable, `grade`
in our case by its relationship to one or multiple dependent variables, here
`hours_centered`.
But this linear line can never explain the variance completely. 
For this it had to pass through every data point. Our line does not. Actually
most data points do not lie on the regression line but at some distance to it.
You will remember that OLS computes *the* regression line for which the squared
distances are smallest. This is the line that explains most of the variance of y
by its relationship to x, but not all variance is explained. An unexplained part
remains. These are the residuals, the distance that points fall from the 
regression line. $R^2$ tells us the relative amount of how much we reduced the
initial variance by fitting the line and thus explaining a part of said
variance.

A $R^2$ of $0$ would mean that no variance is explained, a value of $1$ that all
variance is explained. Two highly unlikely outcomes. We will almost always 
explain something and never explain everything.  

In our model, $R^2 = 0.09344$. This means we explained about $9.3\%$ of the
variance of `grade` through its relationship with `hours_centered`. That's nice,
but this also means that over $90\%$ are still unexplained. We will not explain
all of the variance, i.e. $R^2 = 1$, but in general a higher $R^2$ is desirable.

So what can we do? We can try to add additional variables to the model that help
in explaining the variance in the outcome variable. Last session we concluded
that the "correct" model to measure the effect of invested hours on the achieved
grade would also have to include `contact`:

``` {r reg_grade_hours_contact, echo = FALSE}
m2 <- lm(grade ~ hours_centered + contact, grades)
summary(m2)
```

We can use $R^2$ to compare the *model fit* of multiple models. Here the larger 
model achieved a considerably higher value of $R^2 = 0.2643$. The model fit
improved as we can now explain a higher ratio of the variance in `grade`.

After $R^2$ we see another value, *Adjusted R-squared*. This becomes relevant
if we add additional variables to our model. $R^2$ almost always increases, and
never decreases, when adding additional variables to the model,
especially if we have few observations. Because of this $R^2$ can get less
reliable when we have many variables and few observations. *Adjusted R-squared*
corrects for this by including both factors in the calculation. When we have
many observations the differences are negligible. This is true for our case.
We have relatively many observations and few variables in our model, so the
values of both measures are rather close. But in cases where this relationship
is not as favorable adjusted R-squared should be used in place of the regular
$R^2$.

The block in our output also gives us the *Residual standard error*. As we have
seen above, most actual data point do not lie on the regression line but some
distance away from it. These are residuals. Thus their standard error basically
tells us how much we miss the spot on average. As it is given in units of the
dependent variable, we can say that the estimated for `grade` based on our
second model are on average $0.93$ off. A considerable amount, as this is almost
one whole grading step. This is still an improvement from the $1.028$ in the
first model but nevertheless a substantial error.

The last line in the output gives us two connected measures. The *F-statistic*
is the test statistic for $R^2$ and is used the compute the corresponding
*p-value*.
In this case we are testing if the $R^2$ our model returned based on our sample
is possible, when the actual population value of $R^2$ if $0$. In other words,
could we have achieved this $R^2$ by chance if the independent variables in our
model actually do not explain part of the variance in the population?
Both of our models have very small p-values, so it is highly unlikely that we
have just explained some variance by chance. This gives further credibility to
our model specification.

We can conclude that the second model was an improvement over the first. But can
we do more? Sure! We can add additional explanatory variables:

``` {r reg_grade_hours_prev_att_contact, echo = FALSE}
m3 <- lm(grade ~ hours_centered + previous_grades_centered + attendance + contact, grades)
summary(m3)
```

The p-value is even lowe, and the F-statistic even higher, compared to our
second model, but this was never an issue. What is more interesting is that we
have substantially increased $R^2$ and decreased the residual standard error.
As we have concluded last week, this larger model is better at predicting the
actual values of `grade`. Thus the explained variance has to increase and the
average error in estimating y has to decrease. But is this the better model? The
values on the model fit would suggest so. And this also is true, if our aim is
predicting `grade` to the best of our abilities. But if our aim is still 
measuring the effect of `hours` on `grade` we know from our DAG that we do not
have to or even should not control for the additional variables to get an
unbiased estimator for the effect of interest.

What can we take away from this? While the model fit measures are an important
tool for comparing multiple possible models and better values are desirable in
general, it should not be our goal to just max out all measures and declare this
model the "winner". It is never that easy in statistics.
One thing we can never replace is thorough theoretical work before even
computing our first model. Based on our DAG, if it is correct, we know do not
have to control for previous grades and attendance. Including them may give us a
larger $R^2$, but is still not the correct way to build our model.

Based on this our best model is still the second one:

``` {r m2_again}
summary(m2)
```

So can we finally call it a day and put the issue of the effect from invested
hours on received grade to rest for good? Maybe, but we still do not know for
certain befre we apply a last step in constructing a valid linear regression
model, *regression diagnostics*.

## Regression Diagnostics

As linear regression is a statistical technique, there are certain statistical
assumptions we have to meet. If we violate those, the best laid plans may 
falter and our results may be not as robust as we hoped. 
Let us go through these assumptions and the tests to check for them one by one.

### Linearity

The name already gives it away, a *linear* regression is used to estimate 
**linear** relationships between variables. For this to work, the relationships
actually have to be linear. But a relationship between two variables can have
other functional forms. Consider this for example:

``` {r non_linear, echo = FALSE}
tibble(x = seq(-25, 25),
       y = x^2) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

The relationship is clearly not linear. But we can still fit a regression and
get a result:

``` {r non_linear_reg, echo = FALSE, message = FALSE}
tibble(x = seq(-25, 25),
       y = x^2) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

The regression line shows us that `x` and `y` are completely uncorrelated. This
is clearly not true, but as our linear regression assumes linearity, it tries to
model the relationship in linear terms.

What we can do in such cases is to transform the variable in question in a
non-linear way. Here the quadratic relationship is easy to spot, so if we
transform $x$ to $x^2$, this happens:

``` {r non_linear_reg_squared, echo = FALSE, message = FALSE}
tibble(x = seq(-25, 25),
       y = x^2) %>%
  mutate(x = x^2) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

The non-linear relationship between `x` and `y` has thus been transformed into a
linear one.

For real world data, the non-linearity most often is not as straightforward to
spot as in this example. A first step to approach this, is inspecting a 
scatterplot matrix. This is usually done before starting to model to identify
relationships between the variables used.

``` {r scatterplot_matrix, echo = FALSE, message = FALSE}
library(GGally)
ggpairs(grades, columns = c(1, 2, 5))
```

The diagonal displays the distribution of all variables included. Here metric
variables are displayed as density plots and categorical variables as bar plots.
Below and above the diagonal the relationship between two variables is shown.
The scatterplot on the left of the second row is the one between `hours` and
`grade` we have already seen several times. There is no indication of 
non-linearity here. What we have not inspected yet, is the relationship
between `contact` and the two other variables. 
The bottom row contains histograms of the two metric variables by the category
of contact, the right column boxplots for the same combination. Without going
into too much detail on both types of plots, both show us how the distribution
for both metric variables changes by category. The more personal the contact
with the lecturer, the lower the distribution of final grades is. This makes
sense, as we have already seen this correlation in the results of our model.
Between `hours` and `contact` there seems to be no correlation. The amount of 
hours a student invests in writing the paper, does not lower the hours invested
in a systematic way. 

XXX MAYBE LINK EXPLANATIONS FOR HISTOGRAMS AND BOXPLOTS XXX

But this does not clear the model of suspicions of non-linearity just yet. Even
when all pairwise relationships are linear, controlling for multiple variables
at the same time can introduce non-linearity for this specific combination.
One way to approach this is to inspect the *Residuals vs. Fitted* plot. As the
name suggests, this plots the fitted values, i.e. the estimates for our 
dependent variables based on the model, against the residuals of the dependent
variable. When the relationship is linear, we should see a more or less straight
line along the x-axis, where $y = 0$.

``` {r residuals_fitted, echo = FALSE}
plot(m2, 1)
```

For many use cases, the line is straight enough, indicating no clear and strong
patterns on non-linearity. Still the residuals seem to be slightly off for very
good and very bad estimated grades. 

#### No missing relevant explanatory variables

Besides violating the assumption of linearity, patterns in the residuals vs.
fitted plot can also indicate, that there is some important explanatory variable
missing from the model. If our DAG is correct, we can rule out `attendance` and
`previous_grades`. We did assume that `contact` is a confounder for `hours` and
`grade` and thus included it in our model. Did we go wrong there?

We did assume, that the more personal the contact, the more efficiently the time
working on the paper can be used. And here lies the problem. The way we included
`contact` in the model is not the way we reasoned in our DAG. It would we be
correct if we assumed that the more personal the contact, the less time has to
be invested. But we already saw in the scatterplot matrix that there is no such
relationship between the variables. To specify the effect of `contact` in the
model correctly, we have to include it as an *interaction* with `hours`.
We will correct this, after we have applied the further tests and discussed the
remaining assumptions.

### Normally distributed residuals

Another assumptions in linear regression is that the residuals are normally
distributed. This is especially relevant for sample with a small n as the
test statistics tend to get unreliable in these cases if the residuals are not
normally distributed. For larger samples, as in our case, this is not as
problematic. Still, systematic deviations from normality can indicate that the
model is not *parsimonious*. This means that either not all relevant variables
are included or that variables are included that are not necessary for the model.

We can check the normality assumption using a *Q-Q plot*. This plots the
standardizes residuals, the residuals divided by their standard error, against
a theoretical normal distribution. If the residuals are perfectly normally 
distributed, each data point lies on the diagonal, if they are not they move
away from the line. Small deviations, especially in the tails, should not be
over emphasized. What we are looking for are clear and systematic deviations.

``` {r qq, echo = FALSE}
plot(m2, 2)
```

In the case of our model, most data points lie on the diagonal, while there are 
some small deviations in the tails. As our n is large enough, this should not be
problematic. It may indicate that the model is not parsimonious but there is
no clear cause for concern here.

### Homoscedasticity

The *homoscedasticity* assumption states that the residuals are expected to have
a constant variance over the whole range of the dependent variable. Let us 
assume that the variance of our residuals would be lower for very good grades
and higher for very bad ones. This would indicate that we can make more accurate
estimates for better grades than for worse ones as a small variance would
indicate smaller residuals and thus a smaller error. For the assumption to hold
we must be able to make about the same quality, be it high or low, for all
values of `grade`.

The problem is that the computation of the standard errors, test statistics and
p-values depends on this assumption. If the assumption is violated, if we have
*heteroscedasticity*, these measures are not reliable anymore.

The problem often occurs, if the dependent variable is not symmetric. In the
scatterplot matrix above, we already saw that `grade` is fairly symmetrically
distributed, so we would not expect problems here. If our dependent variable was
unsymmetrical, transforming it to be more symmetrical, e.g. by using the 
logarithm or a square root, could help.

To check for problems with heteroscedasticity, we can use the *Scale-Location*
plot. This plots the fitted values against the square root of the standardized
residuals. For homoscedasticity to hold, we should see our data points as a
horizontal band with more or less constant width running from the left to the
right. The same goes for the plotted line.

``` {r homoscedasticity, echo = FALSE}
plot(m2, 3)
```

In our case the homoscedasticity assumption holds. Slight variations are not
problematic and overall the variance seems to be constant.


### No overly influential data points

Observations can get highly influential if they have unusual values. Sometimes
these are extremely low or high values on some variable. But even "normal" values
on two or more variables can get unusual in their combination. Imagine a student
with $60$ invested hours. A high value but not overly extreme. Now the same student
had in person contact with their lecturer but still received a $5.0$. This could
potentially be an outlier as this combination is unusual in terms of what the 
model expects. In case of our model this observation would most probably not be
overly influential. But imagine the same observation with $300$ invested hours.
Such extreme cases can influence the fit by figuratively "puling" the regression
line in their direction.

We can divide influential data points into unusual values on the dependent
variable, *outliers*, and unusual values on independent variables,
*high leverage points*. The latter have high leverage because the "pull" on the
regression lines and thus change the slope. As a rule of thumb, we can consider
values with standardized residuals over $3$ or under $-3$ outliers. 
Concerning the dependent variables we can compute the *leverage statistic*.
Here values that exceed $2 * (p + 1) / n$, where $p$ is the number of independent
variables, are considered as having high leverage.
We can inspect both at the same time in the *Residuals vs. Leverage* plot.

XXX LINK TO LEVERAGE STATISTIC FORMULA XXX

``` {r residuals_leverage, echo = FALSE}
plot(m2, 5)
```

We can see that there are no clear outliers. To assess points with high leverage
we first have to compute the threshold as: $2 * (2 + 1) / 200 = 0.03$. We can
see that there are a number of points that exceed this value. The question is,
why do these values exist? Sometimes these are measurement errors, extreme values
or unusual combinations that come down to the researcher recording the wrong 
values into the dataset. In these cases we can try to fix the errors or remove
the observations from the data. As we have several values with high leverage,
this seems highly unlikely. But if we had not simulated the data ourselves and 
knew that there is no error, we should at least check. What seems more probable
though, and is often the actual root of high leverage, is that there are
variables missing from the model that could explain the high leverage. 
In this case the values should be lower after we include the missing
variables into the model. We will return to this later.

XXX ALSO INCLUDE COOKS D? XXXX


### No multicollinearity



Independence of residuals error terms.






## Returning to our research question

### Interactions




``` {r reg_grade_hours_contact_interact, echo = FALSE}
m4 <- lm(grade ~ hours_centered + contact + hours_centered * contact, grades)
summary(m4)
```


* here we know, never that easy in realtiy


## Maybes

*	Mediation
  * maybe theory into DAG session and example into application
*	Multiple outcomes
  * maybe leave out

