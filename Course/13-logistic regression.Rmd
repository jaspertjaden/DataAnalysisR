# Logistic regression {#out-look}

## Objectives


## Functions covered in this week


## Basic concepts

Logistic regression is similar to linear regression. The main difference is that 
it can perform better for binary variables (i.e. 0 vs. 1). Binary outcomes have 
only two levels. Linear regression is used to model continuous variables such as the number of points scored in an NBA game. A binary outcome would be the sex of the player (male vs. female) or the maybe the health status (injured vs. not injured).

Logistic regression and linear regression are both part of the family of 
generalized linear regression models (including also Poisson regression, ridge, lasso etc.).
Logistic regression and linear regression share some of the same assumptions, such
as linearity, outliers and multicollienarity.However, under the hood, logistic 
regression uses a different estimation method. 


Rather than fitting a straight line through data points as in linear regression,
logistic regression fits a "s-shaped" line through data points. Coefficients in 
the logistic model are interepreted as the probability of being category 1 or 0. 

-> Niaz: visual here called "linear vs logistic" saved in folder, scatterplot with striaght line and s-shaped line.

Linear regression uses ordinary least sqauares as estimation method (i.e. using the distance from each observation to the regression line). Logistic regression uses the "maximum likelihood function" to approximate the s-shape curve which best fits the data. It moves the s-curve left and right and
calculates for each observation the likelihood of being either 0 or 1 given the assumed distribution. Then we take the average of all likelihoods of observations. The best fitting 
curve is the one with the largest likelihood (that's why it is called "maximum likelihood function").

See this youtube playlist on logistic regression for more details [https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe&ab_channel=StatQuestwithJoshStarmer]. 

## Application

Let's apply logistic regression to our NBA data. 

```{r prep, warning=FALSE, message=FALSE}

# load packages
library("tidyverse")
library("readxl")
library("lubridate")

# import data 
nba_salaries <- read_csv("../datasets/nba/salaries_1985to2018.csv", show_col_types = FALSE)
nba_players <- read_csv("../datasets/nba/players.csv", show_col_types = FALSE)

# merge
data_nba <- merge(nba_players, nba_salaries, by.x = c("_id"), by.y=c("player_id"))

# clean
data_nba <- data_nba %>%
        select(everything(), -league, -highSchool) %>%
        filter(season_start>=1998) %>%
  mutate(year_of_birth = year(mdy(birthDate)),
         age = season_start - year_of_birth,
         position_center = 
      case_when(position = str_detect(position,"Center") ~ 1,
                TRUE ~ 0),
    position_sf = 
      case_when(position = str_detect(position,"Small Forward") ~ 1,
                TRUE ~ 0),
    position_pf = 
      case_when(position = str_detect(position,"Power Forward") ~ 1,
                TRUE ~ 0),
    position_sg = 
      case_when(position = str_detect(position,"Shooting Guard") ~ 1,
                TRUE ~ 0),
    position_pg = 
      case_when(position = str_detect(position,"Point Guard") ~ 1,
                TRUE ~ 0),
    weight = str_replace(weight, "lb", ""),
         weight = as.numeric(weight),
         height = str_replace(height, "-", "."),
         height = as.numeric(height),
    ) %>%
  rename(id = "_id") %>%
  select(id, name, age, weight, height, birthPlace, everything(), -position, -birthDate, -year_of_birth)

data_nba <- data_nba %>%
  group_by(id) %>%
  mutate(seasons_played = n()) %>%
  ungroup()
str(data_nba)

```

First, let's check out dataset for binary outcomes. Binary variables in R are
labelled as "factor" variables. Any categorical variable is a factor variable.
A binary variable is simply a categorical variable with two categories.

```{r}
str(data_nba)
```

We see that there are no "factor" variables currently in the dataset. However, 
there are a few variables that should be marked as factors which currently 
aren't, such as birth place, college, draft team, and the shooting hand (left/right hand).
If we want to use any of these variables in a regression, we need to convert them.
Currently they are marked as "character" variables. 

Let's assume we are interested in knowing how long a player plays in the NBA 
measured by the number of seasons he played. It could be very useful for teams to predict
how long players will last. 

Let's first look at that variable, dichotomize it (turning it into 0 vs. 1), and
then explore it's distribution.

```{r}
# check distribution of original variable
summary(data_nba$seasons_played) 

# dichotomize
data_nba <- data_nba %>%
  mutate(career_long = 
           case_when(seasons_played>12 ~ 1,
                     seasons_played<=12 ~ 0,
                     TRUE ~ NA_real_),
         career_long = as.factor(career_long))

table(data_nba$career_long)

data_nba %>%
  filter(team =="Dallas Mavericks") %>%
  ggplot(aes(x= seasons_played, y=career_long)) +
  geom_count() +
  # stat_smooth(method="glm",
  #             se=FALSE, 
  #             method.args = list(family=binomial),
  #             formula= aes(career_long ~ seasons_playe)) +
  theme_minimal()

# NIAZ - plot below should show the logistic regression curve, but doesnt work
```

The plot above shows the number of players for the Dallas Mavericks by career length
and seasons played. The size of the buble represents the number of players in that 
category.

Now, let's estimate a model explaining whether a player has a long career or not.
Let's include weight and height as independent variables

```{r}

model_logit1 <- glm(career_long ~ height + weight,
                    data = data_nba,
                    family = binomial)

summary(model_logit1)

```

## Interpretation

The interpretation of coefficients in a logistic regression is different from 
the linear model. In a linear model, coefficients are interpreted as the increase 
on the y-scale for one unit increase in x. For logistic regression, coefficients
relate to the increase or decrease of the probability (measured in log odds) that 
y= 1 for a one unit increase x. 

In the output above, we see that the coefficient for height is -0.16. This means 
that for every inch (~ ca. 2.5 cm) in height, the probability of having a long career
is 0.16 log odds lower. This suggests that taller people haver shorter NBA careers.

In practice no one interprets the effect size in log odds. In the standard logistic 
regression model, we only look at the p-value to see if the effect is statistally
significant and we look at the sign in front of the coefficient to check whether
the effect is positive or negative. 

A more meaningful way to interpret effects are odds ratios. Log odds can be 
converted using the following appraoch. 

```{r}
exp(coef(model_logit1))
```

Now, the effect of height is 0.84. This means that the probability of a long career
is 0.85 times higher for taller people than for smaller people. If the odds ratio 
is 1, the probability would be the same. If it is 2, the probability would be twice
as high. If it is lower than 1, it means that the probability is lower and that
the effect is negative. 

There is an alternative "tidyverse" way to get odds ratios using the `broom()` package.
It neatly converts the output into a table which makes it easier to graph later.

```{r}

model_logit1 %>% 
      broom::tidy(exp = TRUE)
```


More recently, it has become more common to convert the coefficients into probabilities.
There are various packages and various options and no clear agreement on what is
the preferred output. 

Documentation for the `ggeffects` package[https://strengejacke.github.io/ggeffects/index.html] and the `margins` package [https://thomasleeper.com/margins/ ] capture this
discussion in detail. In the following, we will show you how to get Average MArginal Effects
and Average Predicted Probabilities. 

The interpretation of Average Marginal Effects (AMEs) is more intuitive than log odds
or odd ratios. AMEs are the average change in probability of y occuring for each unit increase
in x. 

```{r}

# get average predicted probabilities for each level of "height"
predicted <- augment(model_logit1, type.predict = "response") %>%
  group_by(height) %>%
  summarise(mean_fitted = mean(.fitted, rm.na=T))

# same as above, just using "ggpredict"
#install.packages("ggeffects")
library("ggeffects")
ggpredict(model_logit1, "height")

# ggpredict
#install.packages("ggeffects")
library("ggeffects")
ggpredict(model_logit1, "height")

# get Average marginal effects
library("margins")
library("broom")

# Average Marginal Effects - for all model variables
margins(model_logit1, type = "response")

# Only for some
summary(margins(model_logit1, variables = "height"))

```

The AME for height is -0.029. This means that for every additional inch in height,
the NBA players are ~ 3 percentage points less likely to have a long career.


## model fit

As we have seen with linear regression, there are differnt ways to assess whether a model is actually "good". For linear regression, a popular mesaure is the R-squared which reports the "percenatge of variance in y which is explained by the model". 
For logistic regression, there is no consensus on what the best measure is. There are dozens of different ways to calculate different r-squares. It's complicated. See here[https://www.youtube.com/watch?v=xxFYro8QuXA&ab_channel=StatQuestwithJoshStarmer] for more in-depth technical discussion. 

One way to get model performance measures is the `performance` package. One r2
that is often recommended is the McFadden pseudo r2. This measure should not 
be interpreted as "percentage of variance" explained. It is useful as a relative
tool. Larger values are better than smaller values. 

```{r}
model_performance(model_logit1)
r2_mcfadden(model_logit1)
```

Let's add some variables to our model:

```{r}
model_logit2 <- glm(career_long ~ height + weight + position_center + position_pg +
                      season_start,
                    data = data_nba,
                    family = binomial)
```

Now, let's compare models

```{r}
r2_mcfadden(model_logit1)
r2_mcfadden(model_logit2)
```
As we can see, the second model has a larger pseudo r2 and this can be considered the better model. 

Another way to compare models is the performance package:

```{r}
compare_performance(model_logit1, model_logit2)

```


## diagnostics

Testing model assumptions and performing diagnostics is trickier for logistic regression compared to linear regression. Many diagnostics revolve around analysing residuals (difference between predictions and actial values). In the context of logistic regressio, given that the predicted values are probabilities and the actual values are either 0 and 1, it is not very clear what residuals mean.

We won't cover diagnostic tests for logistic regression here, mainly to save time. A deeper dive into diagnostics can be found here[http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/]. A more recent package used in machine learning, called "performance" can also be used for diagnostics using the `check_model` function. 



## prediction

Predicting outcomes based on logistic regression was already covered above because predicted probabilities are an easier way of interpreting coefficients. 

Here, let's predict whether certain "fake" players
will have a long career or a shorter career based on our model. 


```{r}
#define new observation
newdata = data.frame(height=c(5.0, 8.0),
                     weight=c(mean(data_nba$weight),
                              mean(data_nba$weight)))


#use model to predict value of am
predict(model_logit1, newdata, type="response")
```

The results above show that a very short player (5 feet) has a 27% probability of having a long career, whereas a very tall (8 feet) player only has a 18% probability of having a long career. Maybe taller people get injured more. 

## Mediation

Remember week X when we discussed mediation analysis based on linear regression?
Mediation is also possible for logistic regression, however, it is more complicated.
There are dedicated packages that help users apply mediation analysis following
glm(models) such as the `khb` package.

## comparing linear and logistic regression

As we have seen above, logistic regression can cause many headaches because
assessing the model fit, testing diagnostics, prediction and diagnostics are
all very complicated to perform and interpret. 

```{r}

# logistic regression
model_glm <- glm(career_long ~ height + weight + position_center + position_pg +
                      season_start,
                    data = data_nba,
                    family = binomial)
margins(model_glm, type = "response")

# same model as linear regression
model_lm <- lm(as.numeric(career_long) ~ height + weight + position_center + position_pg +
                      season_start,
                    data = data_nba)
summary(model_lm)

```

As we can see, the Average Marginal Effects calculated based on the logistic 
regression model are almost identical compared to the linear regression coefficient. 
This is why in applied research, many scholars simply use OLS regression for binary
outcomes. It is easier to apply, easier to interpret and the results are very similar
in logistic regression. 

As a rule of thumb, many researchers consider using logostoc regression when
the probabilities are extreme, so many 99% or 1% cases. In these cases, linear
regression could predict values higher than 100% or lower than 1%, which is, of course,
not possible. However, if most predicted probabilities are between 20-80%, linear
regression models have many advantages.

Example: If you’re modeling the probability of an NBA player to play more than 5 seasons, then nearly all the modeled probabilities will be between .20 and .80, and a linear probability model should fit nicely and offer a straightforward interpretation. Alternatively, consider you want to predict which player will score on average 30 points per game. This probability is likely
between .000001 and .20. In that situation, the linear model just isn’t viable, and you have to use a logistic model or other alternatives.

For a more in-depth discussion see here [https://statisticalhorizons.com/linear-vs-logistic/].



## logistic regression in Machine Learning context

Such like linear regression, logistic regression is one of the algorithms which
are used in the context of Machine Learning to predict outcomes. 

There are many alternatives to logistic regression when predicting binary outcomes such as ...,...,.... These appraoches go far beyond the scope of this course. Predicting binary outcomes is often called "classification" in machine learning.
Imagine you have 50 million pictures of animals and you want to predict whether
an image contains a cat. Being a cat or not is a binary outcome, so logistic regresison could be used in this context. 

In the following, we provide a very short introduction on how to use logistic regression in a machine learning appraoach using the `caret` package. 

```{r}
library(caret)

# Create a train and test split
set.seed(123)  # For reproducibility
train_indices <- createDataPartition(data_nba$career_long, p = 0.7, list = FALSE)
train_data <- data_nba[train_indices, ]
test_data <- data_nba[-train_indices, ]

# Create a train control object
ctrl <- trainControl(method = "none") 

# Train a linear regression model using caret
model1 <- train(
  career_long ~ height + weight + position_center + position_pg +
                      season_start,
  data = train_data,
  method = "glm",
  trControl = ctrl
)


# Make predictions on the test set
predicted_career_m1 <- predict(model1, newdata = test_data,
                               type = "prob") %>%
  mutate(predicted_class_binary=`1`)
                         
```

The above yields our predicted probabilities for career_long for every observation in the test dataset. Previously, we have used RMSE to evaluate how good the predictions were. Again, this is different for models with binary outcome models since there are no "residuals" in the classic sense. 
Instead, a common metric for assessing model performance is "accuracy". Accuracy measures which percentage of the actual values (whether someone had a long career or not) are corrected predicted by the model. Other metrics are kappa, sensitivity, specificity. The so-called "confusion matrix" compares the predicted "classes" with the actual "classes" in the training data and computes all these metrics:

```{r}
test_data$pred_career <- predicted_career_m1$predicted_class_binary
test_data <- test_data %>%
  mutate(predicted_class_binary = 
           case_when(pred_career>0.5 ~1,
                     pred_career<=0.5 ~0,
                     TRUE~ NA_real_),
         predicted_class_binary = as.factor(predicted_class_binary))

str(test_data)

confusionMatrix(data=test_data$predicted_class_binary, reference= test_data$career_long)
```

The confusion matrix shows that the model predicted short careers (0) for 668 players who actually had long careers (1). It correctely predicted the career for 2249 players. Overall, the career that players had was predicted accurately in 77% of cases. 


## Further resources
