<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 13 Logistic regression | Data Analysis with R for Social Scientists</title>
  <meta name="description" content="In this course, you will learn how to analyse data using regression in R." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content=" 13 Logistic regression | Data Analysis with R for Social Scientists" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="In this course, you will learn how to analyse data using regression in R." />
  <meta name="github-repo" content="jaspertjaden/DataAnalysisR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 13 Logistic regression | Data Analysis with R for Social Scientists" />
  
  <meta name="twitter:description" content="In this course, you will learn how to analyse data using regression in R." />
  

<meta name="author" content="Jakob Tures &amp; Jasper Tjaden" />


<meta name="date" content="2023-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pm-a.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the Authors</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#github-repo"><i class="fa fa-check"></i>GitHub Repo</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-sem.html"><a href="intro-sem.html"><i class="fa fa-check"></i><b>1</b> Introduction to Seminar</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-sem.html"><a href="intro-sem.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="eda-1.html"><a href="eda-1.html"><i class="fa fa-check"></i><b>2</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda-1.html"><a href="eda-1.html#objectives"><i class="fa fa-check"></i><b>2.1</b> Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="eda-1.html"><a href="eda-1.html#r-functions-covered-this-week"><i class="fa fa-check"></i><b>2.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="2.3" data-path="eda-1.html"><a href="eda-1.html#what-is-eda-and-why-is-it-so-important"><i class="fa fa-check"></i><b>2.3</b> What is <em>EDA</em> and why is it so important?</a></li>
<li class="chapter" data-level="2.4" data-path="eda-1.html"><a href="eda-1.html#importing-data-into-r"><i class="fa fa-check"></i><b>2.4</b> Importing data into R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="eda-1.html"><a href="eda-1.html#merge-datasets"><i class="fa fa-check"></i><b>2.4.1</b> Merge datasets</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="eda-1.html"><a href="eda-1.html#clean-dataset"><i class="fa fa-check"></i><b>2.5</b> Clean dataset</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="eda-1.html"><a href="eda-1.html#mutating-variables"><i class="fa fa-check"></i><b>2.5.1</b> Mutating variables</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="eda-1.html"><a href="eda-1.html#explore-the-complete-dataset"><i class="fa fa-check"></i><b>2.6</b> Explore the complete dataset</a></li>
<li class="chapter" data-level="2.7" data-path="eda-1.html"><a href="eda-1.html#explore-individual-variables"><i class="fa fa-check"></i><b>2.7</b> Explore individual variables</a></li>
<li class="chapter" data-level="2.8" data-path="eda-1.html"><a href="eda-1.html#moving-on"><i class="fa fa-check"></i><b>2.8</b> Moving on</a></li>
<li class="chapter" data-level="2.9" data-path="eda-1.html"><a href="eda-1.html#further-resources"><i class="fa fa-check"></i><b>2.9</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="eda-2.html"><a href="eda-2.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis - Exercise</a>
<ul>
<li class="chapter" data-level="3.1" data-path="eda-2.html"><a href="eda-2.html#what-is-r-markdown"><i class="fa fa-check"></i><b>3.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="3.2" data-path="eda-2.html"><a href="eda-2.html#creating-a-r-markdown-file"><i class="fa fa-check"></i><b>3.2</b> Creating a R Markdown file</a></li>
<li class="chapter" data-level="3.3" data-path="eda-2.html"><a href="eda-2.html#writing-in-r-markdown"><i class="fa fa-check"></i><b>3.3</b> Writing in R Markdown</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="eda-2.html"><a href="eda-2.html#document-components"><i class="fa fa-check"></i><b>3.3.1</b> Document components</a></li>
<li class="chapter" data-level="3.3.2" data-path="eda-2.html"><a href="eda-2.html#formatting"><i class="fa fa-check"></i><b>3.3.2</b> Formatting</a></li>
<li class="chapter" data-level="3.3.3" data-path="eda-2.html"><a href="eda-2.html#code-chunks"><i class="fa fa-check"></i><b>3.3.3</b> Code chunks</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="eda-2.html"><a href="eda-2.html#further-resources-1"><i class="fa fa-check"></i><b>3.4</b> Further resources</a></li>
<li class="chapter" data-level="3.5" data-path="eda-2.html"><a href="eda-2.html#eda---exercise"><i class="fa fa-check"></i><b>3.5</b> EDA - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dags-1.html"><a href="dags-1.html"><i class="fa fa-check"></i><b>4</b> DAGs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="dags-1.html"><a href="dags-1.html#objectives-1"><i class="fa fa-check"></i><b>4.1</b> Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="dags-1.html"><a href="dags-1.html#functions-covered"><i class="fa fa-check"></i><b>4.2</b> Functions Covered</a></li>
<li class="chapter" data-level="4.3" data-path="dags-1.html"><a href="dags-1.html#modelling"><i class="fa fa-check"></i><b>4.3</b> Modelling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="dags-1.html"><a href="dags-1.html#what-is-modelling"><i class="fa fa-check"></i><b>4.3.1</b> What is modelling?</a></li>
<li class="chapter" data-level="4.3.2" data-path="dags-1.html"><a href="dags-1.html#estimating-effects-vs.-prediction"><i class="fa fa-check"></i><b>4.3.2</b> Estimating effects vs. prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="dags-1.html"><a href="dags-1.html#dags"><i class="fa fa-check"></i><b>4.4</b> DAGs</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="dags-1.html"><a href="dags-1.html#directed-acyclical-graphs"><i class="fa fa-check"></i><b>4.4.1</b> Directed acyclical graphs</a></li>
<li class="chapter" data-level="4.4.2" data-path="dags-1.html"><a href="dags-1.html#patterns-of-relationships"><i class="fa fa-check"></i><b>4.4.2</b> Patterns of relationships</a></li>
<li class="chapter" data-level="4.4.3" data-path="dags-1.html"><a href="dags-1.html#adjustment-set"><i class="fa fa-check"></i><b>4.4.3</b> Adjustment set</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="dags-1.html"><a href="dags-1.html#nba-dag"><i class="fa fa-check"></i><b>4.5</b> NBA DAG</a></li>
<li class="chapter" data-level="4.6" data-path="dags-1.html"><a href="dags-1.html#resources"><i class="fa fa-check"></i><b>4.6</b> Resources</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="dags-1.html"><a href="dags-1.html#dagitty.net"><i class="fa fa-check"></i><b>4.6.1</b> dagitty.net</a></li>
<li class="chapter" data-level="4.6.2" data-path="dags-1.html"><a href="dags-1.html#how-to-use-dagitty"><i class="fa fa-check"></i><b>4.6.2</b> How to use dagitty()</a></li>
<li class="chapter" data-level="4.6.3" data-path="dags-1.html"><a href="dags-1.html#more-on-dags"><i class="fa fa-check"></i><b>4.6.3</b> More on DAGs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lin-t-1.html"><a href="lin-t-1.html"><i class="fa fa-check"></i><b>5</b> Linear Regression - Theory I: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lin-t-1.html"><a href="lin-t-1.html#objectives-2"><i class="fa fa-check"></i><b>5.1</b> Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lin-t-1.html"><a href="lin-t-1.html#what-is-linear-regression"><i class="fa fa-check"></i><b>5.2</b> What is Linear Regression</a></li>
<li class="chapter" data-level="5.3" data-path="lin-t-1.html"><a href="lin-t-1.html#examplary-research-question-data"><i class="fa fa-check"></i><b>5.3</b> Examplary research question &amp; data</a></li>
<li class="chapter" data-level="5.4" data-path="lin-t-1.html"><a href="lin-t-1.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lin-t-1.html"><a href="lin-t-1.html#regression-formula"><i class="fa fa-check"></i><b>5.4.1</b> Regression Formula</a></li>
<li class="chapter" data-level="5.4.2" data-path="lin-t-1.html"><a href="lin-t-1.html#regressing-grade-on-hours"><i class="fa fa-check"></i><b>5.4.2</b> Regressing <code>grade</code> on <code>hours</code></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lin-t-1.html"><a href="lin-t-1.html#moving-on-1"><i class="fa fa-check"></i><b>5.5</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lin-t-2.html"><a href="lin-t-2.html"><i class="fa fa-check"></i><b>6</b> Linear Regression - Theory II: Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lin-t-2.html"><a href="lin-t-2.html#objectives-3"><i class="fa fa-check"></i><b>6.1</b> Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lin-t-2.html"><a href="lin-t-2.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-additional-metric-variables"><i class="fa fa-check"></i><b>6.2.1</b> Adding additional metric variables</a></li>
<li class="chapter" data-level="6.2.2" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-dummy-variables"><i class="fa fa-check"></i><b>6.2.2</b> Adding dummy variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-categorical-variables"><i class="fa fa-check"></i><b>6.2.3</b> Adding categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lin-t-2.html"><a href="lin-t-2.html#returning-to-our-research-question"><i class="fa fa-check"></i><b>6.3</b> Returning to our research question</a></li>
<li class="chapter" data-level="6.4" data-path="lin-t-2.html"><a href="lin-t-2.html#adressing-the-uncertainty"><i class="fa fa-check"></i><b>6.4</b> Adressing the uncertainty</a></li>
<li class="chapter" data-level="6.5" data-path="lin-t-2.html"><a href="lin-t-2.html#moving-on-2"><i class="fa fa-check"></i><b>6.5</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lin-t-3.html"><a href="lin-t-3.html"><i class="fa fa-check"></i><b>7</b> Linear Regression - Theory III: Diagnostics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lin-t-3.html"><a href="lin-t-3.html#objectives-4"><i class="fa fa-check"></i><b>7.1</b> Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="lin-t-3.html"><a href="lin-t-3.html#model-fit"><i class="fa fa-check"></i><b>7.2</b> Model fit</a></li>
<li class="chapter" data-level="7.3" data-path="lin-t-3.html"><a href="lin-t-3.html#regression-diagnostics"><i class="fa fa-check"></i><b>7.3</b> Regression diagnostics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="lin-t-3.html"><a href="lin-t-3.html#linearity"><i class="fa fa-check"></i><b>7.3.1</b> Linearity</a></li>
<li class="chapter" data-level="7.3.2" data-path="lin-t-3.html"><a href="lin-t-3.html#normally-distributed-residuals"><i class="fa fa-check"></i><b>7.3.2</b> Normally distributed residuals</a></li>
<li class="chapter" data-level="7.3.3" data-path="lin-t-3.html"><a href="lin-t-3.html#homoscedasticity"><i class="fa fa-check"></i><b>7.3.3</b> Homoscedasticity</a></li>
<li class="chapter" data-level="7.3.4" data-path="lin-t-3.html"><a href="lin-t-3.html#no-overly-influential-data-points"><i class="fa fa-check"></i><b>7.3.4</b> No overly influential data points</a></li>
<li class="chapter" data-level="7.3.5" data-path="lin-t-3.html"><a href="lin-t-3.html#no-multicollinearity"><i class="fa fa-check"></i><b>7.3.5</b> No (multi)collinearity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lin-t-3.html"><a href="lin-t-3.html#returning-to-our-research-question-1"><i class="fa fa-check"></i><b>7.4</b> Returning to our research question</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="lin-t-3.html"><a href="lin-t-3.html#interactions"><i class="fa fa-check"></i><b>7.4.1</b> Interactions</a></li>
<li class="chapter" data-level="7.4.2" data-path="lin-t-3.html"><a href="lin-t-3.html#regression-diagnostics-revisited"><i class="fa fa-check"></i><b>7.4.2</b> Regression diagnostics (revisited)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lin-t-3.html"><a href="lin-t-3.html#conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
<li class="chapter" data-level="7.6" data-path="lin-t-3.html"><a href="lin-t-3.html#resources-1"><i class="fa fa-check"></i><b>7.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lin-a.html"><a href="lin-a.html"><i class="fa fa-check"></i><b>8</b> Linear Regression - Application</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lin-a.html"><a href="lin-a.html#objectives-5"><i class="fa fa-check"></i><b>8.1</b> Objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lin-a.html"><a href="lin-a.html#r-functions-covered-this-week-1"><i class="fa fa-check"></i><b>8.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="8.3" data-path="lin-a.html"><a href="lin-a.html#research-question"><i class="fa fa-check"></i><b>8.3</b> Research question</a></li>
<li class="chapter" data-level="8.4" data-path="lin-a.html"><a href="lin-a.html#simple-linear-regression-in-r"><i class="fa fa-check"></i><b>8.4</b> Simple linear regression in R</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lin-a.html"><a href="lin-a.html#interpretation"><i class="fa fa-check"></i><b>8.4.1</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lin-a.html"><a href="lin-a.html#multiple-linear-regression-in-r"><i class="fa fa-check"></i><b>8.5</b> Multiple linear regression in R</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lin-a.html"><a href="lin-a.html#interpretation-1"><i class="fa fa-check"></i><b>8.5.1</b> Interpretation</a></li>
<li class="chapter" data-level="8.5.2" data-path="lin-a.html"><a href="lin-a.html#sidenote-adding-interactions"><i class="fa fa-check"></i><b>8.5.2</b> Sidenote: Adding interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lin-a.html"><a href="lin-a.html#regression-diagnostics-1"><i class="fa fa-check"></i><b>8.6</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lin-a.html"><a href="lin-a.html#skewed-outcome-variable"><i class="fa fa-check"></i><b>8.6.1</b> Skewed outcome variable</a></li>
<li class="chapter" data-level="8.6.2" data-path="lin-a.html"><a href="lin-a.html#non-linearity"><i class="fa fa-check"></i><b>8.6.2</b> Non-linearity</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lin-a.html"><a href="lin-a.html#returning-to-our-research-question-2"><i class="fa fa-check"></i><b>8.7</b> Returning to our research question</a></li>
<li class="chapter" data-level="8.8" data-path="lin-a.html"><a href="lin-a.html#moving-on-3"><i class="fa fa-check"></i><b>8.8</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lin-e.html"><a href="lin-e.html"><i class="fa fa-check"></i><b>9</b> Linear Regression - Exercise</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lin-e.html"><a href="lin-e.html#linear-regression---exercise"><i class="fa fa-check"></i><b>9.1</b> Linear Regression - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pm-t.html"><a href="pm-t.html"><i class="fa fa-check"></i><b>10</b> Prediction</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pm-t.html"><a href="pm-t.html#objectives-6"><i class="fa fa-check"></i><b>10.1</b> Objectives</a></li>
<li class="chapter" data-level="10.2" data-path="pm-t.html"><a href="pm-t.html#r-functions-covered-this-week-2"><i class="fa fa-check"></i><b>10.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="10.3" data-path="pm-t.html"><a href="pm-t.html#making-predictions"><i class="fa fa-check"></i><b>10.3</b> Making predictions</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="pm-t.html"><a href="pm-t.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>10.3.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="pm-t.html"><a href="pm-t.html#why-predict-at-all"><i class="fa fa-check"></i><b>10.3.2</b> Why predict at all?</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="pm-t.html"><a href="pm-t.html#making-better-predictions"><i class="fa fa-check"></i><b>10.4</b> Making better predictions</a></li>
<li class="chapter" data-level="10.5" data-path="pm-t.html"><a href="pm-t.html#making-even-better-predictions"><i class="fa fa-check"></i><b>10.5</b> Making even better predictions?</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pm-t.html"><a href="pm-t.html#comparing-models"><i class="fa fa-check"></i><b>10.5.1</b> Comparing models</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="pm-t.html"><a href="pm-t.html#predicting-hypotheticals"><i class="fa fa-check"></i><b>10.6</b> Predicting hypotheticals</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="pm-t.html"><a href="pm-t.html#adressing-uncertainty"><i class="fa fa-check"></i><b>10.6.1</b> Adressing uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="pm-t.html"><a href="pm-t.html#further-resources-2"><i class="fa fa-check"></i><b>10.7</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>11</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ml.html"><a href="ml.html#objectives-7"><i class="fa fa-check"></i><b>11.1</b> Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="ml.html"><a href="ml.html#r-functions-covered-this-week-3"><i class="fa fa-check"></i><b>11.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="11.3" data-path="ml.html"><a href="ml.html#intro-to-machine-learning"><i class="fa fa-check"></i><b>11.3</b> Intro to Machine Learning</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="ml.html"><a href="ml.html#training--and-testset"><i class="fa fa-check"></i><b>11.3.1</b> Training- and testset</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ml.html"><a href="ml.html#our-baseline-model"><i class="fa fa-check"></i><b>11.4</b> Our baseline model</a></li>
<li class="chapter" data-level="11.5" data-path="ml.html"><a href="ml.html#setting-up-tidymodels"><i class="fa fa-check"></i><b>11.5</b> Setting up tidymodels</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="ml.html"><a href="ml.html#splitting-the-data"><i class="fa fa-check"></i><b>11.5.1</b> Splitting the data</a></li>
<li class="chapter" data-level="11.5.2" data-path="ml.html"><a href="ml.html#defining-the-recipe"><i class="fa fa-check"></i><b>11.5.2</b> Defining the recipe</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="ml.html"><a href="ml.html#linear-regression"><i class="fa fa-check"></i><b>11.6</b> Linear regression</a></li>
<li class="chapter" data-level="11.7" data-path="ml.html"><a href="ml.html#random-forest"><i class="fa fa-check"></i><b>11.7</b> Random forest</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="ml.html"><a href="ml.html#tuning-the-forest"><i class="fa fa-check"></i><b>11.7.1</b> Tuning the forest</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="ml.html"><a href="ml.html#last-fit"><i class="fa fa-check"></i><b>11.8</b> Last fit</a></li>
<li class="chapter" data-level="11.9" data-path="ml.html"><a href="ml.html#going-forward"><i class="fa fa-check"></i><b>11.9</b> Going forward</a></li>
<li class="chapter" data-level="11.10" data-path="ml.html"><a href="ml.html#further-resources-3"><i class="fa fa-check"></i><b>11.10</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pm-a.html"><a href="pm-a.html"><i class="fa fa-check"></i><b>12</b> Prediction &amp; Machine Learning - Exercise</a>
<ul>
<li class="chapter" data-level="12.1" data-path="pm-a.html"><a href="pm-a.html#prediction-machine-learning---exercise"><i class="fa fa-check"></i><b>12.1</b> Prediction &amp; Machine Learning - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="log-est.html"><a href="log-est.html"><i class="fa fa-check"></i><b>13</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="13.1" data-path="log-est.html"><a href="log-est.html#objectives-8"><i class="fa fa-check"></i><b>13.1</b> Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="log-est.html"><a href="log-est.html#functions-covered-in-this-week"><i class="fa fa-check"></i><b>13.2</b> Functions covered in this week</a></li>
<li class="chapter" data-level="13.3" data-path="log-est.html"><a href="log-est.html#what-is-logistic-regression"><i class="fa fa-check"></i><b>13.3</b> What is logistic regression?</a></li>
<li class="chapter" data-level="13.4" data-path="log-est.html"><a href="log-est.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>13.4</b> Logistic regression in R</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="log-est.html"><a href="log-est.html#preparing-the-outcome-variable"><i class="fa fa-check"></i><b>13.4.1</b> Preparing the outcome variable</a></li>
<li class="chapter" data-level="13.4.2" data-path="log-est.html"><a href="log-est.html#running-a-logistic-regression-in-r"><i class="fa fa-check"></i><b>13.4.2</b> Running a logistic regression in R</a></li>
<li class="chapter" data-level="13.4.3" data-path="log-est.html"><a href="log-est.html#interpretation-2"><i class="fa fa-check"></i><b>13.4.3</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="log-est.html"><a href="log-est.html#building-a-better-model"><i class="fa fa-check"></i><b>13.5</b> Building a better model</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="log-est.html"><a href="log-est.html#model-fit-1"><i class="fa fa-check"></i><b>13.5.1</b> Model fit</a></li>
<li class="chapter" data-level="13.5.2" data-path="log-est.html"><a href="log-est.html#interpretation-3"><i class="fa fa-check"></i><b>13.5.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="log-est.html"><a href="log-est.html#additional-pointers"><i class="fa fa-check"></i><b>13.6</b> Additional pointers</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="log-est.html"><a href="log-est.html#predicting-hypotheticals-1"><i class="fa fa-check"></i><b>13.6.1</b> Predicting hypotheticals</a></li>
<li class="chapter" data-level="13.6.2" data-path="log-est.html"><a href="log-est.html#diagnostics"><i class="fa fa-check"></i><b>13.6.2</b> Diagnostics</a></li>
<li class="chapter" data-level="13.6.3" data-path="log-est.html"><a href="log-est.html#comparing-linear-and-logistic-regression"><i class="fa fa-check"></i><b>13.6.3</b> Comparing linear and logistic regression</a></li>
<li class="chapter" data-level="13.6.4" data-path="log-est.html"><a href="log-est.html#logistic-regression-in-machine-learning-context"><i class="fa fa-check"></i><b>13.6.4</b> Logistic regression in Machine Learning context</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="log-est.html"><a href="log-est.html#further-resources-4"><i class="fa fa-check"></i><b>13.7</b> Further resources</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis with R for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="log-est" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number"> 13</span> Logistic regression<a href="log-est.html#log-est" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>For most of this course we were concerned with modelling our data using
linear regression. There are many
other modelling approaches that all have their pros and cons and that
are suited to different empirical problems and types of data. While we
do not have the time and space to tackle them all, in this session we
want to introduce one alternative technique, namely <em>logistic regression</em>.</p>
<div id="objectives-8" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Objectives<a href="log-est.html#objectives-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="objectives">
<ul>
<li>Understand the underlying concept of logistic regression</li>
<li>Apply logistic regression and interpret the results</li>
<li>Assessing the model fit for logistic regressions</li>
</ul>
</div>
</div>
<div id="functions-covered-in-this-week" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Functions covered in this week<a href="log-est.html#functions-covered-in-this-week" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="functions">
<ul>
<li><code>glm()</code> : Base R function used to fit a generalized linear model to
the data. It takes a formula that specifies the dependent and
independent variables, a data frame that contains the variables, and
a family argument that specifies the type of model, such as “binomial”
for logistic regression. It returns a model object that can be used
for further analysis.</li>
<li>Converting Logits into probabilities
<ul>
<li><code>ggpredict()</code> : Function from the <strong>ggeffects</strong> package. It
is used to compute predicted probabilities and their confidence intervals
for different types of models in R. It takes a model object and one or
more arguments that specify which variables and which of their values
to predict for.</li>
<li><code>margins()</code> : Function from the <strong>margins</strong> package. It is
used to compute average marginal effects for various types of models in R.
It takes a model object and other arguments that specify the
variables of interest, the type of marginal effects (response or
link), etc.</li>
</ul></li>
<li>Assessing the model fit
<ul>
<li><code>r2_mcfadden()</code> : Function from the <strong>performance</strong> package.
It is used to compute McFadden’s pseudo <span class="math inline">\(R^2\)</span> for generalized
linear models in R. It takes a model object and returns McFadden’s <span class="math inline">\(R^2\)</span>
and adjusted <span class="math inline">\(R^2\)</span> values.</li>
<li><code>model_performance()</code> &amp; <code>compare_performance()</code>: Functions from the
<strong>performance</strong> package. Used to compute various indices of model
performance for different types of models in R. It takes a model object and
other arguments that specify which indices to include or exclude, among them
AIC, BIC, RMSE, and several more. <code>compare_performance()</code> does the same for
multiple models at once.</li>
</ul></li>
</ul>
</div>
</div>
<div id="what-is-logistic-regression" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> What is logistic regression?<a href="log-est.html#what-is-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression in many ways is similar to linear regression. We
will not go into the mathematical details of it all, as we have done for
linear regression, and rather focus on the two main differences that are
of central importance to us when starting out using logistic regression
models.</p>
<p>The first main difference is that logistic regression is tailored to
binary outcome variables while linear regression is used to model
continuous variables. Binary variables only have two levels.
For example includes dummy variables, which can only have the values <span class="math inline">\(1\)</span> or
<span class="math inline">\(0\)</span>. Either an observation has a certain characteristic or it does not.
If we think about our NBA data, all position variables we created are of
this type. Other examples would be the (binary) gender of an observation, with
the two levels “male” and “female”, or, returning to our NBA data, the
playing hand, either “left” or “right”.</p>
<p>The second main difference is the estimation method used in computing
the model. Again, we will not go into the details of it all, but it is
important to get a sense of the difference of how both approaches fit
their models. Rather than fitting a straight line through data points as
in linear regression, logistic regression fits a “s-shaped” line. This
line represents the probability of an observation being in either
category. As you can see below the left and right ends of this line get
progressively flatter approaching <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> respectively. This means
that the probabilities the model assumes can never fall below <span class="math inline">\(0\)</span> or
rise above <span class="math inline">\(1\)</span>. This is important as we can be <span class="math inline">\(0\%\)</span> to <span class="math inline">\(100\%\)</span> certain
that an observation has a certain value but we can never be <span class="math inline">\(-20%\)</span> or
<span class="math inline">\(120\%\)</span> certain, as this would violate the mathematics of probability.</p>
<div class="float">
<img src="linear-vs-logistic.png" alt="Scatterplot with straight line and s-shaped line." />
<div class="figcaption"><strong>Scatterplot with straight line and s-shaped
line.</strong></div>
</div>
<p>As the line is fitted differently, the estimation method also has to be
different. While linear regression uses ordinary least squares (OLS),
i.e. using the distance from each observation to the regression line,
logistic regression uses the <em>maximum likelihood</em> function to
approximate the s-shape curve which best fits the data. Basically the
algorithm “tries out” different s-shaped curves by trying different
coefficients for the independent variables in the model. For each
proposed line the likelihood of it describing the actual observed values
can be calculated. In each iteration of the algorithm the coefficients
are changed in a way that tries to increase the likelihood until one
curve is found that maximises this value, that is why it is called
<em>maximum likelihood</em> function. The resulting curve and its coefficients
are the best fit when computing a logistic regression.</p>
<p>See this
<a href="https://www.youtube.com/watch?v=yIYKR4sgzI8&amp;list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe&amp;ab_channel=StatQuestwithJoshStarmer">youtube</a>
playlist on logistic regression for more details.</p>
</div>
<div id="logistic-regression-in-r" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> Logistic regression in R<a href="log-est.html#logistic-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Having a map of the basics, we can now start applying a logistic
regression to our NBA data. Let us load the prepared data first.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="log-est.html#cb192-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb192-2"><a href="log-est.html#cb192-2" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;../datasets/nba/data_nba.RData&quot;</span>)</span></code></pre></div>
<p>Over the last sessions we were interested in the relationship between
the points scored and the salary received. We found out that there is a
positive relationship between those measures. But we also saw, that the
range of possible values for <code>salary</code> for each value of <code>career_PTS</code> was
still very broad. In other terms, while the increase in points per game
could in part explain the increase in monetary reward, there were still
some wages that were inexplicably high. We will here use a logistic
regression and try to shed some more light on these extremely high
salaries and their relationship to the points scored as our central
measure of performance. Does scoring more points increase the chance of
being among the high earners and to what extent?</p>
<div id="preparing-the-outcome-variable" class="section level3 hasAnchor" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Preparing the outcome variable<a href="log-est.html#preparing-the-outcome-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To approach this we have to change our outcome variable. Instead of
using the actual salary in dollars, we are now interested in the
question if an observation belongs to the high earners or not. We have
to build a variable that reflects this. But what constitutes a <em>high</em>
earner? This is a definition we have to make and that is necessarily
somewhat arbitrary. As we lack any theoretical claims of what
constitutes a high earner and we do not want to pick a number “out of
the blue”, one approach would be to inspect the distribution of <code>salary</code>
and choose a sensible cutoff based on it.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="log-est.html#cb193-1" tabindex="-1"></a><span class="fu">summary</span>(data_nba<span class="sc">$</span>salary) </span></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##     2706   947907  2240000  4072633  5408700 34682550</code></pre>
<p>We could for example state that belonging to the top <span class="math inline">\(25\%\)</span> constitutes
being a high earner. The salary value that is the cutoff between the
lower <span class="math inline">\(75\%\)</span> and the higher <span class="math inline">\(25\%\)</span> is the third quartile, i.e.
<span class="math inline">\(5,408,700\$\)</span>. We can now build a new binary variable that discerns both
groups. Below we assign the value <code>1</code> to every observation that has a
salary above the threshold and <code>0</code> to everyone else. We also save the
new variable as a <em>factor</em> variable, the representation of categorical
variables in R. A binary variable is simply a categorical variable with
two categories.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="log-est.html#cb195-1" tabindex="-1"></a>data_nba <span class="ot">&lt;-</span> data_nba <span class="sc">%&gt;%</span></span>
<span id="cb195-2"><a href="log-est.html#cb195-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">high_earner =</span> </span>
<span id="cb195-3"><a href="log-est.html#cb195-3" tabindex="-1"></a>           <span class="fu">case_when</span>(salary <span class="sc">&gt;</span> <span class="dv">5408700</span> <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb195-4"><a href="log-est.html#cb195-4" tabindex="-1"></a>                     salary <span class="sc">&lt;=</span> <span class="dv">5408700</span> <span class="sc">~</span> <span class="dv">0</span>),</span>
<span id="cb195-5"><a href="log-est.html#cb195-5" tabindex="-1"></a>         <span class="at">high_earner =</span> <span class="fu">as.factor</span>(high_earner))</span></code></pre></div>
</div>
<div id="running-a-logistic-regression-in-r" class="section level3 hasAnchor" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Running a logistic regression in R<a href="log-est.html#running-a-logistic-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Running a logistic regression in practice is not that different to
running a linear regression; but while we have used the <code>lm()</code> function
for a linear regression, “lm” stands for “linear model”, we have to use
something else for a logistic regression.</p>
<p>Logistic regression is part of the family of <em>generalised linear models</em>,
which for example also includes the Poisson regression for count data or
the ridge and lasso regression which apply feature selection. The
function <code>glm()</code>, short for generalised linear model, gives us access
to several models from this family, including logistic regression. We
have to specify our model formula in the same way we did with <code>lm()</code> and
the data to be used. The additional argument <code>family = binomial</code>
specifies that we want to run a logistic regression. See <code>?family</code> for
an overview of other glm models and how to choose them.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="log-est.html#cb196-1" tabindex="-1"></a>model_logit_1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(high_earner <span class="sc">~</span> career_PTS,</span>
<span id="cb196-2"><a href="log-est.html#cb196-2" tabindex="-1"></a>                     <span class="at">data =</span> data_nba,</span>
<span id="cb196-3"><a href="log-est.html#cb196-3" tabindex="-1"></a>                     <span class="at">family =</span> binomial)</span></code></pre></div>
</div>
<div id="interpretation-2" class="section level3 hasAnchor" number="13.4.3">
<h3><span class="header-section-number">13.4.3</span> Interpretation<a href="log-est.html#interpretation-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using <code>summary()</code> on the model object, we see a familiar output.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="log-est.html#cb197-1" tabindex="-1"></a><span class="fu">summary</span>(model_logit_1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = high_earner ~ career_PTS, family = binomial, data = data_nba)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.7513  -0.6415  -0.4421  -0.2395   2.4894  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.537508   0.069377  -50.99   &lt;2e-16 ***
## career_PTS   0.242502   0.005948   40.77   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 10929.8  on 9727  degrees of freedom
## Residual deviance:  8653.8  on 9726  degrees of freedom
## AIC: 8657.8
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The model fit block, the last few lines of the output, actually do look
different to what we have seen before. The last line tells us how many
iterations the algorithm used to come to a solution, in this case <span class="math inline">\(5\)</span>.
The remainder of the model fit block gives us measures on the quality of
the model, but we will use a different measure for this later on. So let
us focus on the coefficients for now.</p>
<div id="logits" class="section level4 hasAnchor" number="13.4.3.1">
<h4><span class="header-section-number">13.4.3.1</span> Logits<a href="log-est.html#logits" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>While the coefficients are presented in exactly the same way as for a linear
regression, the interpretation is completely different. In a linear
regression, coefficients are interpreted as the change of the value of
<span class="math inline">\(y\)</span> for a one unit increase in <span class="math inline">\(x\)</span>. But what is our <span class="math inline">\(y\)</span> variable now? It
is not a metric variable that can increase or decrease on its scale, it
is a binary variable that can, in our case, have the values <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>
and nothing in between. So what the coefficients in a logistic
regression show us is the change in the probability that <span class="math inline">\(y = 1\)</span> for a
one unit increase <span class="math inline">\(x\)</span>. This change is shown as <em>log odds</em> or <em>logits</em>.</p>
<p>The intercept tells us that the basic probability of being a high earner
is <span class="math inline">\(-3.537508\)</span> expressed in log odds, and the coefficient for <code>career_PTS</code>
tells us that for every point scored these log odds increase by
<span class="math inline">\(0.242502\)</span>. Is this helpful? Not really. Log odds are hard to interpret
directly. All we can actually do with them is to interpret the sign. Is
an effect positive or negative? If we had multiple independent
variables, we could also compare the size of the effects to each other.
A larger log odd means a larger change in probability of <span class="math inline">\(y = 1\)</span>. While
we can do this, we have to be careful. We can say that one effect is
larger but it is hard to state by how much. As log odds are on a
logarithmic scale, we can not interpret a log odd of <span class="math inline">\(0.5\)</span> as being
twice as high as <span class="math inline">\(0.25\)</span>.</p>
<p>As log odds are notoriously hard to deal with directly, in practice no
one really interprets them beyond the direction of an effect. But there
are several different ways to transform log odds into measures that are
more meaningful to us.</p>
</div>
<div id="odds-ratios" class="section level4 hasAnchor" number="13.4.3.2">
<h4><span class="header-section-number">13.4.3.2</span> Odds Ratios<a href="log-est.html#odds-ratios" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One of these approaches is inspecting the <em>odds ratios</em>. As log odds are
actually the logarithmised odds ratios, we can transform them into odds
ratios using <code>exp()</code>. Below we first extract the coefficients from the
model object using <code>coef()</code> on it and then take the exponential of the
coefficients transforming them into odds ratios.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="log-est.html#cb199-1" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(model_logit_1))</span></code></pre></div>
<pre><code>## (Intercept)  career_PTS 
##  0.02908571  1.27443331</code></pre>
<p>The odds ratio for <code>career_PTS</code> of <span class="math inline">\(1.27\)</span> tells us that for each
additional point scored the odds of being a high earner increase by this
value. A odds ratio of <span class="math inline">\(1\)</span> would indicate no change. The odds change by
a factor of <span class="math inline">\(1\)</span>, i.e. not at all. Odds ratios below <span class="math inline">\(1\)</span> indicate a
decrease, above <span class="math inline">\(1\)</span> an increase.</p>
<p>There is an alternative <code>tidyverse</code> way to get odds ratios using the
<code>broom</code> package. It neatly converts the output into a tibble which makes
it easier to work with.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="log-est.html#cb201-1" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb201-2"><a href="log-est.html#cb201-2" tabindex="-1"></a></span>
<span id="cb201-3"><a href="log-est.html#cb201-3" tabindex="-1"></a>model_logit_1 <span class="sc">%&gt;%</span> </span>
<span id="cb201-4"><a href="log-est.html#cb201-4" tabindex="-1"></a>      <span class="fu">tidy</span>(<span class="at">exp =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   0.0291   0.0694      -51.0       0
## 2 career_PTS    1.27     0.00595      40.8       0</code></pre>
</div>
<div id="probabilities" class="section level4 hasAnchor" number="13.4.3.3">
<h4><span class="header-section-number">13.4.3.3</span> Probabilities<a href="log-est.html#probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recently, it has become more common to convert the coefficients into
probabilities, as these can be easily interpreted in a meaningful way.
There are various packages and various options and
no clear agreement on what is the preferred output. Documentation for
the <code>ggeffects</code>
<a href="https://strengejacke.github.io/ggeffects/index.html">package</a> and the
<code>margins</code> <a href="https://thomasleeper.com/margins/">package</a> capture this
discussion in detail. In the following, we will show you how to compute
<em>predicted probabilities</em> and <em>average marginal effects</em> (AME).</p>
<p>Predicted probabilities are very straightforward. Remember what we are
interested in when running a logistic regression, the probability of
<span class="math inline">\(y = 1\)</span> given the values of one or multiple independent variables.
Predicted probabilities give us just that. While we could compute them
manually, we spare you the formula this time and instead use
<code>ggpredict()</code> from the <code>ggeffects</code> package. All we have to give the
function is the model object and the names of one or multiple
independent variables we are interested in.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="log-est.html#cb203-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggeffects&quot;</span>)</span>
<span id="cb203-2"><a href="log-est.html#cb203-2" tabindex="-1"></a></span>
<span id="cb203-3"><a href="log-est.html#cb203-3" tabindex="-1"></a><span class="fu">ggpredict</span>(model_logit_1, <span class="st">&quot;career_PTS&quot;</span>)</span></code></pre></div>
<pre><code>## # Predicted probabilities of high_earner
## 
## career_PTS | Predicted |       95% CI
## -------------------------------------
##          0 |      0.03 | [0.02, 0.03]
##          5 |      0.09 | [0.08, 0.10]
##         10 |      0.25 | [0.24, 0.26]
##         15 |      0.52 | [0.51, 0.54]
##         20 |      0.79 | [0.77, 0.81]
##         25 |      0.93 | [0.91, 0.94]
##         30 |      0.98 | [0.97, 0.98]</code></pre>
<p>The output gives us the predicted probability of being a high earner for
a selection of values for <code>career_PTS</code>. For example for a player that
scores <span class="math inline">\(15\)</span> points on average the model assumes a probability of <span class="math inline">\(52\%\)</span>
of being a high earner, basically a slightly biased coin flip. Looking at
particularly low and high scorers, we can see that the probability dramatically
de- or increases. A player who scores <span class="math inline">\(10\)</span> points per game only has a
probability of being a high earner of <span class="math inline">\(25\%\)</span> while a player who scores
<span class="math inline">\(20\)</span> already has a very high probability of <span class="math inline">\(79\%\)</span>. The nearer we get
to the extreme values of <code>career_PTS</code>, the less dramatic the in- and
decrease gets. The relationship between points per game and the
predicted probability is non-linear!</p>
<p>We can visualise this using <code>ggplot()</code>. Here we also tell <code>ggpredict()</code>
to give us predictions for all values of <code>career_PTS</code> between <span class="math inline">\(0\)</span> and
<span class="math inline">\(30\)</span>.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="log-est.html#cb205-1" tabindex="-1"></a><span class="fu">ggpredict</span>(model_logit_1, <span class="st">&quot;career_PTS [0:30]&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb205-2"><a href="log-est.html#cb205-2" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb205-3"><a href="log-est.html#cb205-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, predicted)) <span class="sc">+</span></span>
<span id="cb205-4"><a href="log-est.html#cb205-4" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb205-5"><a href="log-est.html#cb205-5" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/predicted_probabilities_plot-1.png" width="672" /></p>
<p>This looks familiar. It is the s-curve fitted to the data in a logistic
regression we have seen above. The closer we get to the extreme values
for <code>career_PTS</code>, the flatter the line gets, nearing <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>
respectively but never dipping above or below these values.</p>
<p>Modelling the relationship between independent and dependent variables
as non-linear is inherent to logistic regressions and also one of its
advantages. But this also means that it is not straightforward to
describe the change in probability with one coherent measure. One
popular way to approach this is calculating the
<em>average marginal effect</em> (AME).
As we have seen, the slope of the s-shape curved differs
over its range. In the middle it is steeper, at the fringes it gets less
steep. AMEs give us the average slope over its complete range. We can
interpret this as average change in probability for <span class="math inline">\(y\)</span> occurring for a
one unit increase in <span class="math inline">\(x\)</span>.</p>
<p>The function <code>margins()</code> from the <code>margins</code> package can be used to compute AMEs.
Besides the model object, we also have to specify which variables we are
interested in. <code>summary()</code> is used to give us a nicely formatted output with
additional information on the uncertainty of the calculated AME.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="log-est.html#cb206-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;margins&quot;</span>)</span>
<span id="cb206-2"><a href="log-est.html#cb206-2" tabindex="-1"></a></span>
<span id="cb206-3"><a href="log-est.html#cb206-3" tabindex="-1"></a><span class="fu">margins</span>(model_logit_1, <span class="at">variables =</span> <span class="st">&quot;career_PTS&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb206-4"><a href="log-est.html#cb206-4" tabindex="-1"></a>  <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>##      factor    AME     SE       z      p  lower  upper
##  career_PTS 0.0344 0.0006 59.7946 0.0000 0.0333 0.0355</code></pre>
<p>The AME for <code>career_PTS</code> is <span class="math inline">\(0.0344\)</span>. This means that for every
additional point per game, the NBA players are about <span class="math inline">\(3.5\)</span> percentage
points more likely to be among the high earners.</p>
<p>What do we learn from all of this? There is a strong positive
relationship between points scored and the chances of being a high
earner. Starting from <span class="math inline">\(15\)</span> points per game on average the probability
rises above <span class="math inline">\(50\%\)</span>. Really high scorers, maybe scoring <span class="math inline">\(20\)</span> and more,
have really high chances of being a high earner, low scorers have a
really low chance.</p>
<p>This makes sense, but up till now, we have not included other control
variables in our model. Maybe these will change the relationship we have
observed.</p>
</div>
</div>
</div>
<div id="building-a-better-model" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> Building a better model<a href="log-est.html#building-a-better-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To keep things simple here, we can assume the data generating process
for being a high earner to be the same as for the salary in general.
This would imply that we have to include the position a player occupies
as our sole control variable.</p>
<p>We can do this in <code>glm()</code> the same way as we have done in <code>lm()</code>.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="log-est.html#cb208-1" tabindex="-1"></a>model_logit_2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(high_earner <span class="sc">~</span> career_PTS <span class="sc">+</span> position_center <span class="sc">+</span> position_sf <span class="sc">+</span>  position_pf <span class="sc">+</span> position_sg <span class="sc">+</span> position_pg,</span>
<span id="cb208-2"><a href="log-est.html#cb208-2" tabindex="-1"></a>                     <span class="at">data =</span> data_nba,</span>
<span id="cb208-3"><a href="log-est.html#cb208-3" tabindex="-1"></a>                     <span class="at">family =</span> binomial)</span>
<span id="cb208-4"><a href="log-est.html#cb208-4" tabindex="-1"></a></span>
<span id="cb208-5"><a href="log-est.html#cb208-5" tabindex="-1"></a><span class="fu">summary</span>(model_logit_2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = high_earner ~ career_PTS + position_center + position_sf + 
##     position_pf + position_sg + position_pg, family = binomial, 
##     data = data_nba)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.7356  -0.6312  -0.4305  -0.1778   2.3902  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -4.101017   0.106782 -38.406  &lt; 2e-16 ***
## career_PTS       0.253238   0.006216  40.739  &lt; 2e-16 ***
## position_center  0.866756   0.086616  10.007  &lt; 2e-16 ***
## position_sf      0.234690   0.075762   3.098  0.00195 ** 
## position_pf      0.193818   0.070319   2.756  0.00585 ** 
## position_sg     -0.038478   0.072682  -0.529  0.59652    
## position_pg      0.135863   0.088297   1.539  0.12388    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 10929.8  on 9727  degrees of freedom
## Residual deviance:  8432.7  on 9721  degrees of freedom
## AIC: 8446.7
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>We still see a clear positive effect of points scored, the chances of
being a high earner rising with each additional point. Please note that while
the coefficient is somewhat larger compared to our first model, we can
not directly compare coefficients between two logistic models. For an in depth
discussion, see the paper by Carina Mood as well as the proposed solution by
Karlson, Holm &amp; Breen (KHB) under “Further resources” below. This is of special
importance when conducting mediation analysis with logistic models, as this is
all about comparing coefficients between models. The <em>KHB method</em> is
also implemented in R, for example in the <code>khb</code> package.</p>
<p>We also see that centers have considerably higher chances for being high earners
while the chances for short and power forwards are also increased but
substantially less so. The effect for point guards is also positive but
smaller and not statistically significant. The effect for shooting
guards is very near to <span class="math inline">\(0\)</span> and also not statistically significant.</p>
<div id="model-fit-1" class="section level3 hasAnchor" number="13.5.1">
<h3><span class="header-section-number">13.5.1</span> Model fit<a href="log-est.html#model-fit-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>But is our second model “better” compared to the first one? Does it
better describe the variation in the outcome variable by the values of
the independent variables? For this question we have to turn to measures
of model fit again.</p>
<p>For linear regression, a popular measure for the model fit is the <span class="math inline">\(R^2\)</span>
which reports the “percentage of variance in <span class="math inline">\(y\)</span> which is explained by
the model”. For logistic regression, we can not directly compute <span class="math inline">\(R^2\)</span>.
There are dozens of ways to calculate different approximations to <span class="math inline">\(R^2\)</span>
for logistic regressions with no consensus on what the best measure is.
It’s complicated. A more in-depth technical discussion can be found
<a href="https://www.youtube.com/watch?v=xxFYro8QuXA&amp;ab_channel=StatQuestwithJoshStarmer">here</a>.</p>
<p>For this session we will again keep it simple and focus on McFadden’s pseudo
<span class="math inline">\(R^2\)</span> without going into the details of its computation. All we have to know
for now is that the measure can not be interpreted as “percentage of
variance” explained but that we can still use it as a tool for comparison.
Larger values are better than smaller values.</p>
<p>One way to compute model performance measures is the <code>performance</code>
package. The function <code>r2_mcfadden()</code> returns McFadden’s pseudo <span class="math inline">\(R^2\)</span>
for our model. Note that there are other measures available from the
package. For starters, <code>model_performance()</code> returns a number of them
while <code>compare_performance()</code> returns them for multiple models at the
same time in one table.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="log-est.html#cb210-1" tabindex="-1"></a><span class="fu">library</span>(performance)</span>
<span id="cb210-2"><a href="log-est.html#cb210-2" tabindex="-1"></a></span>
<span id="cb210-3"><a href="log-est.html#cb210-3" tabindex="-1"></a><span class="fu">r2_mcfadden</span>(model_logit_1)</span></code></pre></div>
<pre><code>## # R2 for Generalized Linear Regression
##        R2: 0.208
##   adj. R2: 0.208</code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="log-est.html#cb212-1" tabindex="-1"></a><span class="fu">r2_mcfadden</span>(model_logit_2)</span></code></pre></div>
<pre><code>## # R2 for Generalized Linear Regression
##        R2: 0.228
##   adj. R2: 0.228</code></pre>
<p>The measure is higher for our second model. As we can not really
interpret McFadden’s pseudo <span class="math inline">\(R^2\)</span> beyond “the more, the better”, we can
conclude that the second model performs better compared to the first.</p>
<p>We have to keep in mind that the same caveats apply as with <span class="math inline">\(R^2\)</span> in
linear regressions. Just aiming for a higher model fit does not
necessarily lead to a more correct model. Again we need to invest the
time into thorough thinking and making well founded and sensible
decisions to achieve an estimate for our effect of interest with as
little bias as possible.</p>
</div>
<div id="interpretation-3" class="section level3 hasAnchor" number="13.5.2">
<h3><span class="header-section-number">13.5.2</span> Interpretation<a href="log-est.html#interpretation-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To get a better understanding of the results from model 2, we should
again transform the coefficient for <code>career_PTS</code> into probabilities,
separately for centers and point guards. We can again use <code>ggpredict()</code>
to compute the predicted probabilities but this time we have to also
supply values for our control variables using the argument <code>control</code>. If
we do not do this, all other variables are set to their mean values.
This can make sense for metric variables, representing the average,
but it does not make sense for binary variables. What would
<code>position_center = 0.31</code> mean? Nothing really as a player can either
play as a center or not, i.e. have the value <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> and nothing in
between.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="log-est.html#cb214-1" tabindex="-1"></a><span class="fu">ggpredict</span>(model_logit_2, <span class="st">&quot;career_PTS&quot;</span>,</span>
<span id="cb214-2"><a href="log-est.html#cb214-2" tabindex="-1"></a>          <span class="at">condition =</span> <span class="fu">c</span>(</span>
<span id="cb214-3"><a href="log-est.html#cb214-3" tabindex="-1"></a>            <span class="at">position_center =</span> <span class="dv">1</span>,</span>
<span id="cb214-4"><a href="log-est.html#cb214-4" tabindex="-1"></a>            <span class="at">position_sf =</span> <span class="dv">0</span>,</span>
<span id="cb214-5"><a href="log-est.html#cb214-5" tabindex="-1"></a>            <span class="at">position_pf =</span> <span class="dv">0</span>,</span>
<span id="cb214-6"><a href="log-est.html#cb214-6" tabindex="-1"></a>            <span class="at">position_sg =</span> <span class="dv">0</span>,</span>
<span id="cb214-7"><a href="log-est.html#cb214-7" tabindex="-1"></a>            <span class="at">position_pg =</span> <span class="dv">0</span></span>
<span id="cb214-8"><a href="log-est.html#cb214-8" tabindex="-1"></a>          ))</span></code></pre></div>
<pre><code>## # Predicted probabilities of high_earner
## 
## career_PTS | Predicted |       95% CI
## -------------------------------------
##          0 |      0.04 | [0.03, 0.04]
##          5 |      0.12 | [0.11, 0.14]
##         10 |      0.33 | [0.30, 0.36]
##         15 |      0.64 | [0.60, 0.67]
##         20 |      0.86 | [0.84, 0.88]
##         25 |      0.96 | [0.95, 0.97]
##         30 |      0.99 | [0.98, 0.99]</code></pre>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="log-est.html#cb216-1" tabindex="-1"></a><span class="fu">ggpredict</span>(model_logit_2, <span class="st">&quot;career_PTS&quot;</span>,</span>
<span id="cb216-2"><a href="log-est.html#cb216-2" tabindex="-1"></a>          <span class="at">condition =</span> <span class="fu">c</span>(</span>
<span id="cb216-3"><a href="log-est.html#cb216-3" tabindex="-1"></a>            <span class="at">position_center =</span> <span class="dv">0</span>,</span>
<span id="cb216-4"><a href="log-est.html#cb216-4" tabindex="-1"></a>            <span class="at">position_sf =</span> <span class="dv">0</span>,</span>
<span id="cb216-5"><a href="log-est.html#cb216-5" tabindex="-1"></a>            <span class="at">position_pf =</span> <span class="dv">0</span>,</span>
<span id="cb216-6"><a href="log-est.html#cb216-6" tabindex="-1"></a>            <span class="at">position_sg =</span> <span class="dv">0</span>,</span>
<span id="cb216-7"><a href="log-est.html#cb216-7" tabindex="-1"></a>            <span class="at">position_pg =</span> <span class="dv">1</span></span>
<span id="cb216-8"><a href="log-est.html#cb216-8" tabindex="-1"></a>          ))</span></code></pre></div>
<pre><code>## # Predicted probabilities of high_earner
## 
## career_PTS | Predicted |       95% CI
## -------------------------------------
##          0 |      0.02 | [0.02, 0.02]
##          5 |      0.06 | [0.05, 0.07]
##         10 |      0.19 | [0.17, 0.21]
##         15 |      0.46 | [0.42, 0.49]
##         20 |      0.75 | [0.72, 0.78]
##         25 |      0.91 | [0.90, 0.93]
##         30 |      0.97 | [0.97, 0.98]</code></pre>
<p>Comparing the predicted probabilities for centers and point guards, we
can see that centers have a considerably higher probability of being a
high earner if they score the same amount of points as a point guard. A
center that scores <span class="math inline">\(15\%\)</span> points per game has a predicted probability of
<span class="math inline">\(64\%\)</span>, a point guard that scores the same has only <span class="math inline">\(46\%\)</span>. Centers also
reach very high probabilities with considerably less points.</p>
<p>We can again visualise this, using predictions over the range <span class="math inline">\(0\)</span> to <span class="math inline">\(30\)</span> points
scored per game, separately for both positions.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="log-est.html#cb218-1" tabindex="-1"></a><span class="fu">ggpredict</span>(model_logit_2, <span class="st">&quot;career_PTS [0:30]&quot;</span>,</span>
<span id="cb218-2"><a href="log-est.html#cb218-2" tabindex="-1"></a>          <span class="at">condition =</span> <span class="fu">c</span>(</span>
<span id="cb218-3"><a href="log-est.html#cb218-3" tabindex="-1"></a>            <span class="at">position_center =</span> <span class="dv">1</span>,</span>
<span id="cb218-4"><a href="log-est.html#cb218-4" tabindex="-1"></a>            <span class="at">position_sf =</span> <span class="dv">0</span>,</span>
<span id="cb218-5"><a href="log-est.html#cb218-5" tabindex="-1"></a>            <span class="at">position_pf =</span> <span class="dv">0</span>,</span>
<span id="cb218-6"><a href="log-est.html#cb218-6" tabindex="-1"></a>            <span class="at">position_sg =</span> <span class="dv">0</span>,</span>
<span id="cb218-7"><a href="log-est.html#cb218-7" tabindex="-1"></a>            <span class="at">position_pg =</span> <span class="dv">0</span></span>
<span id="cb218-8"><a href="log-est.html#cb218-8" tabindex="-1"></a>            )</span>
<span id="cb218-9"><a href="log-est.html#cb218-9" tabindex="-1"></a>          ) <span class="sc">%&gt;%</span></span>
<span id="cb218-10"><a href="log-est.html#cb218-10" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb218-11"><a href="log-est.html#cb218-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, predicted)) <span class="sc">+</span></span>
<span id="cb218-12"><a href="log-est.html#cb218-12" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb218-13"><a href="log-est.html#cb218-13" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/predicted_probabilities_plot_m2-1.png" width="672" /></p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="log-est.html#cb219-1" tabindex="-1"></a><span class="fu">ggpredict</span>(model_logit_2, <span class="st">&quot;career_PTS [0:30]&quot;</span>,</span>
<span id="cb219-2"><a href="log-est.html#cb219-2" tabindex="-1"></a>          <span class="at">condition =</span> <span class="fu">c</span>(</span>
<span id="cb219-3"><a href="log-est.html#cb219-3" tabindex="-1"></a>            <span class="at">position_center =</span> <span class="dv">0</span>,</span>
<span id="cb219-4"><a href="log-est.html#cb219-4" tabindex="-1"></a>            <span class="at">position_sf =</span> <span class="dv">0</span>,</span>
<span id="cb219-5"><a href="log-est.html#cb219-5" tabindex="-1"></a>            <span class="at">position_pf =</span> <span class="dv">0</span>,</span>
<span id="cb219-6"><a href="log-est.html#cb219-6" tabindex="-1"></a>            <span class="at">position_sg =</span> <span class="dv">0</span>,</span>
<span id="cb219-7"><a href="log-est.html#cb219-7" tabindex="-1"></a>            <span class="at">position_pg =</span> <span class="dv">1</span></span>
<span id="cb219-8"><a href="log-est.html#cb219-8" tabindex="-1"></a>            )</span>
<span id="cb219-9"><a href="log-est.html#cb219-9" tabindex="-1"></a>          ) <span class="sc">%&gt;%</span></span>
<span id="cb219-10"><a href="log-est.html#cb219-10" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb219-11"><a href="log-est.html#cb219-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, predicted)) <span class="sc">+</span></span>
<span id="cb219-12"><a href="log-est.html#cb219-12" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb219-13"><a href="log-est.html#cb219-13" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/predicted_probabilities_plot_m2-2.png" width="672" /></p>
<p>Overall the results from the linear regression for <code>salary</code> and the
logistic regression for <code>high_earner</code> tell the same story. The more
points per game a player scores on average, the higher the monetary
compensation they receive is estimated. But the relationship is not the
same over the positions players can occupy. A point guard really has to
deliver on those to be among the high earners, a center can reach
these levels of salary with fewer points. Scoring points is not as
central, <del>no</del> pun intended, for centers as it is for point guards. Both
positions fill different roles and centers have other tasks on the field, for
example blocking players and making rebounds. Maybe we should have included
additional performance measures as control variables to get a more accurate
estimate for our effect of interest. There is also the possibility that there
are some unobserved characteristics that discern high earners from those who are
not, e.g. the already alluded to “star power”.</p>
<p>We have to conclude this introduction to data analysis at this point.
While we were able to build models that could estimate our effect of interest,
there is still room for improvement. We hope that this course has given you all
the tools you need to conduct your own projects and improve on ours!</p>
</div>
</div>
<div id="additional-pointers" class="section level2 hasAnchor" number="13.6">
<h2><span class="header-section-number">13.6</span> Additional pointers<a href="log-est.html#additional-pointers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we leave you to it, we will you give you some additional pointers for
logistic regression that could not be covered in full for this session.</p>
<div id="predicting-hypotheticals-1" class="section level3 hasAnchor" number="13.6.1">
<h3><span class="header-section-number">13.6.1</span> Predicting hypotheticals<a href="log-est.html#predicting-hypotheticals-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have actually already covered how we can make predictions based on a logistic
regression. This is what predicted probabilities are!
But we could also approach this as we have done <a href="pm-t.html#pm-t">before</a>, constructing
a new tibble with data on hypothetical players and then using <code>predict()</code> to
receive the predictions. Just be sure to add the argument <code>type = "response"</code> to
<code>predict()</code> to actually receive probabilities.</p>
</div>
<div id="diagnostics" class="section level3 hasAnchor" number="13.6.2">
<h3><span class="header-section-number">13.6.2</span> Diagnostics<a href="log-est.html#diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Logistic regression and linear regression share some of the same
assumptions, e.g. on linearity, outliers and multicollinearity.
Still, testing model assumptions and performing diagnostics somewhat differs
between both approaches. For instance, many diagnostics we have seen for linear
regression revolve around analyzing the residuals.
In the context of logistic regression, given that the
predicted values are probabilities and the actual values are either <span class="math inline">\(0\)</span>
and <span class="math inline">\(1\)</span>, it is not very clear what residuals mean.</p>
<p>We will not cover diagnostic tests for logistic regression in this course but
still wan to give you some pointers.
A deep dive into the topic can be found
<a href="http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/">here</a>.
A simple approach to actually apply the diagnostics could be the function
<code>check_model()</code> from the <code>performance</code> package.</p>
</div>
<div id="comparing-linear-and-logistic-regression" class="section level3 hasAnchor" number="13.6.3">
<h3><span class="header-section-number">13.6.3</span> Comparing linear and logistic regression<a href="log-est.html#comparing-linear-and-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we have seen above, logistic regression can cause many headaches
because assessing the model fit, testing diagnostics, prediction and
diagnostics are all comparably complicated to perform and interpret.</p>
<p>One alternative to conducting a logistic regression on a binary outcome is to
just apply a linear regression to it. In this case we are talking about a
<em>linear probability model</em> (LPM). The coefficients from a LPM show us the
increase in probability for <span class="math inline">\(y = 1\)</span> for a one unit increase in the <span class="math inline">\(x\)</span>
variables. This is comparable to the AMEs from a logistic regression.</p>
<p>Let us reinspect them for our second model and then compare them to the
coefficients from the same model computed as an LPM.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="log-est.html#cb220-1" tabindex="-1"></a><span class="co"># AMEs from alogistic regression</span></span>
<span id="cb220-2"><a href="log-est.html#cb220-2" tabindex="-1"></a><span class="fu">margins</span>(model_logit_2)</span></code></pre></div>
<pre><code>##  career_PTS position_center position_sf position_pf position_sg position_pg
##     0.03487          0.1194     0.03232     0.02669   -0.005299     0.01871</code></pre>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="log-est.html#cb222-1" tabindex="-1"></a><span class="co"># coefficients from a linear regression (LPM)</span></span>
<span id="cb222-2"><a href="log-est.html#cb222-2" tabindex="-1"></a><span class="fu">lm</span>(<span class="fu">as.numeric</span>(high_earner) <span class="sc">~</span> career_PTS <span class="sc">+</span> position_center <span class="sc">+</span> position_sf <span class="sc">+</span>  position_pf <span class="sc">+</span> position_sg <span class="sc">+</span> position_pg,</span>
<span id="cb222-3"><a href="log-est.html#cb222-3" tabindex="-1"></a>   <span class="at">data =</span> data_nba) <span class="sc">%&gt;%</span> </span>
<span id="cb222-4"><a href="log-est.html#cb222-4" tabindex="-1"></a>  <span class="fu">coef</span>()</span></code></pre></div>
<pre><code>##     (Intercept)      career_PTS position_center     position_sf     position_pf 
##     0.827792872     0.041923758     0.115477249     0.021766019     0.027272863 
##     position_sg     position_pg 
##    -0.013857610     0.002697281</code></pre>
<p>As we can see, the average marginal effects calculated based on the
logistic regression model are relatively similar to the linear
regression coefficients, differing more strongly for some variables than they
do for others. While differences exist, the substantial conclusions we would
draw from the LPM are the same as from the logistic regression. At the same
time, when our aim is to estimate an effect with as little bias as possible, we
would still prefer the logistic regression for this case.</p>
<p>For other cases, the differences between both approaches may be minimal. This
mainly depends on the range of probabilities we are looking at.
As a rule of thumb, many researchers consider using an LPM when the
probabilities are relatively close to <span class="math inline">\(50\%\)</span>. It is easier to apply,
easier to interpret and the results are very similar in logistic
regression in these cases.
When the probabilities are more extreme, the results tend to differ more
strongly. Also in these cases a linear regression could predict values higher
than <span class="math inline">\(100\%\)</span> or lower than <span class="math inline">\(0\%\)</span>, which of course should not be possible.</p>
<p>In our case, about <span class="math inline">\(25\%\)</span> of players belong to the high earners. We are working
with relatively low probabilities of belonging to this group from the outset.
This is why the results somewhat differed between both approaches. If we would
have chosen a cutoff that splits the sample in two equally large and thus
probable groups, i.e. the median, the results may have been more close. If we
would have chosen a more extreme cutoff, e.g. the top <span class="math inline">\(10\%\)</span>, the results
would differ even more. This again underlines the importance of EDA and
understanding your data to make a sensible decision on the model used.</p>
<p>For a more in-depth discussion see
<a href="https://statisticalhorizons.com/linear-vs-logistic/">here</a>.</p>
</div>
<div id="logistic-regression-in-machine-learning-context" class="section level3 hasAnchor" number="13.6.4">
<h3><span class="header-section-number">13.6.4</span> Logistic regression in Machine Learning context<a href="log-est.html#logistic-regression-in-machine-learning-context" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In an <a href="ml.html#ml">earlier session</a> we introduced you to using supervised machine
learning for regression problems. Another common usage are <em>classification</em>
problems, predicting whether an observation belongs to a certain category or
not. You should be able to see the connection to logistic regression which is
actually one of the algorithms used for these kinds of problems.</p>
<p>There are many alternatives to using a “pure” logistic regression, e.g. the
lasso, ridge and elastic-net flavours of logistic regression, and many
other algorithms that can also perform classification, e.g. random forests,
support vector machines or naive bayes.</p>
<p>The general approach to machine learning we have seen before would be the same,
only that we have a binary outcome, may use a different algorithm and set the
mode to “classification”. The central difference is the way we assess the
models’ performance. Previously, we have used the <code>RMSE</code> to
evaluate how good the predictions were. We can not use this for
models with binary outcomes, since there are no “residuals” in the
classical sense. Instead, a common metric for assessing model performance
is “accuracy” and related measures like sensitivity and specificity.
All of these measures are based upon comparing the predicted and actual
categories of the outcome variable. Accuracy for example measures the percentage
of the actual values that are correctly predicted by the model. Keeping with
our example all high-earners and non-high-earners that are correctly predicted
would increase the accuracy and all wrong predictions would decrease it.</p>
<p>There are obviously more differences and accuracy should never be used as the
sole measure of performance, but we hope to have given you some pointers that
you can expand on, for example by training a machine learning model to predict
being a high earner in the NBA.</p>
</div>
</div>
<div id="further-resources-4" class="section level2 hasAnchor" number="13.7">
<h2><span class="header-section-number">13.7</span> Further resources<a href="log-est.html#further-resources-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="resources">
<ul>
<li><a href="http://sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/">Logistic Regression Essentials in
R</a>:
This is a web page that introduces the basic concepts and techniques
of logistic regression in R. It covers topics such as model
assumptions, diagnostics, odds ratios, marginal effects, and model
comparison. It also provides examples of applying logistic
regression methods to real-world data sets.</li>
<li><a href="https://www.datacamp.com/tutorial/logistic-regression-R">Logistic Regression in R
Tutorial</a>:
This is a web page that provides a step-by-step tutorial on how to
perform logistic regression in R using both base-R and tidymodels
workflows. It also shows how to interpret the model output, assess
model performance, and make predictions.</li>
<li><a href="https://www.statology.org/r-logistic-regression-predict/">How to Use predict() with Logistic Regression Models in
R</a>: This
is a web page that shows how to use the predict() function in R to
make predictions from a fitted logistic regression model. It also
explains the difference between type = “response” and type = “link”
arguments, and how to use new data for prediction.</li>
<li>Mood, Carina (2009). Logistic Regression: Why We Cannot Do What We Think We
Can Do, and What We Can Do About It. European Sociological Review, 26(1),
67-82.: In this influential article, Carina Mood explains why we can not
interpret logits and odds ratios as direct measures of an effect and can not
directly compare them between models.</li>
<li>Karlson, Kristian Bernt, Anders Holm &amp; Richard Breen (2012). Comparing
Regression Coefficients Between Same-sample Nested Models Using Logit and
Probit: A New Method. Sociological Methodology, 42(1), 286-313.: A proposed
solution for the problem described in Mood 2009.</li>
<li><a href="https://www.r-bloggers.com/2021/01/machine-learning-with-r-a-complete-guide-to-logistic-regression/">Machine Learning with R: A Complete Guide to Logistic
Regression</a>:
This is a blog post that explains how to use logistic regression for
predictive modeling in R, using the trees data set as an example. It
covers how to fit, visualize, and evaluate logistic regression models,
as well as how to calculate confidence and prediction intervals.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pm-a.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/13-logistic-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
