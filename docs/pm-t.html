<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 10 Prediction | Data Analysis with R for Social Scientists</title>
  <meta name="description" content="In this course, you will learn how to analyse data using regression in R." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content=" 10 Prediction | Data Analysis with R for Social Scientists" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="In this course, you will learn how to analyse data using regression in R." />
  <meta name="github-repo" content="jaspertjaden/DataAnalysisR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 10 Prediction | Data Analysis with R for Social Scientists" />
  
  <meta name="twitter:description" content="In this course, you will learn how to analyse data using regression in R." />
  

<meta name="author" content="Jakob Tures &amp; Jasper Tjaden" />


<meta name="date" content="2023-10-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lin-e.html"/>
<link rel="next" href="ml.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the Authors</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#github-repo"><i class="fa fa-check"></i>GitHub Repo</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-sem.html"><a href="intro-sem.html"><i class="fa fa-check"></i><b>1</b> Introduction to Seminar</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-sem.html"><a href="intro-sem.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="eda-1.html"><a href="eda-1.html"><i class="fa fa-check"></i><b>2</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda-1.html"><a href="eda-1.html#objectives"><i class="fa fa-check"></i><b>2.1</b> Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="eda-1.html"><a href="eda-1.html#r-functions-covered-this-week"><i class="fa fa-check"></i><b>2.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="2.3" data-path="eda-1.html"><a href="eda-1.html#what-is-eda-and-why-is-it-so-important"><i class="fa fa-check"></i><b>2.3</b> What is <em>EDA</em> and why is it so important?</a></li>
<li class="chapter" data-level="2.4" data-path="eda-1.html"><a href="eda-1.html#importing-data-into-r"><i class="fa fa-check"></i><b>2.4</b> Importing data into R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="eda-1.html"><a href="eda-1.html#merge-datasets"><i class="fa fa-check"></i><b>2.4.1</b> Merge datasets</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="eda-1.html"><a href="eda-1.html#clean-dataset"><i class="fa fa-check"></i><b>2.5</b> Clean dataset</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="eda-1.html"><a href="eda-1.html#mutating-variables"><i class="fa fa-check"></i><b>2.5.1</b> Mutating variables</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="eda-1.html"><a href="eda-1.html#explore-the-complete-dataset"><i class="fa fa-check"></i><b>2.6</b> Explore the complete dataset</a></li>
<li class="chapter" data-level="2.7" data-path="eda-1.html"><a href="eda-1.html#explore-individual-variables"><i class="fa fa-check"></i><b>2.7</b> Explore individual variables</a></li>
<li class="chapter" data-level="2.8" data-path="eda-1.html"><a href="eda-1.html#moving-on"><i class="fa fa-check"></i><b>2.8</b> Moving on</a></li>
<li class="chapter" data-level="2.9" data-path="eda-1.html"><a href="eda-1.html#further-resources"><i class="fa fa-check"></i><b>2.9</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="eda-2.html"><a href="eda-2.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis - Exercise</a>
<ul>
<li class="chapter" data-level="3.1" data-path="eda-2.html"><a href="eda-2.html#what-is-r-markdown"><i class="fa fa-check"></i><b>3.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="3.2" data-path="eda-2.html"><a href="eda-2.html#creating-a-r-markdown-file"><i class="fa fa-check"></i><b>3.2</b> Creating a R Markdown file</a></li>
<li class="chapter" data-level="3.3" data-path="eda-2.html"><a href="eda-2.html#writing-in-r-markdown"><i class="fa fa-check"></i><b>3.3</b> Writing in R Markdown</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="eda-2.html"><a href="eda-2.html#document-components"><i class="fa fa-check"></i><b>3.3.1</b> Document components</a></li>
<li class="chapter" data-level="3.3.2" data-path="eda-2.html"><a href="eda-2.html#formatting"><i class="fa fa-check"></i><b>3.3.2</b> Formatting</a></li>
<li class="chapter" data-level="3.3.3" data-path="eda-2.html"><a href="eda-2.html#code-chunks"><i class="fa fa-check"></i><b>3.3.3</b> Code chunks</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="eda-2.html"><a href="eda-2.html#further-resources-1"><i class="fa fa-check"></i><b>3.4</b> Further resources</a></li>
<li class="chapter" data-level="3.5" data-path="eda-2.html"><a href="eda-2.html#eda---exercise"><i class="fa fa-check"></i><b>3.5</b> EDA - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dags-1.html"><a href="dags-1.html"><i class="fa fa-check"></i><b>4</b> DAGs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="dags-1.html"><a href="dags-1.html#objectives-1"><i class="fa fa-check"></i><b>4.1</b> Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="dags-1.html"><a href="dags-1.html#functions-covered"><i class="fa fa-check"></i><b>4.2</b> Functions Covered</a></li>
<li class="chapter" data-level="4.3" data-path="dags-1.html"><a href="dags-1.html#modelling"><i class="fa fa-check"></i><b>4.3</b> Modelling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="dags-1.html"><a href="dags-1.html#what-is-modelling"><i class="fa fa-check"></i><b>4.3.1</b> What is modelling?</a></li>
<li class="chapter" data-level="4.3.2" data-path="dags-1.html"><a href="dags-1.html#estimating-effects-vs.-prediction"><i class="fa fa-check"></i><b>4.3.2</b> Estimating effects vs. prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="dags-1.html"><a href="dags-1.html#dags"><i class="fa fa-check"></i><b>4.4</b> DAGs</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="dags-1.html"><a href="dags-1.html#directed-acyclical-graphs"><i class="fa fa-check"></i><b>4.4.1</b> Directed acyclical graphs</a></li>
<li class="chapter" data-level="4.4.2" data-path="dags-1.html"><a href="dags-1.html#patterns-of-relationships"><i class="fa fa-check"></i><b>4.4.2</b> Patterns of relationships</a></li>
<li class="chapter" data-level="4.4.3" data-path="dags-1.html"><a href="dags-1.html#adjustment-set"><i class="fa fa-check"></i><b>4.4.3</b> Adjustment set</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="dags-1.html"><a href="dags-1.html#nba-dag"><i class="fa fa-check"></i><b>4.5</b> NBA DAG</a></li>
<li class="chapter" data-level="4.6" data-path="dags-1.html"><a href="dags-1.html#resources"><i class="fa fa-check"></i><b>4.6</b> Resources</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="dags-1.html"><a href="dags-1.html#dagitty.net"><i class="fa fa-check"></i><b>4.6.1</b> dagitty.net</a></li>
<li class="chapter" data-level="4.6.2" data-path="dags-1.html"><a href="dags-1.html#how-to-use-dagitty"><i class="fa fa-check"></i><b>4.6.2</b> How to use dagitty()</a></li>
<li class="chapter" data-level="4.6.3" data-path="dags-1.html"><a href="dags-1.html#more-on-dags"><i class="fa fa-check"></i><b>4.6.3</b> More on DAGs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lin-t-1.html"><a href="lin-t-1.html"><i class="fa fa-check"></i><b>5</b> Linear Regression - Theory I: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lin-t-1.html"><a href="lin-t-1.html#objectives-2"><i class="fa fa-check"></i><b>5.1</b> Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lin-t-1.html"><a href="lin-t-1.html#what-is-linear-regression"><i class="fa fa-check"></i><b>5.2</b> What is Linear Regression</a></li>
<li class="chapter" data-level="5.3" data-path="lin-t-1.html"><a href="lin-t-1.html#examplary-research-question-data"><i class="fa fa-check"></i><b>5.3</b> Examplary research question &amp; data</a></li>
<li class="chapter" data-level="5.4" data-path="lin-t-1.html"><a href="lin-t-1.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lin-t-1.html"><a href="lin-t-1.html#regression-formula"><i class="fa fa-check"></i><b>5.4.1</b> Regression Formula</a></li>
<li class="chapter" data-level="5.4.2" data-path="lin-t-1.html"><a href="lin-t-1.html#regressing-grade-on-hours"><i class="fa fa-check"></i><b>5.4.2</b> Regressing <code>grade</code> on <code>hours</code></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lin-t-1.html"><a href="lin-t-1.html#moving-on-1"><i class="fa fa-check"></i><b>5.5</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lin-t-2.html"><a href="lin-t-2.html"><i class="fa fa-check"></i><b>6</b> Linear Regression - Theory II: Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lin-t-2.html"><a href="lin-t-2.html#objectives-3"><i class="fa fa-check"></i><b>6.1</b> Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lin-t-2.html"><a href="lin-t-2.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-additional-metric-variables"><i class="fa fa-check"></i><b>6.2.1</b> Adding additional metric variables</a></li>
<li class="chapter" data-level="6.2.2" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-dummy-variables"><i class="fa fa-check"></i><b>6.2.2</b> Adding dummy variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-categorical-variables"><i class="fa fa-check"></i><b>6.2.3</b> Adding categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lin-t-2.html"><a href="lin-t-2.html#returning-to-our-research-question"><i class="fa fa-check"></i><b>6.3</b> Returning to our research question</a></li>
<li class="chapter" data-level="6.4" data-path="lin-t-2.html"><a href="lin-t-2.html#adressing-the-uncertainty"><i class="fa fa-check"></i><b>6.4</b> Adressing the uncertainty</a></li>
<li class="chapter" data-level="6.5" data-path="lin-t-2.html"><a href="lin-t-2.html#moving-on-2"><i class="fa fa-check"></i><b>6.5</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lin-t-3.html"><a href="lin-t-3.html"><i class="fa fa-check"></i><b>7</b> Linear Regression - Theory III: Diagnostics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lin-t-3.html"><a href="lin-t-3.html#objectives-4"><i class="fa fa-check"></i><b>7.1</b> Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="lin-t-3.html"><a href="lin-t-3.html#model-fit"><i class="fa fa-check"></i><b>7.2</b> Model fit</a></li>
<li class="chapter" data-level="7.3" data-path="lin-t-3.html"><a href="lin-t-3.html#regression-diagnostics"><i class="fa fa-check"></i><b>7.3</b> Regression diagnostics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="lin-t-3.html"><a href="lin-t-3.html#linearity"><i class="fa fa-check"></i><b>7.3.1</b> Linearity</a></li>
<li class="chapter" data-level="7.3.2" data-path="lin-t-3.html"><a href="lin-t-3.html#normally-distributed-residuals"><i class="fa fa-check"></i><b>7.3.2</b> Normally distributed residuals</a></li>
<li class="chapter" data-level="7.3.3" data-path="lin-t-3.html"><a href="lin-t-3.html#homoscedasticity"><i class="fa fa-check"></i><b>7.3.3</b> Homoscedasticity</a></li>
<li class="chapter" data-level="7.3.4" data-path="lin-t-3.html"><a href="lin-t-3.html#no-overly-influential-data-points"><i class="fa fa-check"></i><b>7.3.4</b> No overly influential data points</a></li>
<li class="chapter" data-level="7.3.5" data-path="lin-t-3.html"><a href="lin-t-3.html#no-multicollinearity"><i class="fa fa-check"></i><b>7.3.5</b> No (multi)collinearity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lin-t-3.html"><a href="lin-t-3.html#returning-to-our-research-question-1"><i class="fa fa-check"></i><b>7.4</b> Returning to our research question</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="lin-t-3.html"><a href="lin-t-3.html#interactions"><i class="fa fa-check"></i><b>7.4.1</b> Interactions</a></li>
<li class="chapter" data-level="7.4.2" data-path="lin-t-3.html"><a href="lin-t-3.html#regression-diagnostics-revisited"><i class="fa fa-check"></i><b>7.4.2</b> Regression diagnostics (revisited)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lin-t-3.html"><a href="lin-t-3.html#conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
<li class="chapter" data-level="7.6" data-path="lin-t-3.html"><a href="lin-t-3.html#resources-1"><i class="fa fa-check"></i><b>7.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lin-a.html"><a href="lin-a.html"><i class="fa fa-check"></i><b>8</b> Linear Regression - Application</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lin-a.html"><a href="lin-a.html#objectives-5"><i class="fa fa-check"></i><b>8.1</b> Objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lin-a.html"><a href="lin-a.html#r-functions-covered-this-week-1"><i class="fa fa-check"></i><b>8.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="8.3" data-path="lin-a.html"><a href="lin-a.html#research-question"><i class="fa fa-check"></i><b>8.3</b> Research question</a></li>
<li class="chapter" data-level="8.4" data-path="lin-a.html"><a href="lin-a.html#simple-linear-regression-in-r"><i class="fa fa-check"></i><b>8.4</b> Simple linear regression in R</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lin-a.html"><a href="lin-a.html#interpretation"><i class="fa fa-check"></i><b>8.4.1</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lin-a.html"><a href="lin-a.html#multiple-linear-regression-in-r"><i class="fa fa-check"></i><b>8.5</b> Multiple linear regression in R</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lin-a.html"><a href="lin-a.html#interpretation-1"><i class="fa fa-check"></i><b>8.5.1</b> Interpretation</a></li>
<li class="chapter" data-level="8.5.2" data-path="lin-a.html"><a href="lin-a.html#sidenote-adding-interactions"><i class="fa fa-check"></i><b>8.5.2</b> Sidenote: Adding interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lin-a.html"><a href="lin-a.html#regression-diagnostics-1"><i class="fa fa-check"></i><b>8.6</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lin-a.html"><a href="lin-a.html#skewed-outcome-variable"><i class="fa fa-check"></i><b>8.6.1</b> Skewed outcome variable</a></li>
<li class="chapter" data-level="8.6.2" data-path="lin-a.html"><a href="lin-a.html#non-linearity"><i class="fa fa-check"></i><b>8.6.2</b> Non-linearity</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lin-a.html"><a href="lin-a.html#returning-to-our-research-question-2"><i class="fa fa-check"></i><b>8.7</b> Returning to our research question</a></li>
<li class="chapter" data-level="8.8" data-path="lin-a.html"><a href="lin-a.html#moving-on-3"><i class="fa fa-check"></i><b>8.8</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lin-e.html"><a href="lin-e.html"><i class="fa fa-check"></i><b>9</b> Linear Regression - Exercise</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lin-e.html"><a href="lin-e.html#linear-regression---exercise"><i class="fa fa-check"></i><b>9.1</b> Linear Regression - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pm-t.html"><a href="pm-t.html"><i class="fa fa-check"></i><b>10</b> Prediction</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pm-t.html"><a href="pm-t.html#objectives-6"><i class="fa fa-check"></i><b>10.1</b> Objectives</a></li>
<li class="chapter" data-level="10.2" data-path="pm-t.html"><a href="pm-t.html#r-functions-covered-this-week-2"><i class="fa fa-check"></i><b>10.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="10.3" data-path="pm-t.html"><a href="pm-t.html#making-predictions"><i class="fa fa-check"></i><b>10.3</b> Making predictions</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="pm-t.html"><a href="pm-t.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>10.3.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="pm-t.html"><a href="pm-t.html#why-predict-at-all"><i class="fa fa-check"></i><b>10.3.2</b> Why predict at all?</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="pm-t.html"><a href="pm-t.html#making-better-predictions"><i class="fa fa-check"></i><b>10.4</b> Making better predictions</a></li>
<li class="chapter" data-level="10.5" data-path="pm-t.html"><a href="pm-t.html#making-even-better-predictions"><i class="fa fa-check"></i><b>10.5</b> Making even better predictions?</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pm-t.html"><a href="pm-t.html#comparing-models"><i class="fa fa-check"></i><b>10.5.1</b> Comparing models</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="pm-t.html"><a href="pm-t.html#predicting-hypotheticals"><i class="fa fa-check"></i><b>10.6</b> Predicting hypotheticals</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="pm-t.html"><a href="pm-t.html#adressing-uncertainty"><i class="fa fa-check"></i><b>10.6.1</b> Adressing uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="pm-t.html"><a href="pm-t.html#further-resources-2"><i class="fa fa-check"></i><b>10.7</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>11</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ml.html"><a href="ml.html#objectives-7"><i class="fa fa-check"></i><b>11.1</b> Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="ml.html"><a href="ml.html#r-functions-covered-this-week-3"><i class="fa fa-check"></i><b>11.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="11.3" data-path="ml.html"><a href="ml.html#intro-to-machine-learning"><i class="fa fa-check"></i><b>11.3</b> Intro to Machine learning</a></li>
<li class="chapter" data-level="11.4" data-path="ml.html"><a href="ml.html#further-resources-3"><i class="fa fa-check"></i><b>11.4</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pm-a.html"><a href="pm-a.html"><i class="fa fa-check"></i><b>12</b> Prediction &amp; Machine Learning - Exercise</a>
<ul>
<li class="chapter" data-level="12.1" data-path="pm-a.html"><a href="pm-a.html#exercises"><i class="fa fa-check"></i><b>12.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="log-est.html"><a href="log-est.html"><i class="fa fa-check"></i><b>13</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="13.1" data-path="log-est.html"><a href="log-est.html#objectives-8"><i class="fa fa-check"></i><b>13.1</b> Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="log-est.html"><a href="log-est.html#functions-covered-in-this-week"><i class="fa fa-check"></i><b>13.2</b> Functions covered in this week</a></li>
<li class="chapter" data-level="13.3" data-path="log-est.html"><a href="log-est.html#basic-concepts"><i class="fa fa-check"></i><b>13.3</b> Basic concepts</a></li>
<li class="chapter" data-level="13.4" data-path="log-est.html"><a href="log-est.html#application"><i class="fa fa-check"></i><b>13.4</b> Application</a></li>
<li class="chapter" data-level="13.5" data-path="log-est.html"><a href="log-est.html#interpretation-2"><i class="fa fa-check"></i><b>13.5</b> Interpretation</a></li>
<li class="chapter" data-level="13.6" data-path="log-est.html"><a href="log-est.html#model-fit-1"><i class="fa fa-check"></i><b>13.6</b> Model fit</a></li>
<li class="chapter" data-level="13.7" data-path="log-est.html"><a href="log-est.html#diagnostics"><i class="fa fa-check"></i><b>13.7</b> Diagnostics</a></li>
<li class="chapter" data-level="13.8" data-path="log-est.html"><a href="log-est.html#prediction"><i class="fa fa-check"></i><b>13.8</b> Prediction</a></li>
<li class="chapter" data-level="13.9" data-path="log-est.html"><a href="log-est.html#mediation-1"><i class="fa fa-check"></i><b>13.9</b> Mediation</a></li>
<li class="chapter" data-level="13.10" data-path="log-est.html"><a href="log-est.html#comparing-linear-and-logistic-regression"><i class="fa fa-check"></i><b>13.10</b> Comparing linear and logistic regression</a></li>
<li class="chapter" data-level="13.11" data-path="log-est.html"><a href="log-est.html#logistic-regression-in-machine-learning-context"><i class="fa fa-check"></i><b>13.11</b> Logistic regression in Machine Learning context</a></li>
<li class="chapter" data-level="13.12" data-path="log-est.html"><a href="log-est.html#further-resources-4"><i class="fa fa-check"></i><b>13.12</b> Further resources</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis with R for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pm-t" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number"> 10</span> Prediction<a href="pm-t.html#pm-t" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In prior weeks, you learned how to build a linear regression model. The main
interest we pursued so far was to arrive at an unbiased estimate for the effect
of one independent variable (in our case the average points a NBA player scored
per game) on an outcome (here the salary of NBA players).
With the help of a DAG, we identified relevant variables we have to include in
our model, and ones we are not allowed to include, to “isolate’ the effect of
points as much as possible and reduce bias.
All what we have done so far can be considered part of classical statistical
inference, i.e.understanding why an outcome varies and what the magnitude and
direction of effects is.
This is one approach to modelling.</p>
<p>Another approach is <em>predicition</em>.
In this scenario, we build regression models (or other models) solely to predict
an outcome to the highest accuracy. The main interest is not to learn more about
how the outcome can be explained but to predict something which we are
interested in.
Making predictions based on a computed model is hardly new, still it
lies at the heart of more modern approaches to analysing data, i.e. “data
science” and “machine learning”. We will return to the latter <a href="ml.html#ml">next week</a>.</p>
<p>In this week we will focus on how to predict values based on a linear regression
model, like the ones we have computed so far.</p>
<div id="objectives-6" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Objectives<a href="pm-t.html#objectives-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="objectives">
<ul>
<li>Making predictions using a linear regression model</li>
<li>Assessing the quality of the predictions</li>
<li>Predicting the outcome for hypothetical cases</li>
</ul>
</div>
</div>
<div id="r-functions-covered-this-week-2" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> R functions covered this week<a href="pm-t.html#r-functions-covered-this-week-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="functions">
<ul>
<li>Making predictions
<ul>
<li><code>predict()</code> : Base R function used to make predictions from a fitted model object. It takes a model object and a new data frame that contains the values of the independent variables for which predictions are desired. It returns a vector of predicted values for the dependent variable. Optionally it can also return confidence and prediction intervals.</li>
<li><code>augment()</code> : Function from the <code>broom</code> package. It is used to add columns with predictions, residuals, and other information to the original data frame. It takes a model object and an optional new data frame for prediction. It returns a tibble with one row per observation and one column per variable or statistic. Optionally it can also return confidence and prediction intervals.</li>
</ul></li>
<li>Assessing the quality of predictions
<ul>
<li><code>mse()</code>: Function from the <code>Metrics</code> package. It is used to compute the <em>mean squared error</em> (MSE), the average of the squared differences between predicted and actual values.</li>
<li><code>rmse()</code>: Function from the <code>Metrics</code> package. It is used to compute the <em>root mean squared error</em> (RMSE), the root of the MSE. This gives us the average difference between predicted and actual values on the original scale of the outcome variable, which is easier to interpret.</li>
</ul></li>
<li><code>bind_cols()</code>: A <code>dplyr</code> function that adds additional columns to an existing tibble. Used here to add predictions and confidence/prediction intervals to a tiblle containing hypothetical cases we want to predict for.</li>
<li>New <code>ggplot()</code> geoms
<ul>
<li><code>geom_abline()</code> &amp; <code>geom_segment()</code></li>
</ul></li>
</ul>
</div>
</div>
<div id="making-predictions" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Making predictions<a href="pm-t.html#making-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we have been using an <em>x-centered</em> approach. We were focused on
understanding how an independent variable affects a dependent variable.
Let us reconsider the formula used for estimation in a simple linear regression
from <a href="lin-t-1.html#lin-t-1">session 5</a>:</p>
<p><span class="math display">\[\hat{y} = b_0 +b_1*x_1\]</span></p>
<p>Our goal was estimating <span class="math inline">\(b_0\)</span> and especially <span class="math inline">\(b_1\)</span> in a way that describes the
relationship between the x- and y-variables with the least error. Here we were
centered on <span class="math inline">\(x\)</span> because our main interest was understanding how the <span class="math inline">\(x\)</span> variable
influences the outcome <span class="math inline">\(y\)</span> as expressed by the coefficient <span class="math inline">\(b_1\)</span>.</p>
<p>Moving on to prediction, the formula for a linear regression stays the same.
We also still have one or multiple independent x-variables and
their associated coefficients in our model, but they are now more of a means to
an end. The end being predicting the outcome <span class="math inline">\(y\)</span> or to be
more precise our estimate <span class="math inline">\(\hat{y}\)</span> to the highest accuracy.
These approaches are therefore also called <em>y-centered</em>.</p>
<p>So how can we make predictions based on a linear regression model?
Luckily this is very straightforward as we already know all the moving parts we
need from our session on linear regression.
Remember that linear regression is all about finding a straight line through a
cloud of points that describes the relationship between two or more variables
with as little error as possible. The line will have an an intercept, i.e. 
the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>, <span class="math inline">\(b_0\)</span> in our formula, and a slope, <span class="math inline">\(b_1\)</span>.
The latter describes the change in <span class="math inline">\(y\)</span> for an increase of one unit in <span class="math inline">\(x\)</span>.</p>
<div id="simple-linear-regression-1" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Simple linear regression<a href="pm-t.html#simple-linear-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this to make sense, let us return to our NBA data.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="pm-t.html#cb123-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb123-2"><a href="pm-t.html#cb123-2" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;../datasets/nba/data_nba.RData&quot;</span>)</span></code></pre></div>
<p>The simplest model we came up with was regressing <code>salary</code> on <code>career_PTS</code>.
Let us plot this relationship once again.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="pm-t.html#cb124-1" tabindex="-1"></a>data_nba <span class="sc">%&gt;%</span> </span>
<span id="cb124-2"><a href="pm-t.html#cb124-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> career_PTS, <span class="at">y =</span> salary)) <span class="sc">+</span></span>
<span id="cb124-3"><a href="pm-t.html#cb124-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/plot_salary_pts-1.png" width="672" /></p>
<p>When we estimated our first regression, the goal was to find the <em>one</em> line
that describes the relationship between both variables with as little error as
possible.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="pm-t.html#cb125-1" tabindex="-1"></a><span class="fu">lm</span>(salary <span class="sc">~</span> career_PTS, <span class="at">data =</span> data_nba) <span class="sc">%&gt;%</span> </span>
<span id="cb125-2"><a href="pm-t.html#cb125-2" tabindex="-1"></a>  <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ career_PTS, data = data_nba)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -14788659  -2023969   -434599   1311807  24326060 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -851915      76417  -11.15   &lt;2e-16 ***
## career_PTS    552843       7453   74.17   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3732000 on 9726 degrees of freedom
## Multiple R-squared:  0.3613, Adjusted R-squared:  0.3612 
## F-statistic:  5502 on 1 and 9726 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The intercept, <span class="math inline">\(b_0\)</span>, and the coefficient for our sole independent variable,
<span class="math inline">\(b_1\)</span>, are the parameters needed to draw this one line, our <em>best fit</em>. We can
easily add the regression line to the plot using the values provided in the
model summary.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="pm-t.html#cb127-1" tabindex="-1"></a>data_nba <span class="sc">%&gt;%</span> </span>
<span id="cb127-2"><a href="pm-t.html#cb127-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> career_PTS, <span class="at">y =</span> salary)) <span class="sc">+</span></span>
<span id="cb127-3"><a href="pm-t.html#cb127-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb127-4"><a href="pm-t.html#cb127-4" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="sc">-</span><span class="dv">851914</span>, <span class="at">slope =</span> <span class="dv">552843</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/plot_salary_pts_lm-1.png" width="672" /></p>
<p>How is this line drawn? All we did was plugging in the values for the intercept
and the coefficient into our formula:</p>
<p><span class="math display">\[\hat{y} = -851914 + 552843 * x_1\]</span></p>
<p>For every <span class="math inline">\(x\)</span> value, the formula describes the corresponding <span class="math inline">\(y\)</span>
value. For example, we can compute the <span class="math inline">\(y\)</span> values our model assumes for <span class="math inline">\(x = 10\)</span>
and <span class="math inline">\(x = 20\)</span>.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="pm-t.html#cb128-1" tabindex="-1"></a><span class="sc">-</span><span class="dv">851914</span> <span class="sc">+</span> <span class="dv">552843</span> <span class="sc">*</span> <span class="dv">10</span></span></code></pre></div>
<pre><code>## [1] 4676516</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="pm-t.html#cb130-1" tabindex="-1"></a><span class="sc">-</span><span class="dv">851914</span> <span class="sc">+</span> <span class="dv">552843</span> <span class="sc">*</span> <span class="dv">20</span></span></code></pre></div>
<pre><code>## [1] 10204946</code></pre>
<p>We can also visualise these points in our plot. Note that here we used
<code>geom_smooth(method = "lm")</code> to draw the regression line for us. What this
function does is to compute a linear regression for the two variables in the
background and then draw the corresponding regression line.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="pm-t.html#cb132-1" tabindex="-1"></a>data_nba <span class="sc">%&gt;%</span> </span>
<span id="cb132-2"><a href="pm-t.html#cb132-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> career_PTS, <span class="at">y =</span> salary)) <span class="sc">+</span></span>
<span id="cb132-3"><a href="pm-t.html#cb132-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb132-4"><a href="pm-t.html#cb132-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb132-5"><a href="pm-t.html#cb132-5" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="dv">10</span>, <span class="at">y =</span> <span class="dv">4676516</span>, <span class="at">xend =</span> <span class="dv">10</span>, <span class="at">yend =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb132-6"><a href="pm-t.html#cb132-6" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">4676516</span>, <span class="at">xend =</span> <span class="dv">10</span>, <span class="at">yend =</span> <span class="dv">4676516</span>, <span class="at">linetype =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb132-7"><a href="pm-t.html#cb132-7" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">x =</span> <span class="dv">10</span>, <span class="at">y =</span> <span class="dv">4676516</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb132-8"><a href="pm-t.html#cb132-8" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="dv">20</span>, <span class="at">y =</span> <span class="dv">10204946</span>, <span class="at">xend =</span> <span class="dv">20</span>, <span class="at">yend =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb132-9"><a href="pm-t.html#cb132-9" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">10204946</span>, <span class="at">xend =</span> <span class="dv">20</span>, <span class="at">yend =</span> <span class="dv">10204946</span>, <span class="at">linetype =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb132-10"><a href="pm-t.html#cb132-10" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">x =</span> <span class="dv">20</span>, <span class="at">y =</span> <span class="dv">10204946</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/plot_salary_pts_lm_two_points-1.png" width="672" /></p>
<p>The two marked points are the <span class="math inline">\(y\)</span> values our model estimates for <span class="math inline">\(x = 10\)</span>
and <span class="math inline">\(x = 20\)</span> respectively. These are the <span class="math inline">\(y\)</span> values the model <em>predicts</em> for
specific values of <span class="math inline">\(x\)</span>. We were already predicting above; predicting the salary
for a player that scores <span class="math inline">\(10\)</span> or <span class="math inline">\(20\)</span> points per game on average.
This is what prediction with a linear regression model comes down to, the value
of <span class="math inline">\(y\)</span> the model estimates for any given value of <span class="math inline">\(x\)</span>.</p>
<p>You can see in the plot that both predictions fall exactly on the regression
line and the same would be true for any other value of <span class="math inline">\(x\)</span> we plug into the
formula. That is because the regression line represents all the knowledge about
the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> the model possesses. It assumes that for an
increase in points per game of <span class="math inline">\(1\)</span>, the salary increases by <span class="math inline">\(552843\)</span>.</p>
<p>But we can also see that most of the actual data points do not fall exactly on
the regression line. This is the error inherent to our model. The line was
chosen in a way to minimise this error but it still persists. This also means
that all our predictions will have a potential error. Not every player that
scores <span class="math inline">\(10\)</span> points per game will actually earn <span class="math inline">\(4,676,516\$\)</span>. As we can see in
the plot, many earn way more and many less than the predicted value.</p>
<p>This is what this session will be about, optimising our model to decrease the
error and thus make more accurate predictions for our outcome.</p>
</div>
<div id="why-predict-at-all" class="section level3 hasAnchor" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Why predict at all?<a href="pm-t.html#why-predict-at-all" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now you may rightfully ask: “Why do we want to predict salaries if we already
know the actual salaries!?” Fair point. Prediction is commonly used to predict
values which we do not know.</p>
<p>Imagine there are some players that do not report their salaries. We could
predict their salaries based on what we know from players who are similar to
them in other observable characteristics, like the points scored per game above.
This can be helpful to fill in missing values in our data.</p>
<p>Or imagine we want to predict the salary of a hypothetical player that does not
exist. We can basically invent this player by setting other variables included
in the model to values that we see fit and receive a prediction for the salary
received. We can even invent multiple players, systemically varying variables of
interest and compare the predictions. Here you can see how it could get
scientifically interesting. We could for example systematically vary the ethnic
background of fictitious NBA players to assess any race related pay gaps. While
this question sounds interesting, remember that we can only use variables that
were in the data used to compute the model. The model simply does not know what
effect the ethnic background has on the salary, as no data was presented to it
to estimate the relationship.</p>
<p>Prediction could also be used for more practical purposes. A player could use
prediction to assess how much they could or should earn, if they increases their
performance to certain values; maybe to strengthen their position in future
contract negotiations. Sports analysts could also use predictions to forecast
how much a player will make next year, depending on their performance in the
current season.</p>
<p>In the end it comes down to the research question. If we are interested in the
effect one or several <span class="math inline">\(x\)</span> variables have on the outcome, we should use
regression to estimate the effects with as little bias as possible. If we are
interested in the value a <span class="math inline">\(y\)</span> variable will have for certain sets of <span class="math inline">\(x\)</span> values,
we should use regression to predict the value of <span class="math inline">\(y\)</span> with the highest accuracy
possible. But this is no afterthought. We have to know what we are interested
in to design the model in the correct way. As we will see below, the best model
used for estimation is not necessarily the best model for prediction, and vice
versa.</p>
</div>
</div>
<div id="making-better-predictions" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Making better predictions<a href="pm-t.html#making-better-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we worked with the NBA data the <a href="lin-a.html#lin-a">last time</a> we have already moved
beyond a simple linear regression, as our DAG led us to conclude that we also
have to include the position a player is occupying to receive an unbiased
estimate for the effect of points on salary. Maybe this model could also make
better predictions compared to the simple regression.</p>
<p>Let us rerun the model and save it further analysis. As we had transformed our
outcome variable using a logarithmus naturalis, we should this again.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="pm-t.html#cb133-1" tabindex="-1"></a>data_nba <span class="ot">&lt;-</span> data_nba <span class="sc">%&gt;%</span> </span>
<span id="cb133-2"><a href="pm-t.html#cb133-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">salary_log =</span> <span class="fu">log</span>(salary))</span>
<span id="cb133-3"><a href="pm-t.html#cb133-3" tabindex="-1"></a></span>
<span id="cb133-4"><a href="pm-t.html#cb133-4" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary_log <span class="sc">~</span> career_PTS <span class="sc">+</span> <span class="fu">I</span>(career_PTS<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb133-5"><a href="pm-t.html#cb133-5" tabindex="-1"></a>         position_center <span class="sc">+</span> position_sf <span class="sc">+</span> position_pf <span class="sc">+</span> position_sg <span class="sc">+</span> position_pg,</span>
<span id="cb133-6"><a href="pm-t.html#cb133-6" tabindex="-1"></a>         <span class="at">data =</span> data_nba)</span></code></pre></div>
<p>We now could start plugging in some <span class="math inline">\(x\)</span> values into our formula and using the
coefficients computed in the model to predict some <span class="math inline">\(y\)</span> values. Nothing is
stopping us from doing so. In fact we have done so <a href="lin-a.html#lin-a">before</a>.
But there are more convenient ways to approach this.</p>
<p>We can for one extract the <em>fitted values</em> from our model object and add them to
the data. These fitted values are the <span class="math inline">\(y\)</span> values the model estimated for each
observation; the values falling on the regression line the model assumed as the
best fit. They are saved as a variable in the model object and can be extracted
with the syntax <code>model$fitted.values</code>. When we add them to our original data,
we can compare the actual salaries to the predicted ones. As we have used a
logarithmic transformation on our outcome, the predicted values will also be on
a lograithmic scale. To make the values interpretable, we should reverse this
transformation.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="pm-t.html#cb134-1" tabindex="-1"></a>data_nba<span class="sc">$</span>predicted_values_m1 <span class="ot">&lt;-</span> m1<span class="sc">$</span>fitted.values</span>
<span id="cb134-2"><a href="pm-t.html#cb134-2" tabindex="-1"></a></span>
<span id="cb134-3"><a href="pm-t.html#cb134-3" tabindex="-1"></a>data_nba <span class="sc">%&gt;%</span></span>
<span id="cb134-4"><a href="pm-t.html#cb134-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted_values_m1 =</span> <span class="fu">exp</span>(predicted_values_m1)) <span class="sc">%&gt;%</span> </span>
<span id="cb134-5"><a href="pm-t.html#cb134-5" tabindex="-1"></a>  <span class="fu">select</span>(id, name, salary, predicted_values_m1, season_start) <span class="sc">%&gt;%</span> </span>
<span id="cb134-6"><a href="pm-t.html#cb134-6" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">20</span>)</span></code></pre></div>
<pre><code>## # A tibble: 20 × 5
##    id        name                  salary predicted_values_m1 season_start
##    &lt;chr&gt;     &lt;chr&gt;                  &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;
##  1 abdulma02 Mahmoud Abdul-Rauf    798500            4480779.         2000
##  2 abdulta01 Tariq Abdul-Wahad    1411000            1546827.         1998
##  3 abdulta01 Tariq Abdul-Wahad    1594920            1546827.         1999
##  4 abdulta01 Tariq Abdul-Wahad    4500000            1546827.         2000
##  5 abdulta01 Tariq Abdul-Wahad    5062500            1546827.         2001
##  6 abdulta01 Tariq Abdul-Wahad    5625000            1546827.         2002
##  7 abdulta01 Tariq Abdul-Wahad    6187500            1546827.         2003
##  8 abdulta01 Tariq Abdul-Wahad    6750000            1546827.         2004
##  9 abdulta01 Tariq Abdul-Wahad    3656250            1546827.         2005
## 10 abdulta01 Tariq Abdul-Wahad    1968750            1546827.         2006
## 11 abdursh01 Shareef Abdur-Rahim  9000000           12377347.         1999
## 12 abdursh01 Shareef Abdur-Rahim 10130000           12377347.         2000
## 13 abdursh01 Shareef Abdur-Rahim 11250000           12377347.         2001
## 14 abdursh01 Shareef Abdur-Rahim 12375000           12377347.         2002
## 15 abdursh01 Shareef Abdur-Rahim 13500000           12377347.         2003
## 16 abdursh01 Shareef Abdur-Rahim 14625000           12377347.         2004
## 17 abdursh01 Shareef Abdur-Rahim  5000000           12377347.         2005
## 18 abdursh01 Shareef Abdur-Rahim  5400000           12377347.         2006
## 19 abdursh01 Shareef Abdur-Rahim  5800000           12377347.         2007
## 20 abdursh01 Shareef Abdur-Rahim  6600000           12377347.         2009</code></pre>
<p>Looking at these first <span class="math inline">\(20\)</span> rows from the output points to two problems.
The first one is that a single player gets the same predicted salary for each
observation, i.e. each year that they played in the NBA. This is the case because
the data on performance are career averages. We do not know how many points a
player scored on average in a specific season, we only know how many they scored
on average over their career. This is a limitation inherent to our data that we
can not solve here completely. One way to address this at least partially, is to
add variables which may explain the variation of salaries over a career. We
would still prefer performance data by season, but we have to accept that we are
right now trying to predict accurate salaries from data that combines season-
and career-specific variables. This has to be kept in mind when assessing the
quality of all our models.</p>
<p>The second problem is that the predicted values are way off for most
observations. We can not conclude the overall error of our model, more on that
below, by looking at a few observations, but at least the first 20 point to very
inaccurate predictions. We may need further variables in our model to increase
the quality of predictions.</p>
<p>Like for every R problem, there are multiple alternatives to doing it the base R
way, implemented in the many available packages. This is of course also true for
plucking fitted values and other model related values out of a model object.
One alternative, and more <em>tidyversy</em>, way is to use <code>augment()</code> from <code>broom</code>.
<code>augment()</code> takes all observation related values, including the fitted values,
from the model object and presents them as a tibble. If we add the optional
argument <code>data = ...</code>, we can specify the tibble containing the actual data
and have <code>augment()</code> add the values from the model to it.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="pm-t.html#cb136-1" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb136-2"><a href="pm-t.html#cb136-2" tabindex="-1"></a></span>
<span id="cb136-3"><a href="pm-t.html#cb136-3" tabindex="-1"></a>data_nba_m1 <span class="ot">&lt;-</span> <span class="fu">augment</span>(m1, <span class="at">data =</span> data_nba)</span></code></pre></div>
<p>The resulting tibble can be used as above. Remember that we still have to
retransform the predicted values for salary if we want to see the actual dollars
paid.</p>
<p>To get a better idea of the quality of our predictions, we can plot the actual
versus the predicted values. If we made perfect predictions, all data points
should lie perfectly on the diagonal. The more they dispersed around it, the
further off our predictions were.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="pm-t.html#cb137-1" tabindex="-1"></a>data_nba_m1 <span class="sc">%&gt;%</span></span>
<span id="cb137-2"><a href="pm-t.html#cb137-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> salary_log, <span class="at">y =</span> .fitted)) <span class="sc">+</span></span>
<span id="cb137-3"><a href="pm-t.html#cb137-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb137-4"><a href="pm-t.html#cb137-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/fit_vs_actual-1.png" width="672" /></p>
<p>We can see that for most data points there is a clear difference between the
predicted and the actual values. May be we should try to improve our model,
before moving on.</p>
</div>
<div id="making-even-better-predictions" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Making even better predictions?<a href="pm-t.html#making-even-better-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>From our DAG we concluded that the variables used in <code>m1</code> are the correct
adjustment set to estimate the effect from points per game on salary. But there
the goal was just that, <em>estimation</em> of an effect. Now the goal has changed. We
want to make accurate <em>predictions</em> and thus we should reconsider our decisions.
There may be further variables of which we think that they explain a part of the
variation in salaries. The better we can capture variation in salaries between
players, and thus by the values of their variables, the more precise our
prediction will be.</p>
<p>Let us return to our DAG.</p>
<p><img src="_main_files/figure-html/dag_again-1.png" width="672" /></p>
<p>For estimation we concluded that we only have to include the position besides
the points per game. This was correct, but for prediction we want to consider
all the other factors that have an effect on salary. Different teams may be ably
to pay different salaries, so this should be included. There may also be a
general increase of wages over time, so we should include a variable on the
season an observation, and thus their salary, was recorded in.
We also added an age variable to the DAG. This was not present before, but
should also have an impact on the salary as it represents the time a player has
already spent playing and thus their experience.</p>
<p>We can now run a new model including these additional variables. We also
construct a new object holding the original data and the predictions made by
<code>m2</code>.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="pm-t.html#cb138-1" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary_log <span class="sc">~</span> career_PTS <span class="sc">+</span> <span class="fu">I</span>(career_PTS<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb138-2"><a href="pm-t.html#cb138-2" tabindex="-1"></a>           position_center <span class="sc">+</span> position_sf <span class="sc">+</span> position_pf <span class="sc">+</span> position_sg <span class="sc">+</span> position_pg <span class="sc">+</span></span>
<span id="cb138-3"><a href="pm-t.html#cb138-3" tabindex="-1"></a>           age <span class="sc">+</span> team <span class="sc">+</span> season_start,</span>
<span id="cb138-4"><a href="pm-t.html#cb138-4" tabindex="-1"></a>         <span class="at">data =</span> data_nba)</span>
<span id="cb138-5"><a href="pm-t.html#cb138-5" tabindex="-1"></a></span>
<span id="cb138-6"><a href="pm-t.html#cb138-6" tabindex="-1"></a>data_nba_m2 <span class="ot">&lt;-</span> <span class="fu">augment</span>(m2, <span class="at">data =</span> data_nba)</span></code></pre></div>
<p>Does this model make better predictions compared to <code>m1</code>? To assess this we have
to look at techniques to actually compare two models to each other.</p>
<div id="comparing-models" class="section level3 hasAnchor" number="10.5.1">
<h3><span class="header-section-number">10.5.1</span> Comparing models<a href="pm-t.html#comparing-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are multiple approaches to comparing the accuracy of linear regression
models, two of which we will address here.</p>
<p>One approach is already familiar to you: comparing the <span class="math inline">\(R^2\)</span> values.
If you remember <a href="lin-t-3.html#lin-t-3">session 7</a>, this measures how much of the variation
in the outcome can be explained by our set of independent variables.</p>
<p>We can of course access the values by using <code>summary()</code>, but <code>glance()</code> may be
more convenient here, as we do not want to see all coefficients but only the
model fit statistics.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="pm-t.html#cb139-1" tabindex="-1"></a><span class="fu">glance</span>(m1)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 12
##   r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.398         0.397  1.04      917.       0     7 -14172. 28363. 28428.
## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="pm-t.html#cb141-1" tabindex="-1"></a><span class="fu">glance</span>(m2)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 12
##   r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.428         0.426  1.01      165.       0    44 -13921. 27934. 28264.
## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
<p>We can see the <span class="math inline">\(R^2\)</span> actually increased, but not by that much. While <code>m1</code> was
able to explain about <span class="math inline">\(39.8\%\)</span> of the variation in salaries, <code>m2</code> can explain
about <span class="math inline">\(42.8\%\)</span>, an increase of <span class="math inline">\(3\%\)</span>. While the increase is small, the <span class="math inline">\(R^2\)</span>
indicates that <code>m2</code> will be able to make somewhat better predictions compared to
<code>m1</code>.</p>
<p>An additional common measure for comparing linear regression models is the
<em>mean squared error</em> (MSE). The MSE represents the average error of our
predictions compared to their actual values, quantifying the discrepancy between
the model’s predictions and the true observations. The larger the differences
are on average, the larger the MSE will be, thus a lower value is actually
better.</p>
<p>We can use <code>mse()</code> from the <code>Metrics</code> package to compute the MSE. All we have to
do is first indicate the predicted and second the actual values in our data as
arguments.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="pm-t.html#cb143-1" tabindex="-1"></a><span class="fu">library</span>(Metrics)</span>
<span id="cb143-2"><a href="pm-t.html#cb143-2" tabindex="-1"></a></span>
<span id="cb143-3"><a href="pm-t.html#cb143-3" tabindex="-1"></a><span class="fu">mse</span>(data_nba_m1<span class="sc">$</span>.fitted, data_nba_m1<span class="sc">$</span>salary_log)</span></code></pre></div>
<pre><code>## [1] 1.078827</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="pm-t.html#cb145-1" tabindex="-1"></a><span class="fu">mse</span>(data_nba_m2<span class="sc">$</span>.fitted, data_nba_m2<span class="sc">$</span>salary_log)</span></code></pre></div>
<pre><code>## [1] 1.024443</code></pre>
<p>In computing the MSE, the square of the differences between predicted and actual
values is used in computing the mean. Without going into the reasons, this makes
sense mathematically. The only problem is, that the resulting measure is not
as readily interpretable anymore. We can solve this by using the
<em>root mean squared error</em> (RMSE) instead. This is just the square root of the
MSE but it presents us a measure that is interpretable in units of
the outcome variable.</p>
<p>We can use <code>rmse</code> from <code>Metrics</code> to do just that. As we had logarithmised our
outcome and thus our predictions are as well, we should construct a variable
that reverses the transformation for the predicted values and compare it to the
untransformed salaries. If we do not, the RMSE will still be hardly
interpretable as we are comparing two logarithms to each other, which is not
very helpful.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="pm-t.html#cb147-1" tabindex="-1"></a>data_nba_m1 <span class="ot">&lt;-</span> data_nba_m1 <span class="sc">%&gt;%</span> </span>
<span id="cb147-2"><a href="pm-t.html#cb147-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">exp</span>(.fitted))</span>
<span id="cb147-3"><a href="pm-t.html#cb147-3" tabindex="-1"></a></span>
<span id="cb147-4"><a href="pm-t.html#cb147-4" tabindex="-1"></a>data_nba_m2 <span class="ot">&lt;-</span> data_nba_m2 <span class="sc">%&gt;%</span> </span>
<span id="cb147-5"><a href="pm-t.html#cb147-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">exp</span>(.fitted))</span>
<span id="cb147-6"><a href="pm-t.html#cb147-6" tabindex="-1"></a></span>
<span id="cb147-7"><a href="pm-t.html#cb147-7" tabindex="-1"></a><span class="fu">rmse</span>(data_nba_m1<span class="sc">$</span>predicted, data_nba_m1<span class="sc">$</span>salary)</span></code></pre></div>
<pre><code>## [1] 3946193</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="pm-t.html#cb149-1" tabindex="-1"></a><span class="fu">rmse</span>(data_nba_m2<span class="sc">$</span>predicted, data_nba_m2<span class="sc">$</span>salary)</span></code></pre></div>
<pre><code>## [1] 3813271</code></pre>
<p>Both measures tell the same story, <code>m2</code> performs somewhat better compared to
<code>m2</code>. THe RMSE can even tell us what “somewhat” means. Model 1 is on average
<span class="math inline">\(3,946,193\$\)</span> off, model 2 <span class="math inline">\(3,813,271\$\)</span>. Using model 2, we make predictions
which are on average <span class="math inline">\(132,922\$\)</span> less wrong, compared to model 1. This is nice,
but if we compare it to the overall RMSE of model 2, our predictions are still
substantially off.</p>
<p>We could try to improve our model by including further variables
that also explain parts of the variation of <code>salary</code>. We will keep this for the
<a href="ml.html#ml">next session on machine learning</a>. For now we will accept the quality of
predictions we have gained and use the <code>m2</code> to predict salaries for imaginary
NBA players.</p>
</div>
</div>
<div id="predicting-hypotheticals" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Predicting hypotheticals<a href="pm-t.html#predicting-hypotheticals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we have stated in the beginning of this session, one usage we can get out
of prediction is comparing the outcomes a model predicts for hypothetical
observations. The advantage to comparing the predictions of actual observations
is that we can systematically manipulate those variables of interest to us
while keeping all others at the same value.</p>
<p>Before we were interested in the difference the point average and the position
played make for the salary received. We can approach this by using
predictions too. For this we will have to construct some hypothetical cases and
enter their data into a new tibble or data frame. Let us once again compare
centers and point guards who in this case score <span class="math inline">\(10\)</span> or <span class="math inline">\(14\)</span> points per game on
average. The tibble has to include all the independent variables used in
computing the model. So we decide to inspect the predictions for players that
are 20 years of age and played for the New York Knicks in the season of 2017.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="pm-t.html#cb151-1" tabindex="-1"></a>prediction.data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb151-2"><a href="pm-t.html#cb151-2" tabindex="-1"></a>  <span class="at">career_PTS =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">14</span>, <span class="dv">10</span>, <span class="dv">14</span>),</span>
<span id="cb151-3"><a href="pm-t.html#cb151-3" tabindex="-1"></a>  <span class="at">position_center =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb151-4"><a href="pm-t.html#cb151-4" tabindex="-1"></a>  <span class="at">position_pg =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb151-5"><a href="pm-t.html#cb151-5" tabindex="-1"></a>  <span class="at">position_sf =</span> <span class="dv">0</span>,</span>
<span id="cb151-6"><a href="pm-t.html#cb151-6" tabindex="-1"></a>  <span class="at">position_pf =</span> <span class="dv">0</span>,</span>
<span id="cb151-7"><a href="pm-t.html#cb151-7" tabindex="-1"></a>  <span class="at">position_sg =</span> <span class="dv">0</span>,</span>
<span id="cb151-8"><a href="pm-t.html#cb151-8" tabindex="-1"></a>  <span class="at">age =</span> <span class="dv">20</span>,</span>
<span id="cb151-9"><a href="pm-t.html#cb151-9" tabindex="-1"></a>  <span class="at">team =</span> <span class="st">&quot;New York Knicks&quot;</span>,</span>
<span id="cb151-10"><a href="pm-t.html#cb151-10" tabindex="-1"></a>  <span class="at">season_start =</span> <span class="dv">2017</span></span>
<span id="cb151-11"><a href="pm-t.html#cb151-11" tabindex="-1"></a>  )</span></code></pre></div>
<p>We can now use our model to make predictions for these imaginary players. What
salary can they expect based on their characteristics? To predict these values
we can use the base R function <code>predict()</code>. We have to supply it with the model
to be used as the first argument and with the new data as second. To make
sense of the predictions, we should again reverse the logarithmic
transformation.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="pm-t.html#cb152-1" tabindex="-1"></a>prediction.data<span class="sc">$</span>predicted_values <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2, </span>
<span id="cb152-2"><a href="pm-t.html#cb152-2" tabindex="-1"></a>                                            prediction.data)</span>
<span id="cb152-3"><a href="pm-t.html#cb152-3" tabindex="-1"></a></span>
<span id="cb152-4"><a href="pm-t.html#cb152-4" tabindex="-1"></a>prediction.data <span class="sc">%&gt;%</span> </span>
<span id="cb152-5"><a href="pm-t.html#cb152-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted_values =</span> <span class="fu">exp</span>(predicted_values)) <span class="sc">%&gt;%</span> </span>
<span id="cb152-6"><a href="pm-t.html#cb152-6" tabindex="-1"></a>  <span class="fu">select</span>(position_center, position_pg, career_PTS, predicted_values)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 4
##   position_center position_pg career_PTS predicted_values
##             &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;
## 1               1           0         10         4474038.
## 2               1           0         14         7674367.
## 3               0           1         10         2694957.
## 4               0           1         14         4622690.</code></pre>
<p>We can see that the model predicts a substantial salary increase for making two
more baskets, i.e. 4 points, on average each game, for both guards and centers.
We also see that this increase is even stepper for centers, who make more money
given the same amount of points anyway.</p>
<div id="adressing-uncertainty" class="section level3 hasAnchor" number="10.6.1">
<h3><span class="header-section-number">10.6.1</span> Adressing uncertainty<a href="pm-t.html#adressing-uncertainty" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we have seen above and alluded to over the last weeks, our models always
have some amount of uncertainty. In fact we have seen that <code>m2</code> still is
<span class="math inline">\(3,813,271\$\)</span> off on average. Those are not peanuts. So what are our predictions
actually saying? That a center who scores <span class="math inline">\(10\)</span> points per game, is 20 years
of age and plays for the Knicks in 2017 will most definitely make <span class="math inline">\(4,474,038\$\)</span>?
No, this is just the best guess the model can make based on the data it has
seen. It will still be somewhat uncertain about the predicted values.
We can assess this by looking at the confidence and prediction intervals.</p>
<p>Confidence intervals basically tell you the following: “If we repeated our study
on different samples of people many times, then <span class="math inline">\(95\%\)</span> of confidence intervals
would include the true mean population value.” This leaves <span class="math inline">\(5\%\)</span> of repetitions
where the confidecne intervals do not include the true value. If we are in the
<span class="math inline">\(95\%\)</span> or the <span class="math inline">\(5\%\)</span> is unknown to us. But what we can say is that we are <span class="math inline">\(95\%\)</span>
confident that our confidence interval includes the true mean value.
In practice the confidence interval tells us the range of values for which we
have this confidence of <span class="math inline">\(95\%\)</span>, without knowing if our interval actually
includes the true mean value.</p>
<p>We can instruct <code>predict()</code> to return the confidence intervals for our
predictions by adding the argument <code>interval = "confidence"</code>. In this code we
use <code>bind_cols()</code> to add the confidence intervals and predicted values to our
new data. We then reverse the logarithmic transformation for all 3 values to
be more interpretable.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="pm-t.html#cb154-1" tabindex="-1"></a>prediction.data <span class="sc">%&gt;%</span> </span>
<span id="cb154-2"><a href="pm-t.html#cb154-2" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(m2, prediction.data, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb154-3"><a href="pm-t.html#cb154-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted_values =</span> <span class="fu">exp</span>(fit)) <span class="sc">%&gt;%</span></span>
<span id="cb154-4"><a href="pm-t.html#cb154-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">ci_lwr =</span> <span class="fu">exp</span>(lwr)) <span class="sc">%&gt;%</span></span>
<span id="cb154-5"><a href="pm-t.html#cb154-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">ci_upr =</span> <span class="fu">exp</span>(upr)) <span class="sc">%&gt;%</span></span>
<span id="cb154-6"><a href="pm-t.html#cb154-6" tabindex="-1"></a>  <span class="fu">select</span>(position_center, position_pg, career_PTS, predicted_values, ci_lwr, ci_upr)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 6
##   position_center position_pg career_PTS predicted_values   ci_lwr   ci_upr
##             &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1               1           0         10         4474038. 3920882. 5105234.
## 2               1           0         14         7674367. 6711636. 8775195.
## 3               0           1         10         2694957. 2367270. 3068004.
## 4               0           1         14         4622690. 4054310. 5270750.</code></pre>
<p>We can see that the range of values for which we can be <span class="math inline">\(95\%\)</span> confident that
the true mean value is included differs by observation. For example, instead of
stating that a center who scores <span class="math inline">\(10\)</span> points per game, is 20 years
of age and plays for the Knicks in 2017 will most definitely make <span class="math inline">\(4,474,038\$\)</span>
on average we would state that we are <span class="math inline">\(95\%\)</span> confident that the true mean
population value for such a player lies between <span class="math inline">\(3,920,882\$\)</span> and <span class="math inline">\(5,105,234\$\)</span>.</p>
<p>But what confidence intervals present to us is the uncertainty around a mean
value. We can be <span class="math inline">\(95\%\)</span> confident that the true population mean for the
described player lies between the lower and upper bounds of this interval.
If we are more interested in the uncertainty of the specific predicted salary
for our hypothetical player, we can use prediction intervals instead. These do
not give us the confidence for the mean of all players with these
characteristics but around the specific prediction we made for this one player,
so for a specific value.</p>
<p>We can access prediction intervals with the same code as above, only changing
the argument to <code>interval = "prediction"</code>.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="pm-t.html#cb156-1" tabindex="-1"></a>prediction.data <span class="sc">%&gt;%</span> </span>
<span id="cb156-2"><a href="pm-t.html#cb156-2" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(m2, prediction.data, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb156-3"><a href="pm-t.html#cb156-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted_values =</span> <span class="fu">exp</span>(fit)) <span class="sc">%&gt;%</span></span>
<span id="cb156-4"><a href="pm-t.html#cb156-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">ci_lwr =</span> <span class="fu">exp</span>(lwr)) <span class="sc">%&gt;%</span></span>
<span id="cb156-5"><a href="pm-t.html#cb156-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">ci_upr =</span> <span class="fu">exp</span>(upr)) <span class="sc">%&gt;%</span></span>
<span id="cb156-6"><a href="pm-t.html#cb156-6" tabindex="-1"></a>  <span class="fu">select</span>(position_center, position_pg, career_PTS, predicted_values, ci_lwr, ci_upr)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 6
##   position_center position_pg career_PTS predicted_values   ci_lwr    ci_upr
##             &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1               1           0         10         4474038.  609748. 32828335.
## 2               1           0         14         7674367. 1045764. 56318577.
## 3               0           1         10         2694957.  367341. 19771271.
## 4               0           1         14         4622690.  630040. 33917321.</code></pre>
<p>We can see that the prediction intervals are much, in our case very much, wider
compared to the confidence intervals. The range were we expect, with <span class="math inline">\(95\%\)</span>
confidence, the true population mean to lie is much narrower than the range of
salaries any individual player with the same characteristics can expect.
Our model has a confidence of <span class="math inline">\(95\%\)</span> that centers who scored <span class="math inline">\(10\)</span> points per
game, are <span class="math inline">\(20\)</span> years of age and played for the Knicks in 2017 receive a salary
between <span class="math inline">\(609,748\$\)</span> and <span class="math inline">\(32,828,335\$\)</span>. That is quite the range.</p>
<p>What does this imply for our model and our quality of predictions?
The confidence intervals tell us that we can be somewhat certain that we have a
good measure of the mean value for all players with the characteristics we have
defined, while the interval still indicates that our measure could be about
<span class="math inline">\(500,000\$\)</span> of in any direction.
Our individual prediction for this specific hypothetical player on the other
hand is very uncertain.
The actual salary could be way lower and especially way higher than we have
predicted. This indicates that our model is still not very good in making
accurate predictions. We will address this and try to further improve our model
when we turn to machine learning in the <a href="ml.html#ml">next session</a>.</p>
<p>One last note on confidence and prediction intervals: That they are set to
<span class="math inline">\(95\%\)</span> is a scientific convention. It is basically an arbitrarily chosen value
that many scientists agreed upon, at least in the social sciences. In some other
fields other thresholds are used, e.g. <span class="math inline">\(99\%\)</span>. We can tell <code>predict()</code> to
use a different threshold by adding the argument <code>level = 0.99</code> with any
value of your choosing. Still, if you have no clear justification for using a
different value to <span class="math inline">\(95\%\)</span>, it makes sense to stick to the standard, as your
research will be more comparable to other research using the same threshold.</p>
</div>
</div>
<div id="further-resources-2" class="section level2 hasAnchor" number="10.7">
<h2><span class="header-section-number">10.7</span> Further resources<a href="pm-t.html#further-resources-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="resources">
<ul>
<li><a href="https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/">Linear Regression for Predictive Modeling in R - Dataquest</a>: This is a blog post that explains how to use linear regression for predictive modeling in R, using the trees data set as an example. It covers how to fit, visualize, and evaluate linear regression models, as well as how to calculate confidence and prediction intervals.</li>
<li><a href="http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/">Predict in R: Model Predictions and Confidence Intervals - STHDA</a>: This is a web page that shows how to use the predict() function in R to make predictions from a fitted model object. It also explains the difference between confidence intervals and prediction intervals, and how to calculate them using base R or the broom package.</li>
<li><a href="https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/">Mean Squared Error or R-Squared – Which one to use? - Analytics Yogi</a>: A blog post that expands on using <span class="math inline">\(R^2\)</span> and MSE for comparing the performance of linear regression models.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lin-e.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ml.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/10-prediction-theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
