<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 5 Linear Regression - Theory I: Simple Linear Regression | Data Analysis with R for Social Scientists</title>
  <meta name="description" content="In this course, you will learn how to analyse data using regression in R." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content=" 5 Linear Regression - Theory I: Simple Linear Regression | Data Analysis with R for Social Scientists" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="In this course, you will learn how to analyse data using regression in R." />
  <meta name="github-repo" content="jaspertjaden/DataAnalysisR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 5 Linear Regression - Theory I: Simple Linear Regression | Data Analysis with R for Social Scientists" />
  
  <meta name="twitter:description" content="In this course, you will learn how to analyse data using regression in R." />
  

<meta name="author" content="Jakob Tures &amp; Jasper Tjaden" />


<meta name="date" content="2023-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dags-1.html"/>
<link rel="next" href="lin-t-2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the Authors</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#github-repo"><i class="fa fa-check"></i>GitHub Repo</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-sem.html"><a href="intro-sem.html"><i class="fa fa-check"></i><b>1</b> Introduction to Seminar</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-sem.html"><a href="intro-sem.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="eda-1.html"><a href="eda-1.html"><i class="fa fa-check"></i><b>2</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda-1.html"><a href="eda-1.html#objectives"><i class="fa fa-check"></i><b>2.1</b> Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="eda-1.html"><a href="eda-1.html#r-functions-covered-this-week"><i class="fa fa-check"></i><b>2.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="2.3" data-path="eda-1.html"><a href="eda-1.html#what-is-eda-and-why-is-it-so-important"><i class="fa fa-check"></i><b>2.3</b> What is <em>EDA</em> and why is it so important?</a></li>
<li class="chapter" data-level="2.4" data-path="eda-1.html"><a href="eda-1.html#importing-data-into-r"><i class="fa fa-check"></i><b>2.4</b> Importing data into R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="eda-1.html"><a href="eda-1.html#merge-datasets"><i class="fa fa-check"></i><b>2.4.1</b> Merge datasets</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="eda-1.html"><a href="eda-1.html#clean-dataset"><i class="fa fa-check"></i><b>2.5</b> Clean dataset</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="eda-1.html"><a href="eda-1.html#mutating-variables"><i class="fa fa-check"></i><b>2.5.1</b> Mutating variables</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="eda-1.html"><a href="eda-1.html#explore-the-complete-dataset"><i class="fa fa-check"></i><b>2.6</b> Explore the complete dataset</a></li>
<li class="chapter" data-level="2.7" data-path="eda-1.html"><a href="eda-1.html#explore-individual-variables"><i class="fa fa-check"></i><b>2.7</b> Explore individual variables</a></li>
<li class="chapter" data-level="2.8" data-path="eda-1.html"><a href="eda-1.html#moving-on"><i class="fa fa-check"></i><b>2.8</b> Moving on</a></li>
<li class="chapter" data-level="2.9" data-path="eda-1.html"><a href="eda-1.html#further-resources"><i class="fa fa-check"></i><b>2.9</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="eda-2.html"><a href="eda-2.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis - Exercise</a>
<ul>
<li class="chapter" data-level="3.1" data-path="eda-2.html"><a href="eda-2.html#what-is-r-markdown"><i class="fa fa-check"></i><b>3.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="3.2" data-path="eda-2.html"><a href="eda-2.html#creating-a-r-markdown-file"><i class="fa fa-check"></i><b>3.2</b> Creating a R Markdown file</a></li>
<li class="chapter" data-level="3.3" data-path="eda-2.html"><a href="eda-2.html#writing-in-r-markdown"><i class="fa fa-check"></i><b>3.3</b> Writing in R Markdown</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="eda-2.html"><a href="eda-2.html#document-components"><i class="fa fa-check"></i><b>3.3.1</b> Document components</a></li>
<li class="chapter" data-level="3.3.2" data-path="eda-2.html"><a href="eda-2.html#formatting"><i class="fa fa-check"></i><b>3.3.2</b> Formatting</a></li>
<li class="chapter" data-level="3.3.3" data-path="eda-2.html"><a href="eda-2.html#code-chunks"><i class="fa fa-check"></i><b>3.3.3</b> Code chunks</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="eda-2.html"><a href="eda-2.html#further-resources-1"><i class="fa fa-check"></i><b>3.4</b> Further resources</a></li>
<li class="chapter" data-level="3.5" data-path="eda-2.html"><a href="eda-2.html#eda---exercise"><i class="fa fa-check"></i><b>3.5</b> EDA - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dags-1.html"><a href="dags-1.html"><i class="fa fa-check"></i><b>4</b> DAGs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="dags-1.html"><a href="dags-1.html#objectives-1"><i class="fa fa-check"></i><b>4.1</b> Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="dags-1.html"><a href="dags-1.html#functions-covered"><i class="fa fa-check"></i><b>4.2</b> Functions Covered</a></li>
<li class="chapter" data-level="4.3" data-path="dags-1.html"><a href="dags-1.html#modelling"><i class="fa fa-check"></i><b>4.3</b> Modelling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="dags-1.html"><a href="dags-1.html#what-is-modelling"><i class="fa fa-check"></i><b>4.3.1</b> What is modelling?</a></li>
<li class="chapter" data-level="4.3.2" data-path="dags-1.html"><a href="dags-1.html#estimating-effects-vs.-prediction"><i class="fa fa-check"></i><b>4.3.2</b> Estimating effects vs. prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="dags-1.html"><a href="dags-1.html#dags"><i class="fa fa-check"></i><b>4.4</b> DAGs</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="dags-1.html"><a href="dags-1.html#directed-acyclical-graphs"><i class="fa fa-check"></i><b>4.4.1</b> Directed acyclical graphs</a></li>
<li class="chapter" data-level="4.4.2" data-path="dags-1.html"><a href="dags-1.html#patterns-of-relationships"><i class="fa fa-check"></i><b>4.4.2</b> Patterns of relationships</a></li>
<li class="chapter" data-level="4.4.3" data-path="dags-1.html"><a href="dags-1.html#adjustment-set"><i class="fa fa-check"></i><b>4.4.3</b> Adjustment set</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="dags-1.html"><a href="dags-1.html#nba-dag"><i class="fa fa-check"></i><b>4.5</b> NBA DAG</a></li>
<li class="chapter" data-level="4.6" data-path="dags-1.html"><a href="dags-1.html#resources"><i class="fa fa-check"></i><b>4.6</b> Resources</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="dags-1.html"><a href="dags-1.html#dagitty.net"><i class="fa fa-check"></i><b>4.6.1</b> dagitty.net</a></li>
<li class="chapter" data-level="4.6.2" data-path="dags-1.html"><a href="dags-1.html#how-to-use-dagitty"><i class="fa fa-check"></i><b>4.6.2</b> How to use dagitty()</a></li>
<li class="chapter" data-level="4.6.3" data-path="dags-1.html"><a href="dags-1.html#more-on-dags"><i class="fa fa-check"></i><b>4.6.3</b> More on DAGs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lin-t-1.html"><a href="lin-t-1.html"><i class="fa fa-check"></i><b>5</b> Linear Regression - Theory I: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lin-t-1.html"><a href="lin-t-1.html#objectives-2"><i class="fa fa-check"></i><b>5.1</b> Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lin-t-1.html"><a href="lin-t-1.html#what-is-linear-regression"><i class="fa fa-check"></i><b>5.2</b> What is Linear Regression</a></li>
<li class="chapter" data-level="5.3" data-path="lin-t-1.html"><a href="lin-t-1.html#examplary-research-question-data"><i class="fa fa-check"></i><b>5.3</b> Examplary research question &amp; data</a></li>
<li class="chapter" data-level="5.4" data-path="lin-t-1.html"><a href="lin-t-1.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lin-t-1.html"><a href="lin-t-1.html#regression-formula"><i class="fa fa-check"></i><b>5.4.1</b> Regression Formula</a></li>
<li class="chapter" data-level="5.4.2" data-path="lin-t-1.html"><a href="lin-t-1.html#regressing-grade-on-hours"><i class="fa fa-check"></i><b>5.4.2</b> Regressing <code>grade</code> on <code>hours</code></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lin-t-1.html"><a href="lin-t-1.html#moving-on-1"><i class="fa fa-check"></i><b>5.5</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lin-t-2.html"><a href="lin-t-2.html"><i class="fa fa-check"></i><b>6</b> Linear Regression - Theory II: Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lin-t-2.html"><a href="lin-t-2.html#objectives-3"><i class="fa fa-check"></i><b>6.1</b> Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lin-t-2.html"><a href="lin-t-2.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-additional-metric-variables"><i class="fa fa-check"></i><b>6.2.1</b> Adding additional metric variables</a></li>
<li class="chapter" data-level="6.2.2" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-dummy-variables"><i class="fa fa-check"></i><b>6.2.2</b> Adding dummy variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="lin-t-2.html"><a href="lin-t-2.html#adding-categorical-variables"><i class="fa fa-check"></i><b>6.2.3</b> Adding categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lin-t-2.html"><a href="lin-t-2.html#returning-to-our-research-question"><i class="fa fa-check"></i><b>6.3</b> Returning to our research question</a></li>
<li class="chapter" data-level="6.4" data-path="lin-t-2.html"><a href="lin-t-2.html#adressing-the-uncertainty"><i class="fa fa-check"></i><b>6.4</b> Adressing the uncertainty</a></li>
<li class="chapter" data-level="6.5" data-path="lin-t-2.html"><a href="lin-t-2.html#moving-on-2"><i class="fa fa-check"></i><b>6.5</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lin-t-3.html"><a href="lin-t-3.html"><i class="fa fa-check"></i><b>7</b> Linear Regression - Theory III: Diagnostics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lin-t-3.html"><a href="lin-t-3.html#objectives-4"><i class="fa fa-check"></i><b>7.1</b> Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="lin-t-3.html"><a href="lin-t-3.html#model-fit"><i class="fa fa-check"></i><b>7.2</b> Model fit</a></li>
<li class="chapter" data-level="7.3" data-path="lin-t-3.html"><a href="lin-t-3.html#regression-diagnostics"><i class="fa fa-check"></i><b>7.3</b> Regression diagnostics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="lin-t-3.html"><a href="lin-t-3.html#linearity"><i class="fa fa-check"></i><b>7.3.1</b> Linearity</a></li>
<li class="chapter" data-level="7.3.2" data-path="lin-t-3.html"><a href="lin-t-3.html#normally-distributed-residuals"><i class="fa fa-check"></i><b>7.3.2</b> Normally distributed residuals</a></li>
<li class="chapter" data-level="7.3.3" data-path="lin-t-3.html"><a href="lin-t-3.html#homoscedasticity"><i class="fa fa-check"></i><b>7.3.3</b> Homoscedasticity</a></li>
<li class="chapter" data-level="7.3.4" data-path="lin-t-3.html"><a href="lin-t-3.html#no-overly-influential-data-points"><i class="fa fa-check"></i><b>7.3.4</b> No overly influential data points</a></li>
<li class="chapter" data-level="7.3.5" data-path="lin-t-3.html"><a href="lin-t-3.html#no-multicollinearity"><i class="fa fa-check"></i><b>7.3.5</b> No (multi)collinearity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lin-t-3.html"><a href="lin-t-3.html#returning-to-our-research-question-1"><i class="fa fa-check"></i><b>7.4</b> Returning to our research question</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="lin-t-3.html"><a href="lin-t-3.html#interactions"><i class="fa fa-check"></i><b>7.4.1</b> Interactions</a></li>
<li class="chapter" data-level="7.4.2" data-path="lin-t-3.html"><a href="lin-t-3.html#regression-diagnostics-revisited"><i class="fa fa-check"></i><b>7.4.2</b> Regression diagnostics (revisited)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lin-t-3.html"><a href="lin-t-3.html#conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
<li class="chapter" data-level="7.6" data-path="lin-t-3.html"><a href="lin-t-3.html#resources-1"><i class="fa fa-check"></i><b>7.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lin-a.html"><a href="lin-a.html"><i class="fa fa-check"></i><b>8</b> Linear Regression - Application</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lin-a.html"><a href="lin-a.html#objectives-5"><i class="fa fa-check"></i><b>8.1</b> Objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lin-a.html"><a href="lin-a.html#r-functions-covered-this-week-1"><i class="fa fa-check"></i><b>8.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="8.3" data-path="lin-a.html"><a href="lin-a.html#research-question"><i class="fa fa-check"></i><b>8.3</b> Research question</a></li>
<li class="chapter" data-level="8.4" data-path="lin-a.html"><a href="lin-a.html#simple-linear-regression-in-r"><i class="fa fa-check"></i><b>8.4</b> Simple linear regression in R</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lin-a.html"><a href="lin-a.html#interpretation"><i class="fa fa-check"></i><b>8.4.1</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lin-a.html"><a href="lin-a.html#multiple-linear-regression-in-r"><i class="fa fa-check"></i><b>8.5</b> Multiple linear regression in R</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lin-a.html"><a href="lin-a.html#interpretation-1"><i class="fa fa-check"></i><b>8.5.1</b> Interpretation</a></li>
<li class="chapter" data-level="8.5.2" data-path="lin-a.html"><a href="lin-a.html#sidenote-adding-interactions"><i class="fa fa-check"></i><b>8.5.2</b> Sidenote: Adding interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lin-a.html"><a href="lin-a.html#regression-diagnostics-1"><i class="fa fa-check"></i><b>8.6</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lin-a.html"><a href="lin-a.html#skewed-outcome-variable"><i class="fa fa-check"></i><b>8.6.1</b> Skewed outcome variable</a></li>
<li class="chapter" data-level="8.6.2" data-path="lin-a.html"><a href="lin-a.html#non-linearity"><i class="fa fa-check"></i><b>8.6.2</b> Non-linearity</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lin-a.html"><a href="lin-a.html#returning-to-our-research-question-2"><i class="fa fa-check"></i><b>8.7</b> Returning to our research question</a></li>
<li class="chapter" data-level="8.8" data-path="lin-a.html"><a href="lin-a.html#moving-on-3"><i class="fa fa-check"></i><b>8.8</b> Moving on</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lin-e.html"><a href="lin-e.html"><i class="fa fa-check"></i><b>9</b> Linear Regression - Exercise</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lin-e.html"><a href="lin-e.html#linear-regression---exercise"><i class="fa fa-check"></i><b>9.1</b> Linear Regression - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pm-t.html"><a href="pm-t.html"><i class="fa fa-check"></i><b>10</b> Prediction</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pm-t.html"><a href="pm-t.html#objectives-6"><i class="fa fa-check"></i><b>10.1</b> Objectives</a></li>
<li class="chapter" data-level="10.2" data-path="pm-t.html"><a href="pm-t.html#r-functions-covered-this-week-2"><i class="fa fa-check"></i><b>10.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="10.3" data-path="pm-t.html"><a href="pm-t.html#making-predictions"><i class="fa fa-check"></i><b>10.3</b> Making predictions</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="pm-t.html"><a href="pm-t.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>10.3.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="pm-t.html"><a href="pm-t.html#why-predict-at-all"><i class="fa fa-check"></i><b>10.3.2</b> Why predict at all?</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="pm-t.html"><a href="pm-t.html#making-better-predictions"><i class="fa fa-check"></i><b>10.4</b> Making better predictions</a></li>
<li class="chapter" data-level="10.5" data-path="pm-t.html"><a href="pm-t.html#making-even-better-predictions"><i class="fa fa-check"></i><b>10.5</b> Making even better predictions?</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pm-t.html"><a href="pm-t.html#comparing-models"><i class="fa fa-check"></i><b>10.5.1</b> Comparing models</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="pm-t.html"><a href="pm-t.html#predicting-hypotheticals"><i class="fa fa-check"></i><b>10.6</b> Predicting hypotheticals</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="pm-t.html"><a href="pm-t.html#adressing-uncertainty"><i class="fa fa-check"></i><b>10.6.1</b> Adressing uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="pm-t.html"><a href="pm-t.html#further-resources-2"><i class="fa fa-check"></i><b>10.7</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>11</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ml.html"><a href="ml.html#objectives-7"><i class="fa fa-check"></i><b>11.1</b> Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="ml.html"><a href="ml.html#r-functions-covered-this-week-3"><i class="fa fa-check"></i><b>11.2</b> R functions covered this week</a></li>
<li class="chapter" data-level="11.3" data-path="ml.html"><a href="ml.html#intro-to-machine-learning"><i class="fa fa-check"></i><b>11.3</b> Intro to Machine Learning</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="ml.html"><a href="ml.html#training--and-testset"><i class="fa fa-check"></i><b>11.3.1</b> Training- and testset</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ml.html"><a href="ml.html#our-baseline-model"><i class="fa fa-check"></i><b>11.4</b> Our baseline model</a></li>
<li class="chapter" data-level="11.5" data-path="ml.html"><a href="ml.html#setting-up-tidymodels"><i class="fa fa-check"></i><b>11.5</b> Setting up tidymodels</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="ml.html"><a href="ml.html#splitting-the-data"><i class="fa fa-check"></i><b>11.5.1</b> Splitting the data</a></li>
<li class="chapter" data-level="11.5.2" data-path="ml.html"><a href="ml.html#defining-the-recipe"><i class="fa fa-check"></i><b>11.5.2</b> Defining the recipe</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="ml.html"><a href="ml.html#linear-regression"><i class="fa fa-check"></i><b>11.6</b> Linear regression</a></li>
<li class="chapter" data-level="11.7" data-path="ml.html"><a href="ml.html#random-forest"><i class="fa fa-check"></i><b>11.7</b> Random forest</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="ml.html"><a href="ml.html#tuning-the-forest"><i class="fa fa-check"></i><b>11.7.1</b> Tuning the forest</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="ml.html"><a href="ml.html#last-fit"><i class="fa fa-check"></i><b>11.8</b> Last fit</a></li>
<li class="chapter" data-level="11.9" data-path="ml.html"><a href="ml.html#going-forward"><i class="fa fa-check"></i><b>11.9</b> Going forward</a></li>
<li class="chapter" data-level="11.10" data-path="ml.html"><a href="ml.html#further-resources-3"><i class="fa fa-check"></i><b>11.10</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pm-a.html"><a href="pm-a.html"><i class="fa fa-check"></i><b>12</b> Prediction &amp; Machine Learning - Exercise</a>
<ul>
<li class="chapter" data-level="12.1" data-path="pm-a.html"><a href="pm-a.html#prediction-machine-learning---exercise"><i class="fa fa-check"></i><b>12.1</b> Prediction &amp; Machine Learning - Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="log-est.html"><a href="log-est.html"><i class="fa fa-check"></i><b>13</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="13.1" data-path="log-est.html"><a href="log-est.html#objectives-8"><i class="fa fa-check"></i><b>13.1</b> Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="log-est.html"><a href="log-est.html#functions-covered-in-this-week"><i class="fa fa-check"></i><b>13.2</b> Functions covered in this week</a></li>
<li class="chapter" data-level="13.3" data-path="log-est.html"><a href="log-est.html#what-is-logistic-regression"><i class="fa fa-check"></i><b>13.3</b> What is logistic regression?</a></li>
<li class="chapter" data-level="13.4" data-path="log-est.html"><a href="log-est.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>13.4</b> Logistic regression in R</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="log-est.html"><a href="log-est.html#preparing-the-outcome-variable"><i class="fa fa-check"></i><b>13.4.1</b> Preparing the outcome variable</a></li>
<li class="chapter" data-level="13.4.2" data-path="log-est.html"><a href="log-est.html#running-a-logistic-regression-in-r"><i class="fa fa-check"></i><b>13.4.2</b> Running a logistic regression in R</a></li>
<li class="chapter" data-level="13.4.3" data-path="log-est.html"><a href="log-est.html#interpretation-2"><i class="fa fa-check"></i><b>13.4.3</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="log-est.html"><a href="log-est.html#building-a-better-model"><i class="fa fa-check"></i><b>13.5</b> Building a better model</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="log-est.html"><a href="log-est.html#model-fit-1"><i class="fa fa-check"></i><b>13.5.1</b> Model fit</a></li>
<li class="chapter" data-level="13.5.2" data-path="log-est.html"><a href="log-est.html#interpretation-3"><i class="fa fa-check"></i><b>13.5.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="log-est.html"><a href="log-est.html#additional-pointers"><i class="fa fa-check"></i><b>13.6</b> Additional pointers</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="log-est.html"><a href="log-est.html#predicting-hypotheticals-1"><i class="fa fa-check"></i><b>13.6.1</b> Predicting hypotheticals</a></li>
<li class="chapter" data-level="13.6.2" data-path="log-est.html"><a href="log-est.html#diagnostics"><i class="fa fa-check"></i><b>13.6.2</b> Diagnostics</a></li>
<li class="chapter" data-level="13.6.3" data-path="log-est.html"><a href="log-est.html#comparing-linear-and-logistic-regression"><i class="fa fa-check"></i><b>13.6.3</b> Comparing linear and logistic regression</a></li>
<li class="chapter" data-level="13.6.4" data-path="log-est.html"><a href="log-est.html#logistic-regression-in-machine-learning-context"><i class="fa fa-check"></i><b>13.6.4</b> Logistic regression in Machine Learning context</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="log-est.html"><a href="log-est.html#further-resources-4"><i class="fa fa-check"></i><b>13.7</b> Further resources</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis with R for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lin-t-1" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number"> 5</span> Linear Regression - Theory I: Simple Linear Regression<a href="lin-t-1.html#lin-t-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The next three sessions will comprise an introduction for linear regressions.
We will look at the theoretical underpinnings, the interpretation of results
and the underlying assumptions of these models. For this introduction we will
keep the NBA data aside and use some simulated data that plays nice with us.
We will return to the NBA data in session 8 applying everything we
learned to assess the effect of scored points on salary.</p>
<div id="objectives-2" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Objectives<a href="lin-t-1.html#objectives-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="objectives">
<ul>
<li>Understand simple linear regression</li>
<li>Understand the regression formula</li>
<li>Interpret the results</li>
</ul>
</div>
</div>
<div id="what-is-linear-regression" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> What is Linear Regression<a href="lin-t-1.html#what-is-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we eluded to last session, there are two main approaches to using
statistical modelling in the social sciences.
The more classical approach is to use modelling for estimating the effect that
one or several independent variables have on one dependent variable. Maybe we
are interested in knowing if a higher income has an effect on life satisfaction
and if yes, what the direction and magnitude of this effect is. Does more money
actually make you happier?</p>
<p>The other and more recent approach is to use modelling for making predictions
with high accuracy. Based on the relationships between many independent
variables and one dependent variable. We try to predict the latter for actual
or hypothetical cases based on their values for the independent variables.
This approach lies at the heart of <em>machine learning</em> and drives many of the
technologies we use on a daily basis from E-Mail spam filters to ChatGPT.
Returning to the example above, we are not interested in measuring the <em>effect</em>
of money on life satisfaction, but in <em>predicting</em> the value for life satisfaction
based on money and a host of other variables as accurately as possible.</p>
<p>Linear regression is one of the many available modelling techniques and it can
serve both approaches. Over the next sessions we will focus on using
linear regression for estimating an effect of interest but we will return to
prediction in <a href="pm-t.html#pm-t">session 10</a>.</p>
<p>How do we know if we should choose linear regression for a specific task?
This is not easy to answer as there are many alternatives and even variations of
linear regression which may be better suited for a specific empirical problem.
As this is an introduction to modelling and time is of the essence we opted to
mainly focus on an in-depth introduction to linear regression.
This technique is suited for many problems and
is comparably easy to understand and use. Also, after learning the ins and outs
of linear regression, we are in a good position to build upon that knowledge and
learn all of those more complex and specific models that we will encounter in
textbooks and scientific papers.</p>
<p>With the pool of options trimmed down to one, the central question remains unanswered.
Should I use linear regression for my task? As we have no alternatives to chose
from, we can change the question to: Can I use linear regression for my task?
The answer mainly depends on what type of dependent variable we want to use.
If it is metric, we can use linear regression.
In our cases the dependent variable is metric, as you will find out below.
If we had other types of dependent variables we would have to use different
models.
For example a common choice for binary or categorical dependent variables is
logistic regression, which we will introduce at a <a href="log-est.html#log-est">later point in the course</a>.</p>
</div>
<div id="examplary-research-question-data" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Examplary research question &amp; data<a href="lin-t-1.html#examplary-research-question-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this introduction, let us imagine that we are interested in a research
question that asks: <em>What makes a good grade in a seminar paper?</em> In particular we
are interested in the effect that the hours a student invests in working on it
has on the grade. Based on some theoretical considerations, and maybe some
idealistic views, we derive our main hypotheses that putting in more hours will
result in a better grade.</p>
<p>Now we also - hypothetically - held a small survey and asked 200 imaginary
students some questions on how they approached writing a seminar paper. In
particular we asked them how much time they spent working on the paper, if they
have attended (almost) all seminar sessions, how closely they worked with their
lecturers in preparing the paper and what the mean grade for previous papers
was. As these imaginary students have already turned in their papers, we also
know the grades they achieved.</p>
<p>Please note, that this is data on <strong>imaginary</strong> students, meaning we have
simulated the data making some assumptions on how to achieve a good (or bad)
grade in a paper. The assumptions we made do not necessarily reflect the way
<em>you</em> write a good paper, while still being based in our experience on what it
takes to achieve a good grade. But remember, no real students were harmed in
making up this data.</p>
<p>Let us have a first look on the data:</p>
<pre><code>## ── Data Summary ────────────────────────
##                            Values
## Name                       grades
## Number of rows             200   
## Number of columns          7     
## _______________________          
## Column type frequency:           
##   factor                   1     
##   logical                  1     
##   numeric                  5     
## ________________________         
## Group variables            None  
## 
## ── Variable type: factor ───────────────────────────────────────────────────────
##   skim_variable n_missing complete_rate ordered n_unique
## 1 contact               0             1 FALSE          3
##   top_counts               
## 1 No : 80, In : 70, E-M: 50
## 
## ── Variable type: logical ──────────────────────────────────────────────────────
##   skim_variable n_missing complete_rate  mean count            
## 1 attendance            0             1 0.765 TRU: 153, FAL: 47
## 
## ── Variable type: numeric ──────────────────────────────────────────────────────
##   skim_variable            n_missing complete_rate      mean    sd     p0    p25
## 1 grade                            0             1  2.97e+ 0 1.08    1     2.1  
## 2 hours                            0             1  4.03e+ 1 6.29   23    36    
## 3 previous_grades                  0             1  2.94e+ 0 0.965   1     2.3  
## 4 previous_grades_centered         0             1 -7.21e-17 0.965  -1.94 -0.635
## 5 hours_centered                   0             1  1.70e-15 6.29  -17.3  -4.33 
##       p50   p75  p100 hist 
## 1  3       3.73  5    ▅▆▇▆▅
## 2 41      45    57    ▁▅▇▅▁
## 3  2.95    3.62  5    ▅▇▇▆▂
## 4  0.0150  0.69  2.06 ▅▇▇▆▂
## 5  0.670   4.67 16.7  ▁▅▇▅▁</code></pre>
<p>Right now, the observations are ordered by the grade of the seminar paper which
run from <span class="math inline">\(1.0\)</span> to <span class="math inline">\(5.0\)</span> in increments of <span class="math inline">\(0.1\)</span>. While this is somewhat
unrealistic - the German grading system actually only uses the increments <span class="math inline">\(.0\)</span>,
<span class="math inline">\(.3\)</span> and <span class="math inline">\(.7\)</span> - simulating the data in this way will make the demonstrations on
linear regression easier and more straightforward. The variable
<code>previous_grades</code> is set up in the same way and represents the mean of the
grades the student received up to this point. <code>hours</code> represents the time a
student spent on writing the paper, ranging from <span class="math inline">\(23 - 57\)</span> hours, with a mean of
about <span class="math inline">\(40\)</span>. Besides these metric variables, the data set also contains two
categorical measures. <code>attendance</code> is a <em>binary</em> or <em>dummy variable</em>, meaning it can only
have the values <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> or <code>TRUE</code> and <code>FALSE</code> in this case, as it is saved as
a logical variable. <code>TRUE</code> represents that a student attended almost all seminar
sessions before writing the paper - which about <span class="math inline">\(77%\)</span> did -, <code>FALSE</code> states that
they did not.
<code>contact</code> is a factor variable with three categories and shows the answers to
the imaginary question on how much contact the student had to the lecturer
before starting the writing process. Besides <code>No contact</code> the students could
have had <code>E-Mail</code> contact to state their research question and get some short
written feedback or meet the lecturer <code>In Person</code> to achieve a deeper discussion
of the question and the laid out plan for writing the paper.
The two additional variables are versions of <code>previous_grades</code> and <code>hours</code> that
are centered on their respective means. They will come into play at a later
point in this session.</p>
<p>Let’s have a look at some observations.</p>
<pre><code>## # A tibble: 10 × 7
##    grade hours previous_grades attendance contact   previous_grades_centered
##    &lt;dbl&gt; &lt;int&gt;           &lt;dbl&gt; &lt;lgl&gt;      &lt;fct&gt;                        &lt;dbl&gt;
##  1     1    50             1.4 TRUE       E-Mail                      -1.54 
##  2     1    46             1   TRUE       E-Mail                      -1.94 
##  3     1    42             1   TRUE       In Person                   -1.94 
##  4     1    49             1   FALSE      In Person                   -1.94 
##  5     1    42             1.2 TRUE       In Person                   -1.74 
##  6     1    46             1.8 TRUE       In Person                   -1.14 
##  7     1    44             1.4 FALSE      In Person                   -1.54 
##  8     1    45             2   TRUE       In Person                   -0.935
##  9     1    48             1   TRUE       In Person                   -1.94 
## 10     1    45             2   TRUE       In Person                   -0.935
## # ℹ 1 more variable: hours_centered &lt;dbl&gt;</code></pre>
<p>From this first 10 rows, we can see that the students with the best grades spent
more than 40 hours on writing, have already achieved good grades in their papers
up to this point and at least had some contact to the lecturers. Most also
regularly attended the seminar but two did not and still achieved a <span class="math inline">\(1.0\)</span> in
their grade.</p>
<p>So what makes a bad grade?</p>
<pre><code>## # A tibble: 10 × 7
##    grade hours previous_grades attendance contact    previous_grades_centered
##    &lt;dbl&gt; &lt;int&gt;           &lt;dbl&gt; &lt;lgl&gt;      &lt;fct&gt;                         &lt;dbl&gt;
##  1   4.8    37             4.2 TRUE       No contact                    1.27 
##  2   4.8    38             4.3 TRUE       E-Mail                        1.36 
##  3   4.8    35             4.4 TRUE       E-Mail                        1.47 
##  4   4.9    40             4.2 TRUE       E-Mail                        1.27 
##  5   5      35             3.9 FALSE      No contact                    0.965
##  6   5      41             4.9 TRUE       No contact                    1.97 
##  7   5      24             4.7 TRUE       E-Mail                        1.76 
##  8   5      33             5   TRUE       E-Mail                        2.06 
##  9   5      29             4.1 FALSE      E-Mail                        1.16 
## 10   5      50             4.6 FALSE      E-Mail                        1.66 
## # ℹ 1 more variable: hours_centered &lt;dbl&gt;</code></pre>
<p>Here the picture seems less clear. While most students did not put in as many
hours, some did and still failed to pass. Half of the students that received a
<span class="math inline">\(5.0\)</span> regularly attended and most at least had E-Mail contact before writing
their paper. What seems to be more consistent though is that the mean of the
previous grades is rather low.</p>
<p>So what do we know now? Does a good or bad track record in grades predict all
future grades? This seems not only unrealistic but is also a kind of sad take home
message. To get a better understanding on which of the potential influential
variables has an effect on the final grade and what the magnitude and direction
of these effects is, we now turn to linear regression.</p>
</div>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Simple Linear Regression<a href="lin-t-1.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a <em>simple linear regression</em>, the model is used to describe the relationship
between <em>one</em> dependent and <em>one</em> independent or explanatory variable. The
question this model can answer for us is: By how much does the dependent
variable increase or decrease, when the explanatory variable increases by <span class="math inline">\(1\)</span>?</p>
<p>Returning to our exemplary research question on what makes a good grade in a
seminar paper, an intuitive hypotheses would be that the grade gets better the
more hours a student invests in writing the paper. In this case we assume a
linear relationship between the independent variable <code>hours</code> and the dependent
variable <code>grade</code>. As German grades are better the lower their value, we thus
would assume a negative effect from <code>hours</code> on <code>grade</code>.</p>
<p>Before turning to the formalities and practical application of a simple linear
regression model, let us first have a look on this relationship by plotting the
variables against each other.</p>
<p><img src="_main_files/figure-html/plot_grade_hours-1.png" width="672" /></p>
<p>When we are talking about dependent and independent variables, there is the
convention to plot the former on the y-axis and the latter on the x-axis. So the
<em>y-variable</em> is to be explained and the <em>x-variable</em> is used to explain it.
This convention will also be used in all formulas in this seminar.</p>
<p>Looking at the plot we first see a cloud of dots, representing all combinations
of <code>hours</code> and <code>grade</code> in all our <span class="math inline">\(200\)</span> observations. It may be hard to pick out
any pattern, but looking closely we can observe that overall the dots seem to
follow a downward slope from the upper left - indicating few hours worked and a
worse grade - towards the lower right - indicating more invested hours and a
better grade. This would be the relationship stated in our hypotheses. The more
hours a student works on a seminar paper the better the final grade will be.</p>
<p>We can try to describe this pattern by adding a line from the upper left to the
lower right.</p>
<p><img src="_main_files/figure-html/plot_grade_hours_wild_guess-1.png" width="672" /></p>
<p>This describes the relationship between the two variables as linear. Each hour
invested decreases the grade by a certain amount, for this proposed line by
exactly <span class="math inline">\(0.08\)</span> points. Remember that decreasing the value of the grade actually
means getting a better grade.</p>
<p>But is this the only possible line or even the <em>correct</em> one? Most certainly not,
as the values used to draw the line were only a wild guess. We
could imagine several other lines that also look more or less reasonable - as
well as some that look unreasonable - and add them to the plot.</p>
<p><img src="_main_files/figure-html/plot_grade_hours_wild_guesses-1.png" width="672" /></p>
<p>While we have some intuition that the green line completely misses the mark, we
can’t really decide between the others just by looking at the plot. The
data points are way to dispersed to see the relationship clearly.</p>
<p>The goal of using a simple linear regression model is to identify the <em>one</em> line
that describes the relationship the best. Here <em>best</em> means, with as little
error as possible.</p>
<div id="regression-formula" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Regression Formula<a href="lin-t-1.html#regression-formula" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To understand how these lines in the above plot were conceived and how to find
the line with the best <em>fit</em>, i.e. the lowest error, we have to understand the
formula for linear regression. While formulas may always be kind of daunting,
we are in luck as this particular one is actually quite easy to understand,
especially when paired with a graphical representation.</p>
<p><span class="math display">\[y = \beta_0 + \beta_1*x_1 + \epsilon\]</span></p>
<p>Let us first look at the parts we already know. <span class="math inline">\(y\)</span> is the dependent variable,
in our case the grade achieved. So one thing is for sure, the whole right part
of the equation has to be used to calculate the value of <span class="math inline">\(y\)</span> from the data, i.e.
from the dependent variable <span class="math inline">\(x\)</span>. Here we have three terms. Let us skip the first one
for now and focus on the second one <span class="math inline">\(\beta_1*x_1\)</span>.</p>
<p><span class="math inline">\(x_1\)</span> is the dependent variable, in our case <code>hours</code>. <span class="math inline">\(\beta_1\)</span> is the
<em>regression coefficient</em> for <span class="math inline">\(x_1\)</span>. This value gives us the <em>slope</em> of the
regression line. Based on this, we can start rewriting the general formula and
tailor it to our specific use case.</p>
<p><span class="math display">\[y_{grade} = \beta_0 + \beta_{hours}*x_{hours} + \epsilon\]</span></p>
<p>Let us return to the first wild guess we made above.</p>
<p><img src="_main_files/figure-html/plot_grade_hours_wild_guess_expl_slope-1.png" width="672" /></p>
<p>Here we guessed that an increase in invested time of one hour decreases the
value of <code>grade</code> by <span class="math inline">\(0.08\)</span>. This is the slope of the red line and thus also the
coefficient in the regression formula that is used in computing said line. So,
<span class="math inline">\(\beta_{hours} = -0.08\)</span>. We can insert this value into our formula.</p>
<p><span class="math display">\[y_{grade} = \beta_0 -0.08*x_{hours} + \epsilon\]</span></p>
<p>In this way the value of <span class="math inline">\(x_{hours}\)</span> is multiplied by <span class="math inline">\(-0.08\)</span>. Let us assume a
student worked <span class="math inline">\(40\)</span> hours on their paper. <span class="math inline">\(-0.08*40\)</span> being <span class="math inline">\(-3.2\)</span>, we assume that
working 40 hours on a paper <em>on average</em> - more on that later - leads to a <span class="math inline">\(3.2\)</span>
lower grade value. But <span class="math inline">\(3.2\)</span> lower than what?</p>
<p>Looking at the formula again, we see that we subtract this value from <span class="math inline">\(\beta_0\)</span>.
This is the <em>intercept</em>, the value at which the line intersects with the y-axis.
Let us zoom out on our plot to see what happens.</p>
<p><img src="_main_files/figure-html/plot_grade_hours_wild_guess_expl_intercept-1.png" width="672" /></p>
<p>We can now see the point where the red line intersects with the y-axis. This is
the intercept of this line, i.e. <span class="math inline">\(\beta_0 = 6\)</span>.</p>
<p><span class="math display">\[y_{grade} = 6 -0.08*x_{hours} + \epsilon\]</span></p>
<p>If we now again assume a time investment of <span class="math inline">\(40\)</span> hours, we can compute
<span class="math inline">\(6-0.08*40 = 2.8\)</span>. So our red regression line - which is still only a wild guess
- assumes, that working 40 hours on a seminar paper will result in a grade of
<span class="math inline">\(2.8\)</span>, on average. We can mark these values in our plot:</p>
<p><img src="_main_files/figure-html/plot_grade_hours_wild_guess_expl_40hours-1.png" width="672" /></p>
<p>The red dot is the intersection of the values <code>hours = 40</code> and <code>grade = 2.8</code>.
As this is the value for <span class="math inline">\(y\)</span> our regression line assumes a student with a time
investment of 40 hours achieves, the red dot also lies exactly on the red line.</p>
<p>But if we look at the plot once again, we can see that most actual observations
for students that invested 40 hours do not actually lie on the regression line
but are scattered above and below the line. Some of these students achieve
much worse or much better grades than <span class="math inline">\(2.8\)</span> investing the same amount of time in
their work. This leads us to the last part of the formula, <span class="math inline">\(\epsilon\)</span>.</p>
<p>This is the <em>error term</em>. Having data that is dispersed like this - and any real
world data will always be - our linear line will never be able to pass exactly
through every data point. Some points may lie exactly on the line, but many or
most will not.</p>
<p>We can visualize this. To keep the plot readable, we only do this for some
random observations but in reality the distance of every data point from the
regression line is taken into account.</p>
<p><img src="_main_files/figure-html/plot_grade_hours_wild_guess_expl_residuals-1.png" width="672" /></p>
<p>The distance of these or rather all points from the line, the <em>residuals</em>, are
represented in the error term <span class="math inline">\(\epsilon\)</span>. It is a measure for how wrong our line
is in describing the data in its entirety. So why is it wrong? We can not say
for sure, but there are two common main reasons.</p>
<p>For one, there may be other variables that also influence the relationship
between invested hours and achieved grade, something that we will return to
in the <a href="lin-t-2.html#lin-t-2">next session</a>, when we expand the idea of linear regression to multiple
independent variables.</p>
<p>But there is also random variation present in every bit of real world data.
While our data is simulated, we also added random variation on purpose. Because
this is what real world data is, it is messy and it is noisy.</p>
<p>Not every seminar paper that had the same time investment, e.g. 40 hours, will
have the same quality in results. There may be other influential variables, e.g.
the student’s general skill level or if they sought assistance by their lecturer
in preparing the paper, influencing the final grade. But even if the quality of
the paper after working 40 hours would be the same for each student, measurement
error, i.e. noise, will be introduced because not every lecturer will grade
exactly the same or maybe because papers were submitted at different time
points and grading standards may have changed. If we can not measure these
variables we have to accept these unobservable sources of noise and hope, where
<em>hope</em> actually means thorough theoretical and methodical thinking, that we can
still measure our effect of interest. This also means, that measuring and
modelling <strong>always</strong> includes uncertainty. We never know for certain if and to
what extent our results are influenced by unobservable variables and random
variation. Still, there are ways to assess this uncertainty, which we will
regularly return to during the course. This should not stop quantitative
social scientists from making strong or even bold arguments based in thorough
theoretical thinking and responsible data analysis, but we always have to
acknowledge the uncertainty included in every step and make it a part of our
interpretations and conclusions.</p>
<p>The error term <span class="math inline">\(\epsilon\)</span> is the final piece of the puzzle in actually computing
a linear regression model. Without jumping into the mathematics of it all, the
technique that is used to estimate the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is
called <em>OLS</em> - Ordinary Least Squares. What it basically does, is to take the
squares of all residuals, i.e. the distances of the data points from the
regression line, sum them up and minimise this value. All this substantially
means is, that OLS searches for the regression line with the lowest amount of error,
i.e. the lowest overall distance from the actual data points.</p>
<p>OLS gives us estimates for the regression coefficients in this formula:</p>
<p><span class="math display">\[\hat{y} = b_0 +b_1*x_1\]</span></p>
<p>We can see two differences to the formula we started with. First, we write
<span class="math inline">\(\hat{y}\)</span> - pronounced as “y hat” - instead of <span class="math inline">\(y\)</span>. At the same time, we exclude
the error term <span class="math inline">\(\epsilon\)</span>. This means that we are no longer computing the actual
value of <span class="math inline">\(y\)</span>, as in the point on the regression line for a certain value of
<span class="math inline">\(x_1\)</span> <span class="math inline">\(+\)</span> the error, but the estimate <span class="math inline">\(\hat{y}\)</span>, as in the point on the
regression line that is predicted for a certain value of <span class="math inline">\(x_1\)</span>. Second, we write
<span class="math inline">\(b\)</span> instead of <span class="math inline">\(\beta\)</span>. This also alludes to the fact that we are now
computing an estimate for the coefficients based on the data available and not
the real but unknown value of <span class="math inline">\(\beta\)</span>.</p>
<p>This implies that we now estimate the same grade for every student who invested
the same amount of time, the <span class="math inline">\(\hat{y}\)</span> that lies exactly on our regression line
at a certain value of <span class="math inline">\(x_1\)</span>. For all students who invested <span class="math inline">\(40\)</span> hours in writing,
we would estimate exactly the same grade. As we have seen above, these students
received different grades in reality, or more accurately our simulated reality.
The value of <span class="math inline">\(\hat{y}\)</span> is still the best guess our model can make. That is what
we mean when we say “on average”. On average a student is estimated to receive
the grade <span class="math inline">\(\hat{y}\)</span> after investing <span class="math inline">\(x_1\)</span> hours in writing the paper. We have
to keep in mind, that this will not be true for many students; there is always
an error involved in our estimates.</p>
</div>
<div id="regressing-grade-on-hours" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Regressing <code>grade</code> on <code>hours</code><a href="lin-t-1.html#regressing-grade-on-hours" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we have a firmer understanding on what linear regression actually is
and does, we can finally get to the fun part and use the technique for
estimating the effect of <code>hours</code> on <code>grade</code> or in other words, regress
<code>grade</code> on <code>hours</code>.</p>
<pre><code>## 
## Call:
## lm(formula = grade ~ hours, data = grades)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.88006 -0.83961 -0.08006  0.77006  2.53881 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.07912    0.47306  10.737  &lt; 2e-16 ***
## hours       -0.05236    0.01159  -4.517 1.07e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.028 on 198 degrees of freedom
## Multiple R-squared:  0.09344,    Adjusted R-squared:  0.08886 
## F-statistic: 20.41 on 1 and 198 DF,  p-value: 1.075e-05</code></pre>
<p>This is the output from a simple linear regression for <code>grade</code> on <code>hours</code> R
returns to us.
How we can do this in practice and what the first two lines mean will be the
topic of <a href="lin-a.html#lin-a">session 8</a>. For now we will focus on the estimates in the coefficient
block and introduce the additional elements of the output on by one over the
next sessions.</p>
<p>The column <code>Estimate</code> gives us the values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> discussed
above. The estimated coefficient for <code>hours</code> tells us that out intuition was
right, the more hours a student invests in writing a paper, the better the grade
will be. In this case every additional hour spent on working is estimated to decrease the
value of the grade by <span class="math inline">\(-0.05236\)</span> points. In keeping with the example of a 40
hour workload this leads to a decrease of <span class="math inline">\(-0.05236 * 40 = -2.0944\)</span> points.
Adding the intercept from the same column, the estimated grade after working 40
hours is <span class="math inline">\(5.07912 -0.05236 * 40 = 2.98472\)</span>. So on average a student from our
simulated data set will pass after 40 hours of work but will not get a great
grade.
Remember, this is the expected average value. This does not mean that some
students will not get better or worse grades, or even fail to pass with this
amount of time investment.</p>
<p>Now that we know the coefficients for the regression line with the best fit,
i.e. the lowest error, we can again visualise the result.</p>
<p><img src="_main_files/figure-html/plot_lm_grade_hours-1.png" width="672" /></p>
<p>What grade can a student expect, on average, if they invest exactly 0
hours, i.e. do nothing and hand in a blank paper. We can look at the graph or,
to achieve a more precise result, calculate it.</p>
<p><span class="math display">\[5.07912 -0.05236 * 0 = 5.07912\]</span></p>
<p>For this theoretical example of <span class="math inline">\(x_{hours} = 0\)</span>, the estimated value <span class="math inline">\(\hat{y}\)</span>
or <span class="math inline">\(y_{grade}\)</span> is the same as the intercept <span class="math inline">\(\beta_0\)</span>. This is what the
intercept represents in general, the estimated value <span class="math inline">\(\hat{y}\)</span> when the
dependent variable equals <span class="math inline">\(0\)</span>.</p>
<p>Investing zero hours in a seminar paper is not only not advisable, it is
also not a value we observed in our data. If the data would include observations
with zero hours of time invested, the grade would be a firm <span class="math inline">\(5.0\)</span> and the same
would be true for low single digits, i.e. turning in a two-pager as a seminar
paper. The takeaway is, that the model is highly dependent on the data that it
is trained on. If the data would have included such cases we could expect a
higher intercept and a steeper slope, i.e. a stronger negative coefficient.</p>
<p>Luckily all our simulated students have put in at least some hours. But as we do
not have data for zero to <span class="math inline">\(22\)</span> hours, we can not really make reliable estimates
in this range. Because of this, it does not really make sense to enter <code>hours</code>
into the regression model as ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(57\)</span>. One solution that is
often used for metric variables is to center them on their mean. This can be
achieved by simply subtracting the mean of <span class="math inline">\(x\)</span> from each individual value:</p>
<p><span class="math display">\[x_i - \bar{x}\]</span></p>
<p>We can now rerun the regression.</p>
<pre><code>## 
## Call:
## lm(formula = grade ~ hours_centered, data = grades)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.88006 -0.83961 -0.08006  0.77006  2.53881 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     2.96750    0.07267  40.835  &lt; 2e-16 ***
## hours_centered -0.05236    0.01159  -4.517 1.07e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.028 on 198 degrees of freedom
## Multiple R-squared:  0.09344,    Adjusted R-squared:  0.08886 
## F-statistic: 20.41 on 1 and 198 DF,  p-value: 1.075e-05</code></pre>
<p>Comparing the results to the first model shows us, that the coefficient for
<span class="math inline">\(b_{hours\_centered}\)</span> is exactly the same as for <span class="math inline">\(b_{hours}\)</span>. So the effect
of working more hours has not changed. What has changed is the value of the
intercept. This will make more sense if we again plot the regression line.</p>
<p><img src="_main_files/figure-html/plot_lm_grade_hours_centered_0-1.png" width="672" /></p>
<p>By centering the x-Variable on its mean we have changed its interpretation.
A value of <code>hours = 0</code> now stands for investing as much time as the mean
of <code>hours</code> in the whole data set, which in this case is <span class="math inline">\(40.33\)</span> hours. Positive
values indicate that a student worked <span class="math inline">\(x\)</span> hours more, negatives indicate <span class="math inline">\(-x\)</span>
hours less compared to the mean. In this way, we also moved the y-axis and thus
changed the interpretation of the intercept. Its new value of <span class="math inline">\(2.9675\)</span> now
indicates the estimate for a student who invests the mean value of <code>hours</code> in
their work, i.e. <span class="math inline">\(40.33\)</span>.</p>
</div>
</div>
<div id="moving-on-1" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Moving on<a href="lin-t-1.html#moving-on-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Based on our simple linear regression model we achieved an estimate for our
effect of interest. Working more hours results in receiving a better grade.
But there could be other variables that influence this relationship. In the
<a href="lin-t-2.html#lin-t-2">next session</a> we learn how we can include these in our model, when we move from
simple to multiple linear regression.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dags-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lin-t-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-linear-regression-t-1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
